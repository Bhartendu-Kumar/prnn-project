{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sf9jruOaqBpe"
      },
      "outputs": [],
      "source": [
        "#DCGAN\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
        "\n",
        "from __future__ import print_function\n",
        "#%matplotlib inline\n",
        "import argparse\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "from IPython.display import HTML\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision.utils import make_grid\n",
        "from IPython.display import Image\n",
        "# Set random seed for reproducibility\n",
        "manualSeed = 999\n",
        "#manualSeed = random.randint(1, 10000) # use if you want new results\n",
        "print(\"Random Seed: \", manualSeed)\n",
        "random.seed(manualSeed)\n",
        "torch.manual_seed(manualSeed)\n",
        "\n",
        "# Root directory for dataset\n",
        "dataroot = \"../input/celeba-dataset/img_align_celeba\"\n",
        "\n",
        "# Number of workers for dataloader\n",
        "workers = 2\n",
        "\n",
        "# Batch size during training\n",
        "batch_size = 128\n",
        "\n",
        "# Spatial size of training images. All images will be resized to this\n",
        "#   size using a transformer.\n",
        "image_size = 64\n",
        "\n",
        "# Number of channels in the training images. For color images this is 3\n",
        "nc = 3\n",
        "\n",
        "# Size of z latent vector (i.e. size of generator input)\n",
        "nz = 100\n",
        "\n",
        "# Size of feature maps in generator\n",
        "ngf = 64\n",
        "\n",
        "# Size of feature maps in discriminator\n",
        "ndf = 64\n",
        "\n",
        "# Number of training epochs\n",
        "num_epochs = 10\n",
        "\n",
        "# Learning rate for optimizers\n",
        "lr = 0.0002\n",
        "\n",
        "# Beta1 hyperparam for Adam optimizers\n",
        "beta1 = 0.5\n",
        "\n",
        "# Number of GPUs available. Use 0 for CPU mode.\n",
        "ngpu = 1\n",
        "\n",
        "image_size = 64\n",
        "DATA_DIR = \"../input/pneumoniamnist/pneumoniamnist.npz\"\n",
        "X_train = np.load(DATA_DIR)\n",
        "#print(f\"Shape of training data: {X_train.shape}\")\n",
        "print(f\"Data type: {type(X_train)}\")\n",
        "data = X_train[\"train_images\"].astype(np.float64)\n",
        "x = []\n",
        "for img in data:\n",
        "    x.append(np.dstack([img,img,img]))\n",
        "x = np.array(x)\n",
        "y = np.array(X_train[\"train_labels\"])\n",
        "data = x\n",
        "data = 255 * data\n",
        "img = data.astype(np.uint8)\n",
        "X_train = img\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, data, targets, transform=None):\n",
        "        self.data = data\n",
        "        self.targets = torch.LongTensor(targets)\n",
        "        self.transform = transform\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        x = self.data[index]\n",
        "        y = self.targets[index]\n",
        "        \n",
        "        if self.transform:\n",
        "            x = self.transform(x)\n",
        "        \n",
        "        return x, y\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "transform = transforms.Compose([transforms.ToPILImage(),\n",
        "                                transforms.Resize(64),\n",
        "                                transforms.CenterCrop(image_size),\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),])\n",
        "cropped_dataset = MyDataset(X_train, y, transform=transform)\n",
        "print(len(cropped_dataset))\n",
        "\n",
        "print(cropped_dataset[0][0].size())\n",
        "\n",
        "# We can use an image folder dataset the way we have it setup.\n",
        "# Create the dataset\n",
        "# dataset = dset.ImageFolder(root=dataroot,\n",
        "#                            transform=transforms.Compose([\n",
        "#                                transforms.Resize(image_size),\n",
        "#                                transforms.CenterCrop(image_size),\n",
        "#                                transforms.ToTensor(),\n",
        "#                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "#                            ]))\n",
        "# print(len(dataset))\n",
        "\n",
        "# print(type(dataset))\n",
        "# print(dataset[0][0].shape)\n",
        "\n",
        "#dataset_subset = torch.utils.data.Subset(dataset, np.random.choice(len(dataset), 10000, replace=False))\n",
        "#print(len(dataset_subset))\n",
        "# Create the dataloader\n",
        "dataloader = torch.utils.data.DataLoader(cropped_dataset, batch_size=batch_size,\n",
        "                                         shuffle=True, num_workers=workers, pin_memory=True)\n",
        "\n",
        "#dataloader1 = torch.utils.data.DataLoader(dataset_subset, batch_size = 64,shuffle=True, num_workers=workers)\n",
        "# Decide which device we want to run on\n",
        "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
        "\n",
        "\n",
        "real_batch = next(iter(dataloader))\n",
        "#real_batch1 = next(iter(dataloader1))\n",
        "print(real_batch[0].size())\n",
        "#print(real_batch1[0].size())\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Training Images\")\n",
        "plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device), padding=2, normalize=True,nrow=10,ncol=10).cpu(),(1,2,0)))\n",
        "\n",
        "# custom weights initialization called on netG and netD\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        nn.init.constant_(m.bias.data, 0)\n",
        "\n",
        "# Generator Code\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, ngpu):\n",
        "        super(Generator, self).__init__()\n",
        "        self.ngpu = ngpu\n",
        "        self.main = nn.Sequential(\n",
        "            # input is Z, going into a convolution\n",
        "            nn.ConvTranspose2d( nz, ngf * 8, 4, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 8),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf*8) x 4 x 4\n",
        "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 4),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf*4) x 8 x 8\n",
        "            nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 2),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf*2) x 16 x 16\n",
        "            nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf) x 32 x 32\n",
        "            nn.ConvTranspose2d( ngf, nc, 4, 2, 1, bias=False),\n",
        "            nn.Tanh()\n",
        "            # state size. (nc) x 64 x 64\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input)\n",
        "\n",
        "# Create the generator\n",
        "netG = Generator(ngpu).to(device)\n",
        "\n",
        "# Handle multi-gpu if desired\n",
        "if (device.type == 'cuda') and (ngpu > 1):\n",
        "    netG = nn.DataParallel(netG, list(range(ngpu)))\n",
        "\n",
        "# Apply the weights_init function to randomly initialize all weights\n",
        "#  to mean=0, stdev=0.02.\n",
        "netG.apply(weights_init)\n",
        "\n",
        "# Print the model\n",
        "print(netG)\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, ngpu):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.ngpu = ngpu\n",
        "        self.main = nn.Sequential(\n",
        "            # input is (nc) x 64 x 64\n",
        "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf) x 32 x 32\n",
        "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf*2) x 16 x 16\n",
        "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 4),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf*4) x 8 x 8\n",
        "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 8),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf*8) x 4 x 4\n",
        "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input)\n",
        "\n",
        "# Create the Discriminator\n",
        "netD = Discriminator(ngpu).to(device)\n",
        "\n",
        "# Handle multi-gpu if desired\n",
        "if (device.type == 'cuda') and (ngpu > 1):\n",
        "    netD = nn.DataParallel(netD, list(range(ngpu)))\n",
        "\n",
        "# Apply the weights_init function to randomly initialize all weights\n",
        "#  to mean=0, stdev=0.2.\n",
        "netD.apply(weights_init)\n",
        "\n",
        "# Print the model\n",
        "print(netD)\n",
        "\n",
        "# Initialize BCELoss function\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# Create batch of latent vectors that we will use to visualize\n",
        "#  the progression of the generator\n",
        "fixed_noise = torch.randn(64, nz, 1, 1, device=device)\n",
        "\n",
        "# Establish convention for real and fake labels during training\n",
        "real_label = 1.\n",
        "fake_label = 0.\n",
        "\n",
        "# Setup Adam optimizers for both G and D\n",
        "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
        "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))\n",
        "\n",
        "from torch.nn.functional import adaptive_avg_pool2d\n",
        "import torchvision.models as models\n",
        "import torch.nn.functional as F\n",
        "from scipy import linalg\n",
        "\n",
        "class InceptionV3(nn.Module):\n",
        "    \"\"\"Pretrained InceptionV3 network returning feature maps\"\"\"\n",
        "\n",
        "    # Index of default block of inception to return,\n",
        "    # corresponds to output of final average pooling\n",
        "    DEFAULT_BLOCK_INDEX = 3\n",
        "\n",
        "    # Maps feature dimensionality to their output blocks indices\n",
        "    BLOCK_INDEX_BY_DIM = {\n",
        "        64: 0,   # First max pooling features\n",
        "        192: 1,  # Second max pooling featurs\n",
        "        768: 2,  # Pre-aux classifier features\n",
        "        2048: 3  # Final average pooling features\n",
        "    }\n",
        "\n",
        "    def __init__(self,\n",
        "                 output_blocks=[DEFAULT_BLOCK_INDEX],\n",
        "                 resize_input=True,\n",
        "                 normalize_input=True,\n",
        "                 requires_grad=False):\n",
        "        \n",
        "        super(InceptionV3, self).__init__()\n",
        "\n",
        "        self.resize_input = resize_input\n",
        "        self.normalize_input = normalize_input\n",
        "        self.output_blocks = sorted(output_blocks)\n",
        "        self.last_needed_block = max(output_blocks)\n",
        "\n",
        "        assert self.last_needed_block <= 3, \\\n",
        "            'Last possible output block index is 3'\n",
        "\n",
        "        self.blocks = nn.ModuleList()\n",
        "\n",
        "        \n",
        "        inception = models.inception_v3(pretrained=True)\n",
        "\n",
        "        # Block 0: input to maxpool1\n",
        "        block0 = [\n",
        "            inception.Conv2d_1a_3x3,\n",
        "            inception.Conv2d_2a_3x3,\n",
        "            inception.Conv2d_2b_3x3,\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2)\n",
        "        ]\n",
        "        self.blocks.append(nn.Sequential(*block0))\n",
        "\n",
        "        # Block 1: maxpool1 to maxpool2\n",
        "        if self.last_needed_block >= 1:\n",
        "            block1 = [\n",
        "                inception.Conv2d_3b_1x1,\n",
        "                inception.Conv2d_4a_3x3,\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2)\n",
        "            ]\n",
        "            self.blocks.append(nn.Sequential(*block1))\n",
        "\n",
        "        # Block 2: maxpool2 to aux classifier\n",
        "        if self.last_needed_block >= 2:\n",
        "            block2 = [\n",
        "                inception.Mixed_5b,\n",
        "                inception.Mixed_5c,\n",
        "                inception.Mixed_5d,\n",
        "                inception.Mixed_6a,\n",
        "                inception.Mixed_6b,\n",
        "                inception.Mixed_6c,\n",
        "                inception.Mixed_6d,\n",
        "                inception.Mixed_6e,\n",
        "            ]\n",
        "            self.blocks.append(nn.Sequential(*block2))\n",
        "\n",
        "        # Block 3: aux classifier to final avgpool\n",
        "        if self.last_needed_block >= 3:\n",
        "            block3 = [\n",
        "                inception.Mixed_7a,\n",
        "                inception.Mixed_7b,\n",
        "                inception.Mixed_7c,\n",
        "                nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
        "            ]\n",
        "            self.blocks.append(nn.Sequential(*block3))\n",
        "\n",
        "        for param in self.parameters():\n",
        "            param.requires_grad = requires_grad\n",
        "\n",
        "    def forward(self, inp):\n",
        "        \"\"\"Get Inception feature maps\n",
        "        Parameters\n",
        "        ----------\n",
        "        inp : torch.autograd.Variable\n",
        "            Input tensor of shape Bx3xHxW. Values are expected to be in\n",
        "            range (0, 1)\n",
        "        Returns\n",
        "        -------\n",
        "        List of torch.autograd.Variable, corresponding to the selected output\n",
        "        block, sorted ascending by index\n",
        "        \"\"\"\n",
        "        outp = []\n",
        "        x = inp\n",
        "\n",
        "        if self.resize_input:\n",
        "            x = F.interpolate(x,\n",
        "                              size=(299, 299),\n",
        "                              mode='bilinear',\n",
        "                              align_corners=False)\n",
        "\n",
        "        if self.normalize_input:\n",
        "            x = 2 * x - 1  # Scale from range (0, 1) to range (-1, 1)\n",
        "\n",
        "        for idx, block in enumerate(self.blocks):\n",
        "            x = block(x)\n",
        "            if idx in self.output_blocks:\n",
        "                outp.append(x)\n",
        "\n",
        "            if idx == self.last_needed_block:\n",
        "                break\n",
        "\n",
        "        return outp\n",
        "    \n",
        "block_idx = InceptionV3.BLOCK_INDEX_BY_DIM[2048]\n",
        "model = InceptionV3([block_idx])\n",
        "model=model.cuda()\n",
        "\n",
        "def calculate_activation_statistics(images,model,batch_size=128, dims=2048,\n",
        "                    cuda=False):\n",
        "    model.eval()\n",
        "    act=np.empty((len(images), dims))\n",
        "    \n",
        "    if cuda:\n",
        "        batch=images.cuda()\n",
        "    else:\n",
        "        batch=images\n",
        "    pred = model(batch)[0]\n",
        "\n",
        "        # If model output is not scalar, apply global spatial average pooling.\n",
        "        # This happens if you choose a dimensionality not equal 2048.\n",
        "    if pred.size(2) != 1 or pred.size(3) != 1:\n",
        "        pred = adaptive_avg_pool2d(pred, output_size=(1, 1))\n",
        "\n",
        "    act= pred.cpu().data.numpy().reshape(pred.size(0), -1)\n",
        "    \n",
        "    mu = np.mean(act, axis=0)\n",
        "    sigma = np.cov(act, rowvar=False)\n",
        "    return mu, sigma\n",
        "\n",
        "def calculate_frechet_distance(mu1, sigma1, mu2, sigma2, eps=1e-6):\n",
        "    \"\"\"Numpy implementation of the Frechet Distance.\n",
        "    The Frechet distance between two multivariate Gaussians X_1 ~ N(mu_1, C_1)\n",
        "    and X_2 ~ N(mu_2, C_2) is\n",
        "            d^2 = ||mu_1 - mu_2||^2 + Tr(C_1 + C_2 - 2*sqrt(C_1*C_2)).\n",
        "    \"\"\"\n",
        "\n",
        "    mu1 = np.atleast_1d(mu1)\n",
        "    mu2 = np.atleast_1d(mu2)\n",
        "\n",
        "    sigma1 = np.atleast_2d(sigma1)\n",
        "    sigma2 = np.atleast_2d(sigma2)\n",
        "\n",
        "    assert mu1.shape == mu2.shape, \\\n",
        "        'Training and test mean vectors have different lengths'\n",
        "    assert sigma1.shape == sigma2.shape, \\\n",
        "        'Training and test covariances have different dimensions'\n",
        "\n",
        "    diff = mu1 - mu2\n",
        "\n",
        "    \n",
        "    covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n",
        "    if not np.isfinite(covmean).all():\n",
        "        msg = ('fid calculation produces singular product; '\n",
        "               'adding %s to diagonal of cov estimates') % eps\n",
        "        print(msg)\n",
        "        offset = np.eye(sigma1.shape[0]) * eps\n",
        "        covmean = linalg.sqrtm((sigma1 + offset).dot(sigma2 + offset))\n",
        "\n",
        "    \n",
        "    if np.iscomplexobj(covmean):\n",
        "        if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):\n",
        "            m = np.max(np.abs(covmean.imag))\n",
        "            raise ValueError('Imaginary component {}'.format(m))\n",
        "        covmean = covmean.real\n",
        "\n",
        "    tr_covmean = np.trace(covmean)\n",
        "\n",
        "    return (diff.dot(diff) + np.trace(sigma1) +\n",
        "            np.trace(sigma2) - 2 * tr_covmean)\n",
        "\n",
        "def calculate_fretchet(images_real,images_fake,model):\n",
        "     mu_1,std_1=calculate_activation_statistics(images_real,model,cuda=True)\n",
        "     mu_2,std_2=calculate_activation_statistics(images_fake,model,cuda=True)\n",
        "    \n",
        "     \"\"\"get fretched distance\"\"\"\n",
        "     fid_value = calculate_frechet_distance(mu_1, std_1, mu_2, std_2)\n",
        "     return fid_value\n",
        "\n",
        "print(\"Generator Parameters:\",sum(p.numel() for p in netG.parameters() if p.requires_grad))\n",
        "print(\"Discriminator Parameters:\",sum(p.numel() for p in netD.parameters() if p.requires_grad))\n",
        "\n",
        "# Commented out IPython magic to ensure Python compatibility.\n",
        "# Training Loop\n",
        "\n",
        "# Lists to keep track of progress\n",
        "img_list = []\n",
        "G_losses = []\n",
        "D_losses = []\n",
        "FID = []\n",
        "iters = 0\n",
        "\n",
        "print(\"Starting Training Loop...\")\n",
        "# For each epoch\n",
        "for epoch in range(num_epochs):\n",
        "    # For each batch in the dataloader\n",
        "    for i, data in enumerate(dataloader, 0):\n",
        "\n",
        "        ############################\n",
        "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
        "        ###########################\n",
        "        ## Train with all-real batch\n",
        "        netD.zero_grad()\n",
        "        # Format batch\n",
        "        real_cpu = data[0].to(device)\n",
        "        b_size = real_cpu.size(0)\n",
        "        label = torch.full((b_size,), real_label, dtype=torch.float, device=device)\n",
        "        # Forward pass real batch through D\n",
        "        output = netD(real_cpu).view(-1)\n",
        "        # Calculate loss on all-real batch\n",
        "        errD_real = criterion(output, label)\n",
        "        # Calculate gradients for D in backward pass\n",
        "        errD_real.backward()\n",
        "        D_x = output.mean().item()\n",
        "\n",
        "        ## Train with all-fake batch\n",
        "        # Generate batch of latent vectors\n",
        "        noise = torch.randn(b_size, nz, 1, 1, device=device)\n",
        "        # Generate fake image batch with G\n",
        "        fake = netG(noise)\n",
        "        label.fill_(fake_label)\n",
        "        # Classify all fake batch with D\n",
        "        output = netD(fake.detach()).view(-1)\n",
        "        # Calculate D's loss on the all-fake batch\n",
        "        errD_fake = criterion(output, label)\n",
        "        # Calculate the gradients for this batch, accumulated (summed) with previous gradients\n",
        "        errD_fake.backward()\n",
        "        D_G_z1 = output.mean().item()\n",
        "        # Compute error of D as sum over the fake and the real batches\n",
        "        errD = errD_real + errD_fake\n",
        "        # Update D\n",
        "        optimizerD.step()\n",
        "\n",
        "        ############################\n",
        "        # (2) Update G network: maximize log(D(G(z)))\n",
        "        ###########################\n",
        "        netG.zero_grad()\n",
        "        label.fill_(real_label)  # fake labels are real for generator cost\n",
        "        # Since we just updated D, perform another forward pass of all-fake batch through D\n",
        "        output = netD(fake).view(-1)\n",
        "        # Calculate G's loss based on this output\n",
        "        errG = criterion(output, label)\n",
        "        # Calculate gradients for G\n",
        "        errG.backward()\n",
        "        D_G_z2 = output.mean().item()\n",
        "        # Update G\n",
        "        optimizerG.step()\n",
        "\n",
        "        # Output training stats\n",
        "        if i % 50 == 0:\n",
        "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
        "#                   % (epoch, num_epochs, i, len(dataloader),\n",
        "                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
        "\n",
        "        # Save Losses for plotting later\n",
        "        G_losses.append(errG.item())\n",
        "        D_losses.append(errD.item())\n",
        "        fretchet_dist=calculate_fretchet(real_cpu,fake,model)\n",
        "        FID.append(fretchet_dist)\n",
        "        # Check how the generator is doing by saving G's output on fixed_noise\n",
        "        if (iters % 100 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)):\n",
        "            with torch.no_grad():\n",
        "                fake = netG(fixed_noise).detach().cpu()\n",
        "            img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n",
        "\n",
        "        iters += 1\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.title(\"Generator and Discriminator Loss During Training\")\n",
        "plt.plot(G_losses,label=\"G\")\n",
        "plt.plot(D_losses,label=\"D\")\n",
        "plt.xlabel(\"iterations\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.title(\"FID Score During Training\")\n",
        "plt.plot(FID,label=\"FID\")\n",
        "plt.xlabel(\"iterations\")\n",
        "plt.ylabel(\"score\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "#%%capture\n",
        "fig = plt.figure(figsize=(8,8))\n",
        "plt.axis(\"off\")\n",
        "ims = [[plt.imshow(np.transpose(i,(1,2,0)), animated=True)] for i in img_list]\n",
        "ani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)\n",
        "\n",
        "HTML(ani.to_jshtml())\n",
        "\n",
        "# Grab a batch of real images from the dataloader\n",
        "real_batch = next(iter(dataloader))\n",
        "\n",
        "# Plot the real images\n",
        "plt.figure(figsize=(15,15))\n",
        "plt.subplot(1,2,1)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Real Images\")\n",
        "plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=5, normalize=True).cpu(),(1,2,0)))\n",
        "\n",
        "# Plot the fake images from the last epoch\n",
        "plt.subplot(1,2,2)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Fake Images\")\n",
        "plt.imshow(np.transpose(img_list[-1],(1,2,0)))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0tiXsMHkq0KJ"
      },
      "outputs": [],
      "source": [
        "#VAE\n",
        "VAE for PneumoniaMNINST dataset\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from keras import backend as K\n",
        "from sklearn.model_selection import train_test_split\n",
        "from skimage.transform import resize\n",
        "\n",
        "epochs=150\n",
        "batch_size=8\n",
        "input_shape=(28,28,1)\n",
        "latent_dim=2\n",
        "h_image,w_image=28,28\n",
        "path = '/content/drive/MyDrive/PRNN/pneumoniamnist.npz'\n",
        "\n",
        "def process_data(path):\n",
        "    dataset = np.load(path)\n",
        "    train_x, train_y = dataset['train_images'], dataset['train_labels']\n",
        "    val_x, val_y = dataset['val_images'], dataset['val_labels']\n",
        "    test_x, test_y = dataset['test_images'], dataset['test_labels']\n",
        "    return train_x, train_y, test_x, test_y, val_x, val_y\n",
        "\n",
        "#(X_train,y_train) , (X_test,y_test) = tf.keras.datasets.mnist.load_data()\n",
        "X_train, y_train, X_test, y_test, t, d = process_data(path)\n",
        "X_train = X_train.reshape(X_train.shape[0],28,28,1).astype('float32')/255.0\n",
        "X_test = X_test.reshape(X_test.shape[0],28,28,1).astype('float32')/255.0\n",
        "y_train = y_train.astype('int32')\n",
        "y_test = y_test.astype('int32')\n",
        "\n",
        "train_dataset=tf.data.Dataset.from_tensor_slices((X_train,y_train)).\\\n",
        "batch(batch_size).repeat()\n",
        "\n",
        "test_dataset=tf.data.Dataset.from_tensor_slices((X_test,y_test)).\\\n",
        "batch(batch_size).repeat()\n",
        "\n",
        "import time\n",
        "\n",
        "def reparameterize(mean, logvar):\n",
        "    eps = tf.random.normal(shape=tf.shape(mean))\n",
        "    return eps * tf.exp(logvar * .5) + mean  \n",
        "\n",
        "def model(input_shape,latent_dim):\n",
        "    #Encoder \n",
        "    model_input = tf.keras.Input(input_shape)\n",
        "    layer = tf.keras.layers.Conv2D(filters = 32, kernel_size = 3, strides = 1, activation = 'relu', padding = 'same')(model_input)\n",
        "    layer = tf.keras.layers.Conv2D(filters = 16, kernel_size = 3, strides = 2, activation = 'relu', padding = 'same')(layer)\n",
        "    layer = tf.keras.layers.Conv2D(filters = 32, kernel_size = 3, strides = 2, activation = 'relu', padding = 'same')(layer)\n",
        "    shape_before_flatten=layer.shape\n",
        "    layer = tf.keras.layers.Flatten()(layer)\n",
        "    mean = tf.keras.layers.Dense(latent_dim)(layer)\n",
        "    logvar = tf.keras.layers.Dense(latent_dim)(layer)\n",
        "    encoder_model = tf.keras.models.Model(model_input, [mean, logvar])\n",
        "  \n",
        "    #Decoder\n",
        "    decoder_input = tf.keras.Input((latent_dim,))\n",
        "    layer = tf.keras.layers.Dense(np.prod(shape_before_flatten[1:]),\\\n",
        "                              activation = 'relu')(decoder_input)\n",
        "    layer = tf.keras.layers.Reshape(target_shape=shape_before_flatten[1:])(layer)\n",
        "    layer = tf.keras.layers.Conv2DTranspose(filters = 32, kernel_size = 3, strides = 2, activation = 'relu', padding = 'same')(layer)\n",
        "    layer = tf.keras.layers.Conv2DTranspose(filters = 16, kernel_size = 3, strides = 2, activation = 'relu', padding = 'same')(layer)\n",
        "    layer = tf.keras.layers.Conv2DTranspose(filters = 1, kernel_size = 3, strides = 1, activation = 'sigmoid', padding = 'same')(layer)\n",
        "    decoder_model=tf.keras.models.Model(decoder_input, layer)  \n",
        " \n",
        "    #Reparameterization Trick\n",
        "    mean, logvar = encoder_model(model_input)\n",
        "    z = reparameterize(mean, logvar)\n",
        "    model_out = decoder_model(z)\n",
        "    model = tf.keras.models.Model(model_input,model_out)\n",
        "\n",
        "    #Reconstruction loss\n",
        "    reconstruction_loss = K.sum(K.binary_crossentropy(model_input,\n",
        "                                                    model_out), axis = [1, 2, 3])\n",
        "    #KL div loss\n",
        "    kl_loss = - 0.5 * K.sum(1 + logvar - K.square(mean) - K.exp(0.5 * logvar), axis = -1)\n",
        "    elbo = K.mean(reconstruction_loss + kl_loss)\n",
        "    \n",
        "    model.add_loss(elbo)\n",
        "    return model, decoder_model, encoder_model\n",
        "\n",
        "filepath = '/content/drive/MyDrive/PRNN/checkpoint/weights_{epoch:02d}.hdf5'\n",
        "vae_model,decoder_model,encoder_model = model(input_shape,latent_dim)\n",
        "vae_model.compile(optimizer = tf.keras.optimizers.Adam(5e-3))\n",
        "callback1 = tf.keras.callbacks.EarlyStopping(monitor = 'loss', patience = 10)\n",
        "callback2 = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='loss', verbose=0, save_freq=588)\n",
        "history = vae_model.fit(train_dataset, epochs = epochs, callbacks = [callback1, callback2], steps_per_epoch = (X_train.shape[0]//batch_size))\n",
        "\n",
        "#Plot the model training loss\n",
        "loss_vae = history.history['loss']\n",
        "plt.plot(loss_vae)\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"VAE Training loss\")\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "#Testing using decoder\n",
        "img_size=(h_image,w_image)\n",
        "rows = 8\n",
        "cols = 8\n",
        "\n",
        "fig1, axes = plt.subplots(nrows=rows, ncols=cols, figsize=(5,5))\n",
        "for idx in range(rows*cols):\n",
        "    image = X_test[idx, :, :,0]\n",
        "    row = idx // cols\n",
        "    col = idx % cols\n",
        "    axes[row, col].axis(\"off\")\n",
        "    axes[row, col].imshow(image, cmap=\"gray\", aspect=\"auto\")\n",
        "plt.subplots_adjust(wspace=.01, hspace=.01)\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\\n\\n\\n\")\n",
        "\n",
        "fig2, axes = plt.subplots(nrows=rows, ncols=cols, figsize=(5,5))\n",
        "for idx in range(rows*cols):\n",
        "    mean, logvar = encoder_model.predict(np.expand_dims(X_test[idx, :, :, 0],axis=0))\n",
        "    z = reparameterize(mean, logvar)\n",
        "    output = decoder_model.predict(z)\n",
        "    image = tf.reshape(output,img_size)\n",
        "    row = idx // cols\n",
        "    col = idx % cols\n",
        "    axes[row, col].axis(\"off\")\n",
        "    axes[row, col].imshow(image, cmap=\"gray\", aspect=\"auto\")\n",
        "plt.subplots_adjust(wspace=.01, hspace=.01)\n",
        "plt.show()\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n",
        "from torch.nn.functional import adaptive_avg_pool2d\n",
        "import torchvision.models as models\n",
        "import torch.nn.functional as F\n",
        "from scipy import linalg\n",
        "\n",
        "class InceptionV3(nn.Module):\n",
        "    \"\"\"Pretrained InceptionV3 network returning feature maps\"\"\"\n",
        "\n",
        "    # Index of default block of inception to return,\n",
        "    # corresponds to output of final average pooling\n",
        "    DEFAULT_BLOCK_INDEX = 3\n",
        "\n",
        "    # Maps feature dimensionality to their output blocks indices\n",
        "    BLOCK_INDEX_BY_DIM = {\n",
        "        64: 0,   # First max pooling features\n",
        "        192: 1,  # Second max pooling featurs\n",
        "        768: 2,  # Pre-aux classifier features\n",
        "        2048: 3  # Final average pooling features\n",
        "    }\n",
        "\n",
        "    def __init__(self,\n",
        "                 output_blocks=[DEFAULT_BLOCK_INDEX],\n",
        "                 resize_input=True,\n",
        "                 normalize_input=True,\n",
        "                 requires_grad=False):\n",
        "        \n",
        "        super(InceptionV3, self).__init__()\n",
        "\n",
        "        self.resize_input = resize_input\n",
        "        self.normalize_input = normalize_input\n",
        "        self.output_blocks = sorted(output_blocks)\n",
        "        self.last_needed_block = max(output_blocks)\n",
        "\n",
        "        assert self.last_needed_block <= 3, \\\n",
        "            'Last possible output block index is 3'\n",
        "\n",
        "        self.blocks = nn.ModuleList()\n",
        "\n",
        "        \n",
        "        inception = models.inception_v3(pretrained=True)\n",
        "\n",
        "        # Block 0: input to maxpool1\n",
        "        block0 = [\n",
        "            inception.Conv2d_1a_3x3,\n",
        "            inception.Conv2d_2a_3x3,\n",
        "            inception.Conv2d_2b_3x3,\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2)\n",
        "        ]\n",
        "        self.blocks.append(nn.Sequential(*block0))\n",
        "\n",
        "        # Block 1: maxpool1 to maxpool2\n",
        "        if self.last_needed_block >= 1:\n",
        "            block1 = [\n",
        "                inception.Conv2d_3b_1x1,\n",
        "                inception.Conv2d_4a_3x3,\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2)\n",
        "            ]\n",
        "            self.blocks.append(nn.Sequential(*block1))\n",
        "\n",
        "        # Block 2: maxpool2 to aux classifier\n",
        "        if self.last_needed_block >= 2:\n",
        "            block2 = [\n",
        "                inception.Mixed_5b,\n",
        "                inception.Mixed_5c,\n",
        "                inception.Mixed_5d,\n",
        "                inception.Mixed_6a,\n",
        "                inception.Mixed_6b,\n",
        "                inception.Mixed_6c,\n",
        "                inception.Mixed_6d,\n",
        "                inception.Mixed_6e,\n",
        "            ]\n",
        "            self.blocks.append(nn.Sequential(*block2))\n",
        "\n",
        "        # Block 3: aux classifier to final avgpool\n",
        "        if self.last_needed_block >= 3:\n",
        "            block3 = [\n",
        "                inception.Mixed_7a,\n",
        "                inception.Mixed_7b,\n",
        "                inception.Mixed_7c,\n",
        "                nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
        "            ]\n",
        "            self.blocks.append(nn.Sequential(*block3))\n",
        "\n",
        "        for param in self.parameters():\n",
        "            param.requires_grad = requires_grad\n",
        "\n",
        "    def forward(self, inp):\n",
        "        \"\"\"Get Inception feature maps\n",
        "        Parameters\n",
        "        ----------\n",
        "        inp : torch.autograd.Variable\n",
        "            Input tensor of shape Bx3xHxW. Values are expected to be in\n",
        "            range (0, 1)\n",
        "        Returns\n",
        "        -------\n",
        "        List of torch.autograd.Variable, corresponding to the selected output\n",
        "        block, sorted ascending by index\n",
        "        \"\"\"\n",
        "        outp = []\n",
        "        x = inp\n",
        "\n",
        "        if self.resize_input:\n",
        "            x = F.interpolate(x,\n",
        "                              size=(299, 299),\n",
        "                              mode='bilinear',\n",
        "                              align_corners=False)\n",
        "\n",
        "        if self.normalize_input:\n",
        "            x = 2 * x - 1  # Scale from range (0, 1) to range (-1, 1)\n",
        "\n",
        "        for idx, block in enumerate(self.blocks):\n",
        "            x = block(x)\n",
        "            if idx in self.output_blocks:\n",
        "                outp.append(x)\n",
        "\n",
        "            if idx == self.last_needed_block:\n",
        "                break\n",
        "\n",
        "        return outp\n",
        "    \n",
        "block_idx = InceptionV3.BLOCK_INDEX_BY_DIM[2048]\n",
        "fid_model = InceptionV3([block_idx])\n",
        "fid_model = fid_model.cuda()\n",
        "\n",
        "def calculate_activation_statistics(images,model,batch_size=128, dims=2048,\n",
        "                    cuda=False):\n",
        "    model.eval()\n",
        "    act=np.empty((len(images), dims))\n",
        "    \n",
        "    if cuda:\n",
        "        batch=images.cuda()\n",
        "    else:\n",
        "        batch=images\n",
        "    pred = model(batch)[0]\n",
        "\n",
        "        # If model output is not scalar, apply global spatial average pooling.\n",
        "        # This happens if you choose a dimensionality not equal 2048.\n",
        "    if pred.size(2) != 1 or pred.size(3) != 1:\n",
        "        pred = adaptive_avg_pool2d(pred, output_size=(1, 1))\n",
        "\n",
        "    act= pred.cpu().data.numpy().reshape(pred.size(0), -1)\n",
        "    \n",
        "    mu = np.mean(act, axis=0)\n",
        "    sigma = np.cov(act, rowvar=False)\n",
        "    return mu, sigma\n",
        "\n",
        "def calculate_frechet_distance(mu1, sigma1, mu2, sigma2, eps=1e-6):\n",
        "    \"\"\"Numpy implementation of the Frechet Distance.\n",
        "    The Frechet distance between two multivariate Gaussians X_1 ~ N(mu_1, C_1)\n",
        "    and X_2 ~ N(mu_2, C_2) is\n",
        "            d^2 = ||mu_1 - mu_2||^2 + Tr(C_1 + C_2 - 2*sqrt(C_1*C_2)).\n",
        "    \"\"\"\n",
        "\n",
        "    mu1 = np.atleast_1d(mu1)\n",
        "    mu2 = np.atleast_1d(mu2)\n",
        "\n",
        "    sigma1 = np.atleast_2d(sigma1)\n",
        "    sigma2 = np.atleast_2d(sigma2)\n",
        "\n",
        "    assert mu1.shape == mu2.shape, \\\n",
        "        'Training and test mean vectors have different lengths'\n",
        "    assert sigma1.shape == sigma2.shape, \\\n",
        "        'Training and test covariances have different dimensions'\n",
        "\n",
        "    diff = mu1 - mu2\n",
        "\n",
        "    \n",
        "    covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n",
        "    if not np.isfinite(covmean).all():\n",
        "        msg = ('fid calculation produces singular product; '\n",
        "               'adding %s to diagonal of cov estimates') % eps\n",
        "        print(msg)\n",
        "        offset = np.eye(sigma1.shape[0]) * eps\n",
        "        covmean = linalg.sqrtm((sigma1 + offset).dot(sigma2 + offset))\n",
        "\n",
        "    \n",
        "    if np.iscomplexobj(covmean):\n",
        "        if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):\n",
        "            m = np.max(np.abs(covmean.imag))\n",
        "            raise ValueError('Imaginary component {}'.format(m))\n",
        "        covmean = covmean.real\n",
        "\n",
        "    tr_covmean = np.trace(covmean)\n",
        "\n",
        "    return (diff.dot(diff) + np.trace(sigma1) +\n",
        "            np.trace(sigma2) - 2 * tr_covmean)\n",
        "\n",
        "def calculate_fretchet(images_real,images_fake,model):\n",
        "     mu_1,std_1=calculate_activation_statistics(images_real,model,cuda=True)\n",
        "     mu_2,std_2=calculate_activation_statistics(images_fake,model,cuda=True)\n",
        "    \n",
        "     \"\"\"get fretched distance\"\"\"\n",
        "     fid_value = calculate_frechet_distance(mu_1, std_1, mu_2, std_2)\n",
        "     return fid_value\n",
        "\n",
        "block_idx = InceptionV3.BLOCK_INDEX_BY_DIM[2048]\n",
        "model = InceptionV3([block_idx])\n",
        "model = model.cuda()\n",
        "avg_fid_score = 0.0\n",
        "img_size=(h_image,w_image)\n",
        "\n",
        "for i in range(10):\n",
        "    mean, logvar = encoder_model.predict(np.expand_dims(X_test[i, :, :, 0],axis=0))\n",
        "    z = reparameterize(mean, logvar)\n",
        "    output = decoder_model.predict(z)\n",
        "    output = tf.reshape(output,img_size)\n",
        "    c = resize(np.asarray(X_test[i, :, :, 0]), (64, 64), anti_aliasing=True)\n",
        "    d = resize(np.asarray(output), (64, 64), anti_aliasing=True)\n",
        "    e = np.zeros((3, 64, 64))\n",
        "    for i in range(3):\n",
        "        e[i, :, :] = c\n",
        "    f = np.zeros((3, 64, 64))\n",
        "    for i in range(3):\n",
        "        f[i, :, :] = d\n",
        "    a = torch.from_numpy(e)\n",
        "    b = torch.from_numpy(f)\n",
        "    a = torch.from_numpy(np.expand_dims(a, 0)).float()\n",
        "    b = torch.from_numpy(np.expand_dims(b, 0)).float()\n",
        "    avg_fid_score += calculate_fretchet(a, b, model)\n",
        "print(avg_fid_score/10.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H7K453xqrCTD"
      },
      "outputs": [],
      "source": [
        "#PCA\n",
        "\"\"\"PCA for PneumoniaMNIST dataset\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "def process_data(path):\n",
        "    dataset = np.load(path)\n",
        "    train_x, train_y = dataset['train_images'], dataset['train_labels']\n",
        "    val_x, val_y = dataset['val_images'], dataset['val_labels']\n",
        "    test_x, test_y = dataset['test_images'], dataset['test_labels']\n",
        "    return train_x, train_y, test_x, test_y\n",
        "\n",
        "def pca(X_train, num_components):\n",
        "\n",
        "    X_train = X_train.reshape((X_train.shape[0], X_train.shape[1]**2))\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    scaler.fit(X_train)\n",
        "    X_train_scaled = scaler.transform(X_train)\n",
        "\n",
        "    pca = PCA(n_components=num_components)\n",
        "    x_ = pca.fit_transform(X_train_scaled)\n",
        "    return x_\n",
        "\n",
        "def plot_PCA(x_, y_train, plot_points, num_components):\n",
        "    if(num_components == 3):\n",
        "        fig = plt.figure()\n",
        "        ax = fig.add_subplot(projection='3d')\n",
        "        sc = ax.scatter(x_[0:plot_points, 0], x_[0:plot_points, 1], x_[0:plot_points, 2], c=y_train[0:plot_points], marker='o', cmap='coolwarm', alpha=1)\n",
        "        plt.legend(*sc.legend_elements(), bbox_to_anchor=(1.05, 1), loc=2)\n",
        "        plt.savefig(\"3d.png\")\n",
        "        plt.title(\"PCA with 3 principal components\")\n",
        "        ax.set_xlabel(\"PC1\")\n",
        "        ax.set_ylabel(\"PC2\")\n",
        "        ax.set_zlabel(\"PC3\")\n",
        "    else:\n",
        "        sc = plt.scatter(x_[0:plot_points, 0], x_[0:plot_points, 1], c=y_train[0:plot_points], marker='o', cmap='coolwarm', alpha=1)\n",
        "        plt.legend(*sc.legend_elements(), bbox_to_anchor=(1.05, 1), loc=2)\n",
        "        plt.savefig(\"2d.png\")\n",
        "        plt.title(\"PCA with 2 principal components\") \n",
        "        plt.xlabel(\"PC1\")\n",
        "        plt.ylabel(\"PC2\") \n",
        "    plt.grid()      \n",
        "    plt.show()\n",
        "    return\n",
        "\n",
        "def main():\n",
        "    path = 'drive/MyDrive/PRNN/pneumoniamnist.npz'\n",
        "    X_train, y_train, test_x, test_y = process_data(path)\n",
        "    n = 3\n",
        "    x = pca(X_train, num_components=n)\n",
        "    plot_PCA(x, y_train, plot_points=60000, num_components=n)\n",
        "    return\n",
        "\n",
        "if __name__=='__main__':\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-OECU4NXrGd4"
      },
      "outputs": [],
      "source": [
        "#tsne for pneumoniamnist\n",
        "\n",
        "data = np.load(\"../input/pneumoniamnist/pneumoniamnist.npz\" )\n",
        "train_x = data[\"train_images\"]\n",
        "train_y = data[\"train_labels\"]\n",
        "X = np.reshape(train_x,(4708,784))/255.0\n",
        "y = np.reshape(train_y,(4708))\n",
        "\n",
        "print(X.shape)\n",
        "print(y.shape)\n",
        "\n",
        "# Commented out IPython magic to ensure Python compatibility.\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "# %matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import seaborn as sns\n",
        "\n",
        "feat_cols = [ 'pixel'+str(i) for i in range(X.shape[1]) ]\n",
        "df = pd.DataFrame(X,columns=feat_cols)\n",
        "df['y'] = y\n",
        "df['label'] = df['y'].apply(lambda i: str(i))\n",
        "X, y = None, None\n",
        "print('Size of the dataframe: {}'.format(df.shape))\n",
        "\n",
        "# For reproducability of the results\n",
        "np.random.seed(42)\n",
        "rndperm = np.random.permutation(df.shape[0])\n",
        "\n",
        "plt.gray()\n",
        "fig = plt.figure( figsize=(16,7) )\n",
        "for i in range(0,15):\n",
        "    ax = fig.add_subplot(3,5,i+1, title=\"Digit: {}\".format(str(df.loc[rndperm[i],'label'])) )\n",
        "    ax.matshow(df.loc[rndperm[i],feat_cols].values.reshape((28,28)).astype(float))\n",
        "plt.show()\n",
        "\n",
        "N = 4708\n",
        "df_subset = df.loc[rndperm[:N],:].copy()\n",
        "data_subset = df_subset[feat_cols].values\n",
        "#print(df_subset)\n",
        "print(df_subset['label'].shape)\n",
        "tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\n",
        "tsne_results = tsne.fit_transform(data_subset)\n",
        "\n",
        "df_subset['tsne-2d-one'] = tsne_results[:,0]\n",
        "df_subset['tsne-2d-two'] = tsne_results[:,1]\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.scatterplot(\n",
        "    x=\"tsne-2d-one\", y=\"tsne-2d-two\",\n",
        "    hue=\"y\",\n",
        "    palette=sns.color_palette(\"hls\", 2),\n",
        "    data=df_subset,\n",
        "    legend=\"full\",\n",
        "    alpha=0.3\n",
        ")\n",
        "\n",
        "import plotly.express as px\n",
        "labels = df_subset[\"label\"]\n",
        "\n",
        "df0 = pd.DataFrame({\n",
        "'label': np.squeeze(labels), 'col_x':tsne_results[:,0], 'col_y':tsne_results[:,1]\n",
        "})\n",
        "\n",
        "fig = px.scatter(df0, x='col_x', y='col_y', color='label',\n",
        "                 width=700, height=500,\n",
        "                 title=\"2D Scatter Plot\")\n",
        "fig.show()\n",
        "\n",
        "tsne1 = TSNE(n_components=3, verbose=1, perplexity=40, n_iter=300)\n",
        "tsne1_results = tsne1.fit_transform(data_subset)\n",
        "print(tsne_results.shape)\n",
        "\n",
        "labels = df_subset[\"label\"]\n",
        "print(np.squeeze(tsne1_results[:,0]).shape)\n",
        "print(np.squeeze(labels).shape)\n",
        "df1 =  pd.DataFrame({\n",
        "'label': np.squeeze(labels), 'col_x':tsne1_results[:,0], 'col_y':tsne1_results[:,1], 'col_z':tsne1_results[:,2]\n",
        "})\n",
        "\n",
        "fig = px.scatter_3d(df1, x='col_x', y='col_y', z='col_z',\n",
        "                    color='label',\n",
        "                    title=\"3D Scatter Plot\")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WNP_2aGGrOsC"
      },
      "outputs": [],
      "source": [
        "#PCA reconstruction\n",
        "#in this code we will use PCA to reduce the dimension of the data\n",
        "# and then we will use the reduced data to reconstruct the original data\n",
        "#this is an experiment to see how much reconstruction error we can get\n",
        "# we will pneumoniamnist.npy dataset\n",
        "\n",
        "\n",
        "#Global variable\n",
        "\n",
        "# path = 'drive/MyDrive/Colab Notebooks/PRNN_A1_DATA/'\n",
        "import os\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "path = os.getcwd() + '/'\n",
        "\n",
        "#importing Libraries\n",
        "import numpy as np\n",
        "from matplotlib.image import imread\n",
        "import matplotlib.pyplot as plt\n",
        "# import cv2\n",
        "import scipy.linalg as la\n",
        "\n",
        "from sklearn.decomposition import PCA, IncrementalPCA\n",
        "\n",
        "#pre-processing the data\n",
        "\n",
        "\n",
        "def data_preprocessing():\n",
        "    data_path = path + 'pneumoniamnist.npz'\n",
        "    data = np.load(data_path)\n",
        "    # print(data.files)\n",
        "    # Reshape images and save in numpy arrays\n",
        "    train1_data = np.reshape(data['train_images'], (4708, 784))\n",
        "\n",
        "\n",
        "    train1_data = train1_data.astype(float)\n",
        "    data_vectors = train1_data\n",
        "\n",
        "    return data_vectors\n",
        "\n",
        "#form the data covariance matrix\n",
        "\n",
        "def data_cov_matrix(data_vectors):\n",
        "    data_covariance_matrix = np.cov(data_vectors.T)\n",
        "\n",
        "    #plot all the eigenvalues largest to smallest\n",
        "    eigenvalues, eigenvectors = la.eig(data_covariance_matrix)\n",
        "\n",
        "    #print number of eigenvalues\n",
        "    print('Number of eigenvalues: ', len(eigenvalues))\n",
        "    # #plot eigen values on scale of 10^(5)\n",
        "    # temp = eigenvalues*(10**(-5))\n",
        "    # plt.plot(temp)\n",
        "    # plt.title('Eigenvalues of Data Covariance matrix' )\n",
        "    # #y axis label\n",
        "    # plt.ylabel('Eigenvalues x 10^5')\n",
        "    # plt.show()\n",
        "\n",
        "    #plottying residual error\n",
        "    #making a vector J same length as eigenvalues and is generated as\n",
        "    # J_i = sum(eigenvalues[i+1:])\n",
        "    J = np.zeros(len(eigenvalues))\n",
        "    for i in range(len(eigenvalues)):\n",
        "        J[i] = sum(eigenvalues[i+1:])\n",
        "\n",
        "    #plotting eigen values and residual error in same plot\n",
        "    plt.figure(figsize=(10,10))\n",
        "    plt.subplot(1,2,1)\n",
        "    temp_eigenvalues = eigenvalues*(10**(-5))\n",
        "    plt.plot(temp_eigenvalues)\n",
        "    plt.title('Eigenvalues of Data Covariance matrix')\n",
        "    plt.ylabel('Eigenvalues x 10^5')\n",
        "    #x axis label i\n",
        "    plt.xlabel('i')\n",
        "    plt.subplot(1,2,2)\n",
        "    temp_J = J*(10**(-5))\n",
        "    plt.plot(temp_J)\n",
        "    plt.title('Residual Error ')\n",
        "    plt.ylabel('Residual Error x 10^5')\n",
        "    plt.xlabel('M')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    return data_covariance_matrix\n",
        "\n",
        "\n",
        "#plotting eigenvectors as images\n",
        "\n",
        "def eigen_vectors_images(data_covariance_matrix):\n",
        "    eigenvalues, eigenvectors = la.eig(data_covariance_matrix)\n",
        "    #print number of eigenvalues\n",
        "    # print('Number of eigenvalues: ', len(eigenvalues))\n",
        "    #plot eigen values on scale of 10^(5)\n",
        "    # temp = eigenvalues*(10**(-5))\n",
        "    # plt.plot(temp)\n",
        "    # plt.title('Eigenvalues of Data Covariance matrix' )\n",
        "    # #y axis label\n",
        "    # plt.ylabel('Eigenvalues x 10^5')\n",
        "    # plt.show()\n",
        "\n",
        "    #plotting eigen vectors as images\n",
        "    #we will plot the first 10 eigen vectors in same plot\n",
        "    plt.figure(figsize=(10,10))\n",
        "    for i in range(10):\n",
        "        plt.subplot(2,5,i+1)\n",
        "        plt.imshow(eigenvectors[:,i].reshape(28,28), cmap='gray')\n",
        "        plt.title('Eigenvector ' + str(i+1))\n",
        "        plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# def PCA_reconstruction(input_vector, k, ):\n",
        "#\n",
        "#     #here input vector is 1 single vector of data set\n",
        "# #k is the number of eigenvectors we want to use for first PCA step\n",
        "#     #we will use the top k eigenvectors to reconstruct the input vector\n",
        "#\n",
        "#     #getting the t\n",
        "\n",
        "\n",
        "def return_U_matrix(eigenvectors, k):\n",
        "    #here eigenvectors is the matrix of eigenvectors\n",
        "    #k is the number of eigenvectors we want to use for first PCA step\n",
        "    #we will use the top k eigenvectors to reconstruct the input vector\n",
        "    U = eigenvectors[:, :k]\n",
        "    return U\n",
        "\n",
        "\n",
        "def forward_PCA(input_vector, k, eigenvectors):\n",
        "    #here input vector is 1 single vector of data set\n",
        "    #k is the number of eigenvectors we want to use for first PCA step\n",
        "    #making U^T\n",
        "    U_transpose = return_U_matrix(eigenvectors, k).T\n",
        "    y = U_transpose.dot(input_vector)\n",
        "    return y\n",
        "\n",
        "def reconstruct_PCA(y, k, eigenvectors):\n",
        "    #y is a single output vector\n",
        "#k is the number of eigenvectors we want to use for first PCA step\n",
        "    #making U\n",
        "    U = return_U_matrix(eigenvectors, k)\n",
        "    x = U.dot(y)\n",
        "    return x\n",
        "\n",
        "\n",
        "def reconstruction(k, image_vector, eigenvectors):\n",
        "    #forward PCA\n",
        "    y = forward_PCA(image_vector, k, eigenvectors)\n",
        "    #reconstruction\n",
        "    x = reconstruct_PCA(y, k, eigenvectors)\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#doing PCA on the data\n",
        "def PCA(data_vectors):\n",
        "\n",
        "    #mean centre the data\n",
        "    mean_centre_data = data_vectors - np.mean(data_vectors, axis=0)\n",
        "    data_vectors = mean_centre_data\n",
        "    #get the covariance matrix\n",
        "    data_covariance_matrix = np.cov(data_vectors.T)\n",
        "    #get the eigenvalues and eigenvectors\n",
        "    eigenvalues, eigenvectors = la.eig(data_covariance_matrix)\n",
        "    #sort the eigenvalues and eigenvectors in descending order\n",
        "    eigenvalues = np.sort(eigenvalues)[::-1]\n",
        "    eigenvectors = eigenvectors[:, eigenvalues.argsort()[::-1]]\n",
        "\n",
        "    #print eigenvalues\n",
        "    # print('Eigenvalues of Data Covariance matrix: ', eigenvalues)\n",
        "    #print number of eigenvalues\n",
        "    # print('Number of eigenvalues: ', len(eigenvalues))\n",
        "    #print eigrnvectors\n",
        "\n",
        "    # print('Eigenvectors of Data Covariance matrix: ', eigenvectors)\n",
        "    #print first eigenvector\n",
        "    #print shape of eigenvectors\n",
        "    # print('Shape of eigenvectors: ', eigenvectors.shape)\n",
        "    # print('First Eigenvector of Data Covariance matrix: ', eigenvectors[:,0])\n",
        "\n",
        "    #original image\n",
        "    image_vector = data_vectors[0]\n",
        "\n",
        "    k_values = [1 , 2 , 10,  50, 100, 500, 784]\n",
        "\n",
        "    #getting the reconstructed images for all k\n",
        "    reconstructed_images = []\n",
        "    for k in k_values:\n",
        "        reconstructed_images.append(reconstruction(k, image_vector, eigenvectors))\n",
        "\n",
        "    #calculating MSE for all k\n",
        "    MSE_values = []\n",
        "    for i in range(len(reconstructed_images)):\n",
        "        MSE_values.append(np.mean((reconstructed_images[i] - image_vector)**2))\n",
        "\n",
        "    # calculating the reconstruction error for all k\n",
        "    reconstruction_error = []\n",
        "    for i in range(len(reconstructed_images)):\n",
        "        #reconstruction error is the sum of eigenvalues from k+1 to 784\n",
        "        reconstruction_error.append(np.sum(eigenvalues[k_values[i]+1:]))\n",
        "\n",
        "    #plotting the reconstructed images and the original image\n",
        "    plt.figure(figsize=(10,10))\n",
        "    for i in range(len(reconstructed_images)+1):\n",
        "        plt.subplot(1,8,i+1)\n",
        "        #if i is 0 then plot the original image\n",
        "        if i == 0:\n",
        "            plt.imshow(image_vector.reshape(28,28), cmap='gray')\n",
        "            plt.title('Original')\n",
        "            plt.axis('off')\n",
        "        #if i is not 0 then plot the reconstructed image\n",
        "        else:\n",
        "            plt.imshow(reconstructed_images[i-1].reshape(28,28), cmap='gray')\n",
        "            plt.title('k=' + str(k_values[i-1]))\n",
        "            plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    #plotting MSE vs k\n",
        "    plt.figure(figsize=(10,10))\n",
        "    #plot line graph\n",
        "    plt.plot(k_values, MSE_values)\n",
        "    plt.xlabel('k')\n",
        "    plt.ylabel('MSE')\n",
        "    plt.title('MSE vs k')\n",
        "    plt.show()\n",
        "\n",
        "    #plotting reconstruction error vs k\n",
        "    plt.figure(figsize=(10,10))\n",
        "    #plot line graph\n",
        "    plt.plot(k_values, reconstruction_error)\n",
        "    plt.xlabel('k')\n",
        "    plt.ylabel('Reconstruction Error')\n",
        "    plt.title('Reconstruction Error vs k')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    data_vectors = data_preprocessing()\n",
        "    # data_covariance_matrix = data_cov_matrix(data_vectors)\n",
        "    # eigen_vectors_images(data_covariance_matrix)\n",
        "\n",
        "    PCA(data_vectors)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kF19eLcnrjjE"
      },
      "outputs": [],
      "source": [
        "#KMeans\n",
        "#this code will run kmeans clustering on the data and then use the kmeans clusters to find the nearest neighbors of the test data\n",
        "#the cluster centers will be used as the representative points for the kmeans clusters\n",
        "# we will test for various values of k and find the best k value\n",
        "\n",
        "\n",
        "#import the packages\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "#metrics\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import roc_curve\n",
        "#Global variable\n",
        "\n",
        "\n",
        "# path = 'drive/MyDrive/Colab Notebooks/PRNN_A1_DATA/'\n",
        "import os\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "path = os.getcwd() + '/'\n",
        "\n",
        "\n",
        "#data preprocessing\n",
        "def data_preprocessing():\n",
        "    data_path = path + 'pneumoniamnist.npz'\n",
        "    data = np.load(data_path)\n",
        "    # print(data.files)\n",
        "    # Reshape images and save in numpy arrays\n",
        "    train1_data = np.reshape(data['train_images'], (4708, 784))\n",
        "    train1_labels = np.reshape(data['train_labels'], (4708))\n",
        "    #change data type to float of train1_data\n",
        "    train1_data = train1_data.astype(float)\n",
        "    #change datatype of train1_labels to int\n",
        "    train1_labels = train1_labels.astype(int)\n",
        "\n",
        "    #validation data\n",
        "    valid1_data = np.reshape(data['val_images'], (524, 784))\n",
        "    valid1_labels = np.reshape(data['val_labels'], (524))\n",
        "    #change datatype of valid1_data to float\n",
        "    valid1_data = valid1_data.astype(float)\n",
        "    #change datatype of valid1_labels to int\n",
        "    valid1_labels = valid1_labels.astype(int)\n",
        "\n",
        "\n",
        "    test1_data = np.reshape(data['test_images'], (624, 784))\n",
        "    test1_labels = np.reshape(data['test_labels'], (624))\n",
        "    #change data type to float of test1_data\n",
        "    test1_data = test1_data.astype(float)\n",
        "    #change datatype of test1_labels to int\n",
        "    test1_labels = test1_labels.astype(int)\n",
        "\n",
        "    # feature scaling\n",
        "    sc = StandardScaler()\n",
        "    X_train = sc.fit_transform(train1_data , train1_labels)\n",
        "    X_test = sc.fit_transform(test1_data, test1_labels)\n",
        "    X_valid = sc.fit_transform(valid1_data, valid1_labels)\n",
        "\n",
        "    Y_train = train1_labels\n",
        "\n",
        "    Y_test = test1_labels\n",
        "\n",
        "    Y_valid = valid1_labels\n",
        "\n",
        "    return X_train, Y_train, X_test, Y_test , X_valid, Y_valid\n",
        "\n",
        "#kmeans clustering\n",
        "\n",
        "def kmeans_clustering(data, k):\n",
        "    #run kmeans clustering\n",
        "    kmeans = KMeans(n_clusters=k, random_state=0).fit(data)\n",
        "    labels = kmeans.labels_\n",
        "    centroids = kmeans.cluster_centers_\n",
        "\n",
        "    # #print kmeans\n",
        "    # print(kmeans)\n",
        "    # #print labels\n",
        "    # print(labels)\n",
        "    return kmeans, labels, centroids\n",
        "\n",
        "\n",
        "def get_centroid_in_dataset(centroids, X_train):\n",
        "    new_centroids = []\n",
        "    # using NearestNeighbors to find closed point to centroid in dataset\n",
        "    nbrs = NearestNeighbors(n_neighbors=1, algorithm='brute').fit(X_train)\n",
        "    distances, indices = nbrs.kneighbors(centroids)\n",
        "    # loop over indices\n",
        "    for i in range(len(indices)):\n",
        "        new_centroids.append(X_train[indices[i][0]])\n",
        "\n",
        "    return new_centroids\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#finding the y_label of the centroids\n",
        "def find_centroid_label(centroids, data , labels):\n",
        "    # find the nearest point to each centroid\n",
        "\n",
        "\n",
        "    centroids= get_centroid_in_dataset(centroids, data)\n",
        "    centroid_labels = []\n",
        "    for i in range(len(centroids)):\n",
        "        #search the data for the centroid\n",
        "        for j in range(len(data)):\n",
        "            if np.array_equal(centroids[i], data[j]):\n",
        "                centroid_labels.append(labels[j])\n",
        "\n",
        "\n",
        "    #list to numpy array\n",
        "    centroid_labels = np.array(centroid_labels)\n",
        "    return centroid_labels\n",
        "\n",
        "\n",
        "#testing find_centroid_label function\n",
        "def test_find_centroid_label(X_train, Y_train):\n",
        "\n",
        "\n",
        "    centroids = [X_train[0], X_train[1]]\n",
        "    #nparray\n",
        "    centroids = np.array(centroids)\n",
        "    centroid_labels = find_centroid_label(centroids, X_train, Y_train)\n",
        "    # print(centroid_labels)\n",
        "    #printing the centroid labels\n",
        "\n",
        "\n",
        "#generating classification report\n",
        "def classification_report_generator(labels, predicted_labels):\n",
        "    # calculating metrics\n",
        "    # accuracy\n",
        "    accuracy = accuracy_score(labels, predicted_labels)\n",
        "    # print(\"Accuracy: \", accuracy)\n",
        "    # confusion matrix\n",
        "    confusion_matrix_metric = confusion_matrix(labels, predicted_labels)\n",
        "    # print(\"Confusion Matrix: \\n\", confusion_matrix_metric)\n",
        "    # # plotting confusion matrix\n",
        "    # plt.matshow(confusion_matrix_metric)\n",
        "    # plt.title('Confusion Matrix')\n",
        "    # plt.colorbar()\n",
        "    # plt.ylabel('True label')\n",
        "    # plt.xlabel('Predicted label')\n",
        "    # plt.show()\n",
        "    # classification report\n",
        "    classification_report_metric = classification_report(labels, predicted_labels)\n",
        "    # print(\"Classification Report: \\n\", classification_report_metric)\n",
        "    # f1 score\n",
        "    f1_score_metric = f1_score(labels, predicted_labels, average='weighted', zero_division=True)\n",
        "    # print(\"F1 Score: \", f1_score_metric)\n",
        "    # plotting ROC curve\n",
        "    # fpr, tpr, thresholds = roc_curve(labels, predicted_labels)\n",
        "    roc_auc = roc_auc_score(labels, predicted_labels)\n",
        "    # plt.title('Receiver Operating Characteristic')\n",
        "    # plt.plot(fpr, tpr, 'b', label='AUC = %0.2f' % roc_auc)\n",
        "    # plt.legend(loc='lower right')\n",
        "    # plt.plot([0, 1], [0, 1], 'r--')\n",
        "    # plt.xlim([0, 1])\n",
        "    # plt.ylim([0, 1])\n",
        "    # plt.ylabel('True Positive Rate')\n",
        "    # plt.xlabel('False Positive Rate')\n",
        "    # plt.show()\n",
        "    # # printing AUC score\n",
        "    # print(\"AUC: \", roc_auc)\n",
        "\n",
        "    # saving the metric in dictionary\n",
        "    metrics = {'Accuracy': accuracy, 'Confusion Matrix': confusion_matrix_metric,\n",
        "               'Classification Report': classification_report_metric, 'F1 Score': f1_score_metric, 'ROC Curve': roc_auc,\n",
        "               'AUC': roc_auc}\n",
        "    return metrics\n",
        "\n",
        "# k nearest neighbour classifier\n",
        "def k_nearest_neighbour_classifier(data, labels, test_data, test_labels,k):\n",
        "\n",
        "    # print(data.shape)\n",
        "    # print(labels.shape)\n",
        "    data = np.array(data)\n",
        "    labels = np.array(labels)\n",
        "\n",
        "\n",
        "    #nearest neighbour classifier\n",
        "    neigh = KNeighborsClassifier(n_neighbors=k)\n",
        "    neigh.fit(data, labels)\n",
        "    #predict the labels\n",
        "    predicted_labels = neigh.predict(test_data)\n",
        "\n",
        "    #generating classification report\n",
        "    # print(\"k nearest neighbour classifier\")\n",
        "    #\n",
        "    # print(predicted_labels)\n",
        "    # print(test_labels)\n",
        "    # print(np.array(predicted_labels).shape)\n",
        "    # print(np.array(test_labels).shape)\n",
        "\n",
        "    metrics = classification_report_generator(test_labels, predicted_labels)\n",
        "\n",
        "    return predicted_labels, metrics\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def kmeans_knn_classifier(dataset, y_data, test_data, y_test,k):\n",
        "    #training phase\n",
        "    #run kmeans clustering\n",
        "    kmeans, labels, centroids = kmeans_clustering(dataset, k)\n",
        "    #find the centroid labels\n",
        "    centroid_labels = find_centroid_label(centroids, dataset , y_data)\n",
        "\n",
        "    # print(\"in kmeans_knn_classifier\")\n",
        "    # print(centroid_labels)\n",
        "\n",
        "    # creating augmented dataset using the centroid labels and centroids\n",
        "    data_augmented = centroids\n",
        "    labels_augmented = centroid_labels\n",
        "\n",
        "    # using k nearest neighbour classifier on augmented dataset\n",
        "    ret_val = k_nearest_neighbour_classifier(data_augmented, labels_augmented, test_data, y_test,k)\n",
        "\n",
        "\n",
        "    predicted_labels = ret_val[0]\n",
        "    metrics = ret_val[1]\n",
        "\n",
        "    # print(\"kmeans_knn_classifier\")\n",
        "    # print(predicted_labels)\n",
        "    # print(y_test)\n",
        "    # print(np.array(predicted_labels).shape)\n",
        "    # print(np.array(y_test).shape)\n",
        "\n",
        "\n",
        "    metrics = classification_report_generator(y_test, predicted_labels)\n",
        "    return predicted_labels , metrics\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#comparing the performance of these 2 different classifiers\n",
        "def compare_classifiers(dataset, labels, test_data, test_labels, k):\n",
        "\n",
        "    # using kmeans knn classifier\n",
        "    predicted_labels , metrics = kmeans_knn_classifier(dataset, labels, test_data, test_labels, k)\n",
        "\n",
        "\n",
        "    # using k nearest neighbour classifier\n",
        "    predicted_labels_knn, metrics_knn = k_nearest_neighbour_classifier(dataset, labels, test_data, test_labels,k)\n",
        "\n",
        "    #printing the accuracy of the classifiers\n",
        "    print(\"Accuracy of Kmeans KNN Classifier: \", metrics['Accuracy'])\n",
        "    print(\"Accuracy of KNN Classifier: \", metrics_knn['Accuracy'])\n",
        "\n",
        "    #f1 score of the classifiers\n",
        "    print(\"F1 Score of Kmeans KNN Classifier: \", metrics['F1 Score'])\n",
        "    print(\"F1 Score of KNN Classifier: \", metrics_knn['F1 Score'])\n",
        "\n",
        "    #plotting the confusion matrix of kmeans knn classifier\n",
        "    plt.matshow(metrics['Confusion Matrix'])\n",
        "    plt.title('Confusion Matrix Kmeans KNN Classifier k = ' + str(k))\n",
        "    plt.colorbar()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.show()\n",
        "\n",
        "    #plotting the confusion matrix\n",
        "    plt.matshow(metrics_knn['Confusion Matrix'])\n",
        "    plt.title('Confusion Matrix KNN Classifier k = ' + str(k))\n",
        "    plt.colorbar()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.show()\n",
        "\n",
        "    # #plot shape of labels\n",
        "    # print(np.array(predicted_labels).shape)\n",
        "    # print(np.array(test_labels).shape)\n",
        "\n",
        "    #plotting both ROCCurves on the same plot with different colors and labelled\n",
        "    fpr_kmeans, tpr_kmeans, thresholds_kmeans = roc_curve(test_labels, predicted_labels)\n",
        "    fpr_knn, tpr_knn, thresholds_knn = roc_curve(test_labels, predicted_labels_knn)\n",
        "    plt.title('Receiver Operating Characteristic')\n",
        "    plt.plot(fpr_kmeans, tpr_kmeans, 'b', label='Kmeans KNN AUC = %0.2f' % metrics['ROC Curve'])\n",
        "    plt.plot(fpr_knn, tpr_knn, 'r', label='KNN AUC = %0.2f' % metrics_knn['ROC Curve'])\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.plot([0, 1], [0, 1], 'r--')\n",
        "    plt.xlim([0, 1])\n",
        "    plt.ylim([0, 1])\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.show()\n",
        "\n",
        "    # # plotting ROC curve of kmeans knn classifier\n",
        "    # fpr_kmeans, tpr_kmeans, thresholds_kmeans = roc_curve(labels, predicted_labels)\n",
        "    # plt.title('Receiver Operating Characteristic')\n",
        "    # plt.plot(fpr_kmeans, tpr_kmeans, 'b', label='Kmeans KNN AUC = %0.2f' % metrics['ROC Curve'])\n",
        "    # plt.legend(loc='lower right')\n",
        "    # plt.plot([0, 1], [0, 1], 'r--')\n",
        "    # plt.xlim([0, 1])\n",
        "    # plt.ylim([0, 1])\n",
        "    # plt.ylabel('True Positive Rate')\n",
        "    # plt.xlabel('False Positive Rate')\n",
        "    # plt.show()\n",
        "\n",
        "\n",
        "    #printing the AUC score of the classifiers\n",
        "    print(\"AUC of Kmeans KNN Classifier: \", metrics['AUC'])\n",
        "    print(\"AUC of KNN Classifier: \", metrics_knn['AUC'])\n",
        "\n",
        "\n",
        "\n",
        "# now we will vary k and plot curves for accuracy and AUC at each k\n",
        "def vary_k_plot_curves(dataset, labels, test_data, test_labels):\n",
        "    # k = [1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10, 11,  15,  20, 50, 100, 500, 1000]\n",
        "    k = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 15, 20, 50, 75]\n",
        "    accuracy_kmeans_knn = []\n",
        "\n",
        "    auc_kmeans_knn = []\n",
        "    accuracy_knn = []\n",
        "    auc_knn = []\n",
        "\n",
        "    best_k_for_kmeans_knn = 1\n",
        "    best_k_for_knn = 1\n",
        "    temp_l1 , temp_m1 = kmeans_knn_classifier(dataset, labels, test_data, test_labels, best_k_for_kmeans_knn)\n",
        "    best_accuracy_for_kmeans_knn = temp_m1['Accuracy']\n",
        "    temp_l2 , temp_m2 = k_nearest_neighbour_classifier(dataset, labels, test_data, test_labels, best_k_for_knn)\n",
        "    best_accuracy_for_knn = temp_m2['Accuracy']\n",
        "\n",
        "\n",
        "\n",
        "    for i in k:\n",
        "        predicted_labels , metrics = kmeans_knn_classifier(dataset, labels, test_data, test_labels, i)\n",
        "        predicted_labels_knn, metrics_knn = k_nearest_neighbour_classifier(dataset, labels, test_data, test_labels,i)\n",
        "        accuracy_kmeans_knn.append(metrics['Accuracy'])\n",
        "        auc_kmeans_knn.append(metrics['AUC'])\n",
        "        accuracy_knn.append(metrics_knn['Accuracy'])\n",
        "        auc_knn.append(metrics_knn['AUC'])\n",
        "\n",
        "        if metrics['Accuracy'] > best_accuracy_for_kmeans_knn:\n",
        "            best_accuracy_for_kmeans_knn = metrics['Accuracy']\n",
        "            best_k_for_kmeans_knn = i\n",
        "        if metrics_knn['Accuracy'] > best_accuracy_for_knn:\n",
        "            best_accuracy_for_knn = metrics_knn['Accuracy']\n",
        "            best_k_for_knn = i\n",
        "\n",
        "\n",
        "    plt.plot(k, accuracy_kmeans_knn, 'b', label='Kmeans KNN Accuracy')\n",
        "    plt.plot(k, accuracy_knn, 'r', label='KNN Accuracy')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.xlabel('k')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.show()\n",
        "\n",
        "    plt.plot(k, auc_kmeans_knn, 'b', label='Kmeans KNN AUC')\n",
        "    plt.plot(k, auc_knn, 'r', label='KNN AUC')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.xlabel('k')\n",
        "    plt.ylabel('AUC')\n",
        "    plt.show()\n",
        "\n",
        "    return best_k_for_kmeans_knn, best_k_for_knn, best_accuracy_for_kmeans_knn, best_accuracy_for_knn\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "\n",
        "    X_train, Y_train, X_test, Y_test, X_valid, Y_valid = data_preprocessing()\n",
        "    # test_find_centroid_label(X_train, Y_train)\n",
        "\n",
        "\n",
        "    # kmeans_clustering(data_vectors, 3)\n",
        "    # compare_classifiers(X_train, Y_train, X_test,Y_test, 11)\n",
        "    best_k_for_kmeans_knn, best_k_for_knn, best_accuracy_for_kmeans_knn, best_accuracy_for_knn = vary_k_plot_curves(X_train, Y_train, X_test, Y_test)\n",
        "\n",
        "    #print best k\n",
        "    print(\"best_k_for_kmeans_knn\", best_k_for_kmeans_knn)\n",
        "    print(\"best_accuracy_for_kmeans_knn\", best_accuracy_for_kmeans_knn)\n",
        "    print(\"best_k_for_knn\", best_k_for_knn)\n",
        "    print(\"best_accuracy_for_knn\", best_accuracy_for_knn)\n",
        "\n",
        "    # plot\n",
        "    compare_classifiers(X_train, Y_train, X_test, Y_test, best_k_for_kmeans_knn)\n",
        "    compare_classifiers(X_train, Y_train, X_test, Y_test,best_k_for_knn)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y7zZEVFerpiv"
      },
      "outputs": [],
      "source": [
        "#Adaboost\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn import metrics\n",
        "\n",
        "from sklearn import tree\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "dtc = tree.DecisionTreeClassifier()\n",
        "svc = SVC(probability=True, kernel='linear')\n",
        "nnc = MLPClassifier(hidden_layer_sizes=(20,10))\n",
        "\n",
        "dtcA = AdaBoostClassifier(n_estimators=20, base_estimator=dtc, learning_rate=1)\n",
        "svcA = AdaBoostClassifier(n_estimators=5, base_estimator=svc, learning_rate=1)\n",
        "nncA = AdaBoostClassifier(n_estimators=20, base_estimator=nnc, learning_rate=1)\n",
        "\n",
        "dPnuModel = dtc.fit(pnuTrainX, pnuTrainY)\n",
        "dPnuTestR = dPnuModel.predict(pnuTestX).tolist()\n",
        "dPnuMatrix = confMatrix(pnuTestY, dPnuTestR, pnuClasses)\n",
        "print(\"Accuracy:\", accuracy(dPnuMatrix), \", f1-score:\", \"and auc:\", roc_auc_score(dAPnuTestR, pnuTestY)))\n",
        "\n",
        "sPnuModel = svc.fit(pnuTrainX, pnuTrainY)\n",
        "sPnuTestR = sPnuModel.predict(pnuTestX).tolist()\n",
        "sPnuMatrix = confMatrix(pnuTestY, sPnuTestR, pnuClasses)\n",
        "print(\"Accuracy:\", accuracy(sPnuMatrix), \", f1-score:\", \"and auc:\", roc_auc_score(dAPnuTestR, pnuTestY)))\n",
        "\n",
        "nPnuModel = nnc.fit(pnuTrainX, pnuTrainY)\n",
        "nPnuTestR = nPnuModel.predict(pnuTestX).tolist()\n",
        "nPnuMatrix = confMatrix(pnuTestY, nPnuTestR, pnuClasses)\n",
        "print(\"Accuracy:\", accuracy(nPnuMatrix), \", f1-score:\", \"and auc:\", roc_auc_score(dAPnuTestR, pnuTestY)))\n",
        "\n",
        "dAPnuModel = dtcA.fit(pnuTrainX, pnuTrainY)\n",
        "dAPnuTestR = dAPnuModel.predict(pnuTestX).tolist()\n",
        "dAPnuMatrix = confMatrix(pnuTestY, dAPnuTestR, pnuClasses)\n",
        "print(\"Accuracy:\", accuracy(dAPnuMatrix), \", f1-score:\", f1Score(dAPnuMatrix), \"and auc:\", roc_auc_score(dAPnuTestR, pnuTestY))\n",
        "\n",
        "sAPnuModel = svcA.fit(pnuTrainX, pnuTrainY)\n",
        "sAPnuTestR = sAPnuModel.predict(pnuTestX).tolist()\n",
        "sAPnuMatrix = confMatrix(pnuTestY, sAPnuTestR, pnuClasses)\n",
        "print(\"Accuracy:\", accuracy(sAPnuMatrix), \", f1-score:\", f1Score(sAPnuMatrix), \"and auc:\", roc_auc_score(sAPnuTestR, pnuTestY))\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "PRNN_A3_Nishanth_Rahul_Jeevithiesh_Bhartendu.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
