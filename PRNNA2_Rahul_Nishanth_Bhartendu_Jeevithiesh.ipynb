{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jp7CBJZlkqli"
      },
      "outputs": [],
      "source": [
        "#Q1 SVM\n",
        "# importing libraries ------------------------------------------------------------\n",
        "import matplotlib\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "# %matplotlib inline\n",
        "\n",
        "\n",
        "# for test dataset - not using any library functions\n",
        "\n",
        "from sklearn.datasets import make_blobs, make_circles, make_moons\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "\n",
        "# imports done-----------------------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "# (DUAL) objective function\n",
        "\n",
        "def objective_function(alphas, target, kernel, X_train):\n",
        "    \"\"\"Returns the SVM objective function based in the input model defined by:\n",
        "    `alphas`: vector of Lagrange multipliers\n",
        "    `target`: vector of class labels (-1 or 1) for training data\n",
        "    `kernel`: kernel function\n",
        "    `X_train`: training data for model.\"\"\"\n",
        "\n",
        "    return np.sum(alphas) - 0.5 * np.sum(\n",
        "        (target[:, None] * target[None, :]) * kernel(X_train, X_train) * (alphas[:, None] * alphas[None, :]))\n",
        "\n",
        "\n",
        "# Decision function for test data\n",
        "\n",
        "def decision_function(alphas, target, kernel, X_train, x_test, b):\n",
        "    \"\"\"Applies the SVM decision function to the input feature vectors in `x_test`.\"\"\"\n",
        "\n",
        "    result = (alphas * target) @ kernel(X_train, x_test) - b\n",
        "    return result\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#helper function for plotting the decision boundary\n",
        "# from sklearn.svm._libsvm import decision_function\n",
        "\n",
        "\n",
        "def plot_decision_boundary(model, ax, resolution=100, colors=('b', 'k', 'r'), levels=(-1, 0, 1)):\n",
        "    \"\"\"Plots the model's decision boundary on the input axes object.\n",
        "    Range of decision boundary grid is determined by the training data.\n",
        "    Returns decision boundary grid and axes object (`grid`, `ax`).\"\"\"\n",
        "\n",
        "    # Generate coordinate grid of shape [resolution x resolution]\n",
        "    # and evaluate the model over the entire space\n",
        "    xrange = np.linspace(model.X[:, 0].min(), model.X[:, 0].max(), resolution)\n",
        "    yrange = np.linspace(model.X[:, 1].min(), model.X[:, 1].max(), resolution)\n",
        "    grid = [[decision_function(model.alphas, model.y,\n",
        "                               model.kernel, model.X,\n",
        "                               np.array([xr, yr]), model.b) for xr in xrange] for yr in yrange]\n",
        "    grid = np.array(grid).reshape(len(xrange), len(yrange))\n",
        "\n",
        "    # Plot decision contours using grid and\n",
        "    # make a scatter plot of training data\n",
        "    ax.contour(xrange, yrange, grid, levels=levels, linewidths=(1, 1, 1),\n",
        "               linestyles=('--', '-', '--'), colors=colors)\n",
        "    ax.scatter(model.X[:, 0], model.X[:, 1],\n",
        "               c=model.y, cmap=plt.cm.viridis, lw=0, alpha=0.25)\n",
        "\n",
        "    # Plot support vectors (non-zero alphas)\n",
        "    # as circled points (linewidth > 0)\n",
        "    mask = np.round(model.alphas, decimals=2) != 0.0\n",
        "    ax.scatter(model.X[mask, 0], model.X[mask, 1],\n",
        "               c=model.y[mask], cmap=plt.cm.viridis, lw=1, edgecolors='k')\n",
        "\n",
        "    return grid, ax\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#defining the kernel functions --------------------------------------------------\n",
        "\n",
        "# the linearkenal function\n",
        "def linear_kernel(x, y, hyperparameter):\n",
        "    \"\"\"Returns the linear combination of arrays `x` and `y` with\n",
        "    the optional bias term `b` (set to 0 by default).\"\"\"\n",
        "    b = 0\n",
        "\n",
        "    return x @ y.T + b  # Note the @ operator for matrix multiplication\n",
        "\n",
        "#the gaussian kernel function\n",
        "def gaussian_kernel(x, y, hyperparameter= [1.0] ):\n",
        "    \"\"\"Returns the gaussian similarity of arrays `x` and `y` with\n",
        "    kernel width parameter `sigma` (set to 1 by default).\"\"\"\n",
        "\n",
        "    # sigma = 1.0\n",
        "    sigma = hyperparameter[0]\n",
        "    if np.ndim(x) == 1 and np.ndim(y) == 1:\n",
        "        result = np.exp(- (np.linalg.norm(x - y, 2)) ** 2 / (2 * sigma ** 2))\n",
        "    elif (np.ndim(x) > 1 and np.ndim(y) == 1) or (np.ndim(x) == 1 and np.ndim(y) > 1):\n",
        "        result = np.exp(- (np.linalg.norm(x - y, 2, axis=1) ** 2) / (2 * sigma ** 2))\n",
        "    elif np.ndim(x) > 1 and np.ndim(y) > 1:\n",
        "        result = np.exp(- (np.linalg.norm(x[:, np.newaxis] - y[np.newaxis, :], 2, axis=2) ** 2) / (2 * sigma ** 2))\n",
        "    return result\n",
        "\n",
        "\n",
        "# sigmoide kernal function\n",
        "def sigmoid_kernel(x, y, hyperparameter= [1.0] ):\n",
        "    # gamma = 1\n",
        "    gamma = hyperparameter[0]\n",
        "    return np.tanh(gamma * x @ y.T + 1)\n",
        "\n",
        "\n",
        "# polynomial kernel\n",
        "def polynomial_kernel(x, y,hyperparameter=[3, 1]):\n",
        "    # , gamma = 1, p = 3  degree, gamma\n",
        "    p = hyperparameter[0]\n",
        "    gamma = hyperparameter[1]\n",
        "    return ( (gamma * (x @ y.T )+ 1) ** p )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#kernals defined ------------------------------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#SVM MODEL Class\n",
        "\n",
        "class SMOModel:\n",
        "    \"\"\"Container object for the model used for sequential minimal optimization.\"\"\"\n",
        "\n",
        "    def __init__(self, X, y, C, tol, eps,  kernel, alphas, b, errors , hyperparams):\n",
        "        self.X = X  # training data vector\n",
        "        self.y = y  # class label vector\n",
        "        self.C = C  # regularization parameter\n",
        "        self.tol = tol  # error tolerance for stopping criterion\n",
        "        self.eps = eps  # tolerance for alpha update\n",
        "        self.kernel = kernel  # kernel function\n",
        "        self.alphas = alphas  # lagrange multiplier vector\n",
        "        self.b = b  # scalar bias term\n",
        "        self.errors = errors  # error cache\n",
        "        self._obj = []  # record of objective function value\n",
        "        self.m = len(self.X)  # store size of training set\n",
        "        self.hyperparams = hyperparams  # store hyperparameters\n",
        "\n",
        "        #predict function\n",
        "        # def predict(self, x):\n",
        "        #     predicted_label =\n",
        "        #     return np.sign(self.decision_function(x))\n",
        "\n",
        "\n",
        "\n",
        "##SMO algorithm implementation\n",
        "# The code here is based off of the pseudocode provided in Platt's paper. The implementation here is slightly different. I changed the training functions to pass around our model object to make the variable scoping clearer.\n",
        "#\n",
        "# The three functions used to train our model are take_step(), examine_example(), and train(). These are structured to work as follows:\n",
        "#\n",
        "#     The train() function implements selection of the first α\n",
        "#\n",
        "# to optimize via the first choice heuristic and passes this value to examine_example().\n",
        "# Then examine_example() implements the second choice heuristic to choose the second α\n",
        "# to optimize, and passes the index of both α\n",
        "# values to take_step().\n",
        "# Finally take_step() carries out the meat of the calculations and computes the two new α\n",
        "# values, a new threshold b\n",
        "#\n",
        "#     , and updates the error cache.\n",
        "#\n",
        "# The train() function uses a while loop to iterate through the α\n",
        "# values in a few different ways until no more optimizations can be made, at which point it returns the optimized α vector (embedded in an SMOModel object).\n",
        "\n",
        "\n",
        "#take_step()\n",
        "def take_step(i1, i2, model):\n",
        "    # Skip if chosen alphas are the same\n",
        "    if i1 == i2:\n",
        "        return 0, model\n",
        "\n",
        "    alph1 = model.alphas[i1]\n",
        "    alph2 = model.alphas[i2]\n",
        "    y1 = model.y[i1]\n",
        "    y2 = model.y[i2]\n",
        "    E1 = model.errors[i1]\n",
        "    E2 = model.errors[i2]\n",
        "    s = y1 * y2\n",
        "\n",
        "    # Compute L & H, the bounds on new possible alpha values\n",
        "    if (y1 != y2):\n",
        "        L = max(0, alph2 - alph1)\n",
        "        H = min(model.C, model.C + alph2 - alph1)\n",
        "    elif (y1 == y2):\n",
        "        L = max(0, alph1 + alph2 - model.C)\n",
        "        H = min(model.C, alph1 + alph2)\n",
        "    if (L == H):\n",
        "        return 0, model\n",
        "\n",
        "    # Compute kernel & 2nd derivative eta\n",
        "    k11 = model.kernel(model.X[i1], model.X[i1], model.hyperparams)\n",
        "    k12 = model.kernel(model.X[i1], model.X[i2], model.hyperparams)\n",
        "    k22 = model.kernel(model.X[i2], model.X[i2], model.hyperparams)\n",
        "    eta = 2 * k12 - k11 - k22\n",
        "\n",
        "    # Compute new alpha 2 (a2) if eta is negative\n",
        "    if (eta < 0):\n",
        "        a2 = alph2 - y2 * (E1 - E2) / eta\n",
        "        # Clip a2 based on bounds L & H\n",
        "        if L < a2 < H:\n",
        "            a2 = a2\n",
        "        elif (a2 <= L):\n",
        "            a2 = L\n",
        "        elif (a2 >= H):\n",
        "            a2 = H\n",
        "\n",
        "    # If eta is non-negative, move new a2 to bound with greater objective function value\n",
        "    else:\n",
        "        alphas_adj = model.alphas.copy()\n",
        "        alphas_adj[i2] = L\n",
        "        # objective function output with a2 = L\n",
        "        Lobj = objective_function(alphas_adj, model.y, model.kernel, model.X)\n",
        "        alphas_adj[i2] = H\n",
        "        # objective function output with a2 = H\n",
        "        Hobj = objective_function(alphas_adj, model.y, model.kernel, model.X)\n",
        "        if Lobj > (Hobj + model.eps):\n",
        "            a2 = L\n",
        "        elif Lobj < (Hobj - model.eps):\n",
        "            a2 = H\n",
        "        else:\n",
        "            a2 = alph2\n",
        "\n",
        "    # Push a2 to 0 or C if very close\n",
        "    if a2 < 1e-8:\n",
        "        a2 = 0.0\n",
        "    elif a2 > (model.C - 1e-8):\n",
        "        a2 = model.C\n",
        "\n",
        "    # If examples can't be optimized within epsilon (eps), skip this pair\n",
        "    if (np.abs(a2 - alph2) < model.eps * (a2 + alph2 + model.eps)):\n",
        "        return 0, model\n",
        "\n",
        "    # Calculate new alpha 1 (a1)\n",
        "    a1 = alph1 + s * (alph2 - a2)\n",
        "\n",
        "    # Update threshold b to reflect newly calculated alphas\n",
        "    # Calculate both possible thresholds\n",
        "    b1 = E1 + y1 * (a1 - alph1) * k11 + y2 * (a2 - alph2) * k12 + model.b\n",
        "    b2 = E2 + y1 * (a1 - alph1) * k12 + y2 * (a2 - alph2) * k22 + model.b\n",
        "\n",
        "    # Set new threshold based on if a1 or a2 is bound by L and/or H\n",
        "    if 0 < a1 and a1 < model.C:\n",
        "        b_new = b1\n",
        "    elif 0 < a2 and a2 < model.C:\n",
        "        b_new = b2\n",
        "    # Average thresholds if both are bound\n",
        "    else:\n",
        "        b_new = (b1 + b2) * 0.5\n",
        "\n",
        "    # Update model object with new alphas & threshold\n",
        "    model.alphas[i1] = a1\n",
        "    model.alphas[i2] = a2\n",
        "\n",
        "    # Update error cache\n",
        "    # Error cache for optimized alphas is set to 0 if they're unbound\n",
        "    for index, alph in zip([i1, i2], [a1, a2]):\n",
        "        if 0.0 < alph < model.C:\n",
        "            model.errors[index] = 0.0\n",
        "\n",
        "    # Set non-optimized errors based on equation 12.11 in Platt's book\n",
        "    non_opt = [n for n in range(model.m) if (n != i1 and n != i2)]\n",
        "    model.errors[non_opt] = model.errors[non_opt] + \\\n",
        "                            y1 * (a1 - alph1) * model.kernel(model.X[i1], model.X[non_opt], model.hyperparams) + \\\n",
        "                            y2 * (a2 - alph2) * model.kernel(model.X[i2], model.X[non_opt], model.hyperparams) + model.b - b_new\n",
        "\n",
        "    # Update model threshold\n",
        "    model.b = b_new\n",
        "\n",
        "    return 1, model\n",
        "\n",
        "\n",
        "#--\n",
        "\n",
        "# examine_example()\n",
        "def examine_example(i2, model):\n",
        "    y2 = model.y[i2]\n",
        "    alph2 = model.alphas[i2]\n",
        "    E2 = model.errors[i2]\n",
        "    r2 = E2 * y2\n",
        "\n",
        "    # Proceed if error is within specified tolerance (tol)\n",
        "    if ((r2 < -model.tol and alph2 < model.C) or (r2 > model.tol and alph2 > 0)):\n",
        "\n",
        "        if len(model.alphas[(model.alphas != 0) & (model.alphas != model.C)]) > 1:\n",
        "            # Use 2nd choice heuristic is choose max difference in error\n",
        "            if model.errors[i2] > 0:\n",
        "                i1 = np.argmin(model.errors)\n",
        "            elif model.errors[i2] <= 0:\n",
        "                i1 = np.argmax(model.errors)\n",
        "            step_result, model = take_step(i1, i2, model)\n",
        "            if step_result:\n",
        "                return 1, model\n",
        "\n",
        "        # Loop through non-zero and non-C alphas, starting at a random point\n",
        "        for i1 in np.roll(np.where((model.alphas != 0) & (model.alphas != model.C))[0],\n",
        "                          np.random.choice(np.arange(model.m))):\n",
        "            step_result, model = take_step(i1, i2, model)\n",
        "            if step_result:\n",
        "                return 1, model\n",
        "\n",
        "        # loop through all alphas, starting at a random point\n",
        "        for i1 in np.roll(np.arange(model.m), np.random.choice(np.arange(model.m))):\n",
        "            step_result, model = take_step(i1, i2, model)\n",
        "            if step_result:\n",
        "                return 1, model\n",
        "\n",
        "    return 0, model\n",
        "\n",
        "#--\n",
        "\n",
        "#train()\n",
        "\n",
        "def train(model):\n",
        "    numChanged = 0\n",
        "    examineAll = 1\n",
        "\n",
        "    while (numChanged > 0) or (examineAll):\n",
        "        numChanged = 0\n",
        "        if examineAll:\n",
        "            # loop over all training examples\n",
        "            for i in range(model.alphas.shape[0]):\n",
        "                examine_result, model = examine_example(i, model)\n",
        "                numChanged += examine_result\n",
        "                if examine_result:\n",
        "                    obj_result = objective_function(model.alphas, model.y, model.kernel, model.X)\n",
        "                    model._obj.append(obj_result)\n",
        "        else:\n",
        "            # loop over examples where alphas are not already at their limits\n",
        "            for i in np.where((model.alphas != 0) & (model.alphas != model.C))[0]:\n",
        "                examine_result, model = examine_example(i, model)\n",
        "                numChanged += examine_result\n",
        "                if examine_result:\n",
        "                    obj_result = objective_function(model.alphas, model.y, model.kernel, model.X)\n",
        "                    model._obj.append(obj_result)\n",
        "        if examineAll == 1:\n",
        "            examineAll = 0\n",
        "        elif numChanged == 0:\n",
        "            examineAll = 1\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# SVM with grid search for kernel parameters and C\n",
        "class Grid_Search():\n",
        "    \"We get \"\n",
        "\n",
        "    def __init__(self,X_train, y, name_of_kernal , X_validation , y_validation ):\n",
        "        self.X_train = X_train\n",
        "        self.y = y\n",
        "        self.name_of_kernal = name_of_kernal\n",
        "        self.X_validation = X_validation\n",
        "        self.y_validation = y_validation\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class SVM():\n",
        "\n",
        "    \"\"\" This class is the SVM clsss. It has the following methods:\n",
        "    __init__()\n",
        "    train()\n",
        "    predict()\n",
        "    test()\"\"\"\n",
        "    def __init__(self , X_train , y_train , X_validation , y_validation, X_test, y_test, name_of_kernal , positive_class,negative_class, grid_search_on=False):\n",
        "\n",
        "        self.X_train = X_train\n",
        "        self.y_train = y_train\n",
        "        self.X_validation = X_validation\n",
        "        self.y_validation = y_validation\n",
        "        self.X_test = X_test\n",
        "        self.y_test = y_test\n",
        "        self.name_of_kernal = name_of_kernal\n",
        "\n",
        "        self.grid_search_on = grid_search_on\n",
        "\n",
        "        # SVM works on normalized data\n",
        "        # function to preprocess the scale the data to be centered at the origin with a unit standard deviation, using scikit-learn's StandardScaler objec\n",
        "        self.scaler = StandardScaler()\n",
        "        self.X_train = self.scaler.fit_transform(self.X_train , y_train)\n",
        "        # make y labels to be +1 or -1\n",
        "        self.y_train[self.y_train == positive_class] = 1\n",
        "        self.y_train[self.y_train == negative_class] = -1\n",
        "\n",
        "        #if grid search is on, we need to do the grid search and construct the hyperparameter grid\n",
        "        if self.grid_search_on:\n",
        "            self.hyperparameters = self.construct_hyperparameters_grid(self)\n",
        "\n",
        "\n",
        "\n",
        "    #shared by all\n",
        "    tolerance = 0.01\n",
        "    epsilon = 0.01\n",
        "\n",
        "\n",
        "   #function to construct hyper-parameters grid , for each kernel\n",
        "\n",
        "    def construct_hyperparameters_grid(self):\n",
        "        if self.name_of_kernal == linear_kernel:\n",
        "            hyperparameters_grid = {'C': [0.1, 1, 10, 100, 1000]}\n",
        "        elif self.name_of_kernal == polynomial_kernel:\n",
        "            hyperparameters_grid = {'C': [0.1, 1, 10, 100, 1000], 'degree': [2, 3, 4, 5], 'gamma': [0.1, 0.01, 0.001, 0.0001]}\n",
        "            # making hyperparameters grid as list of tuples\n",
        "            hyperparameters_grid = [(C, degree, gamma) for C in hyperparameters_grid['C'] for degree in hyperparameters_grid['degree'] for gamma in hyperparameters_grid['gamma']]\n",
        "\n",
        "        elif self.name_of_kernal == gaussian_kernel:\n",
        "            hyperparameters_grid = {'C': [0.1, 1, 10, 100, 1000], 'gamma': [0.1, 0.01, 0.001, 0.0001]}\n",
        "            # making hyperparameters grid as list of tuples\n",
        "            hyperparameters_grid = [(C, gamma) for C in hyperparameters_grid['C'] for gamma in hyperparameters_grid['gamma']]\n",
        "        elif self.name_of_kernal == sigmoid_kernel:\n",
        "            hyperparameters_grid = {'C': [0.1, 1, 10, 100, 1000], 'gamma': [0.1, 0.01, 0.001, 0.0001]}\n",
        "            # making hyperparameters grid as list of tuples\n",
        "            hyperparameters_grid = [(C, gamma) for C in hyperparameters_grid['C'] for gamma in hyperparameters_grid['gamma']]\n",
        "\n",
        "        else:\n",
        "            raise Exception('Kernel not supported')\n",
        "        return hyperparameters_grid\n",
        "\n",
        "\n",
        "    #function to predict the labels of the unknown data\n",
        "    # def predict(self, opt_model,unknown_data):\n",
        "    #     # if unknown data is a collection of data, we need to predict for each data point\n",
        "    #     if type(unknown_data) == np.ndarray:# if\n",
        "    #         output = []\n",
        "    #         for unknown in unknown_data:\n",
        "    #             output.append()\n",
        "    #\n",
        "    #\n",
        "    #         output = np.array(output)\n",
        "    #\n",
        "    #\n",
        "    #     else:  #only single test data point is passed\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def get_optimal_hyperplane(self , kernal_parameters)  :\n",
        "        # setting initial values\n",
        "        C = kernal_parameters[0]\n",
        "        m = len(self.X_train)\n",
        "        initial_alphas = np.zeros(m)\n",
        "        initial_b = 0.0\n",
        "        # set tolerance\n",
        "        tol = self.tolerance\n",
        "        # set epsilon\n",
        "        eps = self.epsilon\n",
        "\n",
        "        # hyper-parameters\n",
        "        hyperparam = np.array(kernal_parameters[1:])\n",
        "\n",
        "        # instantiate the model\n",
        "        model = SMOModel(self.X_train, self.y_train, C, tol, eps, self.name_of_kernal, initial_alphas, initial_b,\n",
        "                         np.zeros(m), hyperparam)\n",
        "\n",
        "        # Initialize error cache\n",
        "        initial_error = decision_function(model.alphas, model.y, model.kernel,\n",
        "                                          model.X, model.X, model.b) - model.y\n",
        "        model.errors = initial_error\n",
        "\n",
        "        np.random.seed(0)\n",
        "        output = train(model)\n",
        "        return output\n",
        "\n",
        "\n",
        "    #function to get the F1 score on the validation set\n",
        "    # def get_F1_score(self, model):\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # function to train the classifier given the hyperparameters\n",
        "    def train(self , kernal_parameters):\n",
        "        #check if grid search is on\n",
        "        if self.grid_search_on:     #we will loop over the hyperparameters grid\n",
        "            for kernal_parameters in self.hyperparameters:\n",
        "                output_model = self.get_optimal_hyperplane(kernal_parameters)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# SVM implementation over --------------------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# testing --------------------------------------------------------------------------------------------------------------\n",
        "# toy data generated by scikit-learn's make_blobs.\n",
        "\n",
        "#testing and debugging purposes functions\n",
        "def testing_toy_data(C = 1000.0 , tol = 0.01 , eps = 0.01 , kernel = linear_kernel ):\n",
        "\n",
        "    X_train, y = make_blobs(n_samples=1000, centers=2,\n",
        "                        n_features=2, random_state=1)\n",
        "\n",
        "    # scale the data to be centered at the origin with a unit standard deviation, using scikit-learn's StandardScaler object.\n",
        "    scaler = StandardScaler()\n",
        "    X_train = scaler.fit_transform(X_train, y)\n",
        "\n",
        "    # Finally, we have to change the class labels to be +1 and -1 instead of 0 and 1.\n",
        "\n",
        "    y[y == 0] = -1\n",
        "\n",
        "    # Set model parameters and initial values\n",
        "    # C = 1000.0\n",
        "    m = len(X_train)\n",
        "    initial_alphas = np.zeros(m)\n",
        "    initial_b = 0.0\n",
        "\n",
        "    # Set tolerances\n",
        "    tol = 0.01  # error tolerance\n",
        "    eps = 0.01  # alpha tolerance\n",
        "\n",
        "    # Instantiate model\n",
        "    model = SMOModel(X_train, y, C,tol,eps, linear_kernel,\n",
        "                     initial_alphas, initial_b, np.zeros(m))\n",
        "\n",
        "    # Initialize error cache\n",
        "    initial_error = decision_function(model.alphas, model.y, model.kernel,\n",
        "                                      model.X, model.X, model.b) - model.y\n",
        "    model.errors = initial_error\n",
        "\n",
        "    np.random.seed(0)\n",
        "    output = train(model)\n",
        "\n",
        "    # Let's see what the decision boundary looks like.\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    grid, ax = plot_decision_boundary(output, ax)\n",
        "\n",
        "\n",
        "\n",
        "def test_driver():\n",
        "\n",
        "    #initialize C (the regularization parameter)\n",
        "    C = 1000.0\n",
        "    tol = 0.01\n",
        "    eps = 0.01\n",
        "    kernel = linear_kernel\n",
        "    # C = 1.0\n",
        "    testing_toy_data(C, tol, eps, kernel)\n",
        "\n",
        "#testing kernals\n",
        "def test_kernal():\n",
        "    \"\"\"Tests the kernel function `kernel` on\"\"\"\n",
        "    x = [ 1,2,3]\n",
        "    y = [ 4,5,6]\n",
        "    #convert to numpy arrays\n",
        "    x = np.array(x)\n",
        "    y = np.array(y)\n",
        "    # call gaussian kernel function\n",
        "    result = gaussian_kernel(x, y, sigma=1)\n",
        "    print(result)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#defining main function\n",
        "def main():\n",
        "    print(\"Entering main function\")\n",
        "    # test_kernal()\n",
        "\n",
        "    test_driver()\n",
        "\n",
        "#if __name__ == \"__main__\":\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# #gridsearch\n",
        "#     # create a grid of points to evaluate the kernel function on\n",
        "#     x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "#     y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "#     xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
        "#                          np.arange(y_min, y_max, 0.02))\n",
        "#\n",
        "#     # evaluate the kernel function on the grid\n",
        "#     Z = kernel(X, X, sigma=sigma)\n",
        "#     Z = Z @ y.T\n",
        "#     Z = Z.reshape(xx.shape)\n",
        "#\n",
        "#     # plot the contour and training examples\n",
        "#     plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)\n",
        "#     plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Spectral)\n",
        "#     plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# #gridsearch\n",
        "#     def __init__(self, X, y, C_range, gamma_range, tol=1e-3):\n",
        "#         self.X = X\n",
        "#         self.y = y\n",
        "#         self.C_range = C_range\n",
        "#         self.gamma_range = gamma_range\n",
        "#         self.tol = tol\n",
        "#         self.best_params = None\n",
        "#         self.best_score = None\n",
        "#         self.best_model = None\n",
        "#\n",
        "#     def fit(self):\n",
        "#         self.best_params = None\n",
        "#         self.best_score = None\n",
        "#         self.best_model = None\n",
        "#\n",
        "#         for C in self.C_range:\n",
        "#             for gamma in self.gamma_range:\n",
        "#                 model = SVM(self.X, self.y, C, gamma, self.tol)\n",
        "#                 model = train(model)\n",
        "#                 score = accuracy_score(model.predict(self.X), self.y)\n",
        "#                 if self.best_score is None or score > self.best_score:\n",
        "#                     self.best_params = (C, gamma)\n",
        "#                     self.best_score = score\n",
        "#                     self.best_model = model\n",
        "#\n",
        "#         return self.best_model\n",
        "\n",
        "\n",
        "\n",
        "# #gridsearch\n",
        "#         self.C_range = np.logspace(-2, 10, 13)\n",
        "#         self.gamma_range = np.logspace(-9, 3, 13)\n",
        "#         self.kernel_parameter_grid = dict(gamma=self.gamma_range, C=self.C_range)\n",
        "#         self.grid_search_model = GridSearchCV(SVC(kernel=self.name_of_kernal), self.kernel_parameter_grid, cv=5)\n",
        "#         self.grid_search_model.fit(self.X_train, self.y)\n",
        "#         self.best_C = self.grid_search_model.best_estimator_.C\n",
        "#         self.best_gamma = self.grid_search_model.best_estimator_.gamma\n",
        "#         self.best_model = SVC(kernel=self.name_of_kernal, C=self.best_C, gamma=self.best_gamma)\n",
        "#         self.best_model.fit(self.X_train, self.y)\n",
        "#         self.best_score = self.best_model.score(self.X_validation, self.y_validation)\n",
        "#         self.best_score_train = self.best_model.score(self.X_train, self.y)\n",
        "#         self.best_score_test = self.best_model.score(self.X_validation, self.y_validation)\n",
        "#         self.best_score_train_validation = self.best_model.score(self.X_train, self.y)\n",
        "#         self.best_score_test_validation = self.best_model.score(self.X_validation, self.y_validation)\n",
        "#         self.best_score_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xR2qh_yjlRiB"
      },
      "outputs": [],
      "source": [
        "#SVM Binary pneumonia mnist\n",
        "# we are implementing a simple SVM classifier for the pneumonia MNIST dataset\n",
        "# the dataset is at the folder: pneumoniamnist in the same directory as this file\n",
        "\n",
        "#result folder is: binary_svm\n",
        "\n",
        "\n",
        "\n",
        "#importing libraries\n",
        "import csv\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score, precision_score, recall_score, roc_curve\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "#importing svm\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import Normalize\n",
        "\n",
        "\n",
        "\n",
        "class MidpointNormalize(Normalize):\n",
        "    def __init__(self, vmin=None, vmax=None, midpoint=None, clip=False):\n",
        "        self.midpoint = midpoint\n",
        "        Normalize.__init__(self, vmin, vmax, clip)\n",
        "\n",
        "    def __call__(self, value, clip=None):\n",
        "        x, y = [self.vmin, self.midpoint, self.vmax], [0, 0.5, 1]\n",
        "        return np.ma.masked_array(np.interp(value, x, y))\n",
        "\n",
        "\n",
        "\n",
        "def data_preprocessing(path):\n",
        "    data_path = path + '/pneumoniamnist.npz'\n",
        "    data = np.load(data_path)\n",
        "    # print(data.files)\n",
        "    # Reshape images and save in numpy arrays\n",
        "    train1_data = np.reshape(data['train_images'], (4708, 784))\n",
        "    train1_labels = np.reshape(data['train_labels'], (4708))\n",
        "    #change data type to float of train1_data\n",
        "    train1_data = train1_data.astype(float)\n",
        "    #change datatype of train1_labels to int\n",
        "    train1_labels = train1_labels.astype(int)\n",
        "\n",
        "    #validation data\n",
        "    valid1_data = np.reshape(data['val_images'], (524, 784))\n",
        "    valid1_labels = np.reshape(data['val_labels'], (524))\n",
        "    #change datatype of valid1_data to float\n",
        "    valid1_data = valid1_data.astype(float)\n",
        "    #change datatype of valid1_labels to int\n",
        "    valid1_labels = valid1_labels.astype(int)\n",
        "\n",
        "\n",
        "    test1_data = np.reshape(data['test_images'], (624, 784))\n",
        "    test1_labels = np.reshape(data['test_labels'], (624))\n",
        "    #change data type to float of test1_data\n",
        "    test1_data = test1_data.astype(float)\n",
        "    #change datatype of test1_labels to int\n",
        "    test1_labels = test1_labels.astype(int)\n",
        "\n",
        "    # feature scaling\n",
        "    sc = StandardScaler()\n",
        "    X_train = sc.fit_transform(train1_data , train1_labels)\n",
        "    X_test = sc.fit_transform(test1_data, test1_labels)\n",
        "    X_valid = sc.fit_transform(valid1_data, valid1_labels)\n",
        "    #change label of train1_labels to -1\n",
        "    train1_labels[train1_labels == 0] = -1\n",
        "    Y_train = train1_labels\n",
        "    #change label of test1_labels to -1\n",
        "    test1_labels[test1_labels == 0] = -1\n",
        "    Y_test = test1_labels\n",
        "    #change label of valid1_labels to -1\n",
        "    valid1_labels[valid1_labels == 0] = -1\n",
        "    Y_valid = valid1_labels\n",
        "\n",
        "    return X_train, Y_train, X_test, Y_test , X_valid, Y_valid\n",
        "\n",
        "\n",
        "# def validation_dataset(path):\n",
        "#     data = np.load(path)\n",
        "#     # print(data.files)\n",
        "#     # Reshape images and save in numpy arrays\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def grid_search_poly(X_train, Y_train,  X_valid, Y_valid ):\n",
        "\n",
        "    #create csv file for the results name: results_poly.csv\n",
        "    with open('binary_svm/results_poly.csv', 'w') as f:\n",
        "        f.write('C, degree, gamma, accuracy, precision, recall, f1\\n')\n",
        "\n",
        "\n",
        "    #SVM with C = 1 and degree = 2 and gamma = 0.1\n",
        "    clf = SVC(kernel='poly', C=1, degree=2)\n",
        "\n",
        "    clf.fit(X_train, Y_train)\n",
        "    Y_pred = clf.predict(X_valid)\n",
        "    #f1 score\n",
        "    f1 = f1_score(Y_valid, Y_pred, average='micro')\n",
        "    optimal_f1 = f1\n",
        "    optimal_C = 1\n",
        "    optimal_degree = 2\n",
        "\n",
        "    optimal_classifier = clf\n",
        "    optimal_accuracy = clf.score(X_valid, Y_valid)\n",
        "\n",
        "\n",
        "\n",
        "    #for testing\n",
        "    # c_range = [0.1, 1, 10]\n",
        "    # degree_range = [ 2 , 3]\n",
        "    gamma_range = [0.1 ]\n",
        "\n",
        "    # c_range = np.logspace(-2, 10, 13)\n",
        "    # degree_range = [ 2, 3, 4, 5, 6 , 7, 8, 9, 10, 100, 1000, 10000]\n",
        "    # gamma_range = np.logspace(-9, 3, 13)\n",
        "\n",
        "    c_range = [0.001,  1, 100]\n",
        "    degree_range = [ 2, 10, 100]\n",
        "    # gamma_range = [ 0.01, 10, 1000]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    for C in c_range:\n",
        "        for degree in degree_range:\n",
        "            for gamma in gamma_range:\n",
        "                clf = SVC(kernel='poly', C=C, degree=degree )\n",
        "                clf.fit(X_train, Y_train)\n",
        "                Y_pred = clf.predict(X_valid)\n",
        "                # f1 score\n",
        "                f1 = f1_score(Y_valid, Y_pred, average='micro')\n",
        "                # accuracy\n",
        "                accuracy = clf.score(X_valid, Y_valid)\n",
        "                # precision\n",
        "                precision = precision_score(Y_valid, Y_pred, average='micro')\n",
        "                # recall\n",
        "                recall = recall_score(Y_valid, Y_pred, average='micro')\n",
        "                # write results to csv file\n",
        "                with open('binary_svm/results_poly.csv', 'a') as f:\n",
        "                    f.write('{}, {}, {}, {}, {}, {}, {}\\n'.format(C, degree, gamma, accuracy, precision, recall, f1))\n",
        "\n",
        "                if f1 > optimal_f1:\n",
        "                    optimal_f1 = f1\n",
        "                    optimal_C = C\n",
        "                    optimal_degree = degree\n",
        "\n",
        "                    optimal_classifier = clf\n",
        "                    optimal_accuracy = accuracy\n",
        "\n",
        "\n",
        "\n",
        "    optimal_hyperparameters = {'C': optimal_C, 'degree': optimal_degree}\n",
        "    optimal_metrics = {'accuracy': optimal_accuracy, 'f1': optimal_f1}\n",
        "    return optimal_classifier, optimal_hyperparameters, optimal_metrics\n",
        "\n",
        "\n",
        "\n",
        "def grid_search_sigmoid(X_train, Y_train,  X_valid, Y_valid):\n",
        "    #create csv file for the results name: results_sigmoid.csv\n",
        "    with open('binary_svm/results_sigmoid.csv', 'w') as f:\n",
        "        f.write('C, gamma, accuracy, precision, recall, f1\\n')\n",
        "\n",
        "    #SVM with C = 1 and gamma = 0.1\n",
        "    clf = SVC(kernel='sigmoid', C=1, gamma=0.1)\n",
        "\n",
        "    clf.fit(X_train, Y_train)\n",
        "    Y_pred = clf.predict(X_valid)\n",
        "    # f1 score\n",
        "    f1 = f1_score(Y_valid, Y_pred, average='micro')\n",
        "    optimal_f1 = f1\n",
        "    optimal_C = 1\n",
        "\n",
        "    optimal_gamma = 0.1\n",
        "    optimal_classifier = clf\n",
        "    optimal_accuracy = clf.score(X_valid, Y_valid)\n",
        "\n",
        "    #for testing\n",
        "    # c_range = [1]\n",
        "    # gamma_range = [0.1]\n",
        "\n",
        "    # c_range = np.logspace(-2, 10, 13)\n",
        "    # gamma_range = np.logspace(-9, 3, 13)\n",
        "    # c_range = [0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]\n",
        "    #\n",
        "    # gamma_range = [ 0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
        "\n",
        "    c_range = [0.001, 1, 100]\n",
        "\n",
        "    gamma_range = [ 0.01, 10, 1000]\n",
        "\n",
        "    for C in c_range:\n",
        "        for gamma in gamma_range:\n",
        "            clf = SVC(kernel='sigmoid', C=C, gamma=gamma)\n",
        "            clf.fit(X_train, Y_train)\n",
        "            Y_pred = clf.predict(X_valid)\n",
        "            # f1 score\n",
        "            f1 = f1_score(Y_valid, Y_pred, average='micro')\n",
        "            # accuracy\n",
        "            accuracy = clf.score(X_valid, Y_valid)\n",
        "            # precision\n",
        "            precision = precision_score(Y_valid, Y_pred, average='micro')\n",
        "            # recall\n",
        "            recall = recall_score(Y_valid, Y_pred, average='micro')\n",
        "            # write results to csv file\n",
        "            with open('binary_svm/results_sigmoid.csv', 'a') as f:\n",
        "                f.write('{}, {}, {}, {}, {}, {}\\n'.format(C, gamma, accuracy, precision, recall, f1))\n",
        "\n",
        "\n",
        "            if f1 > optimal_f1:\n",
        "                optimal_f1 = f1\n",
        "                optimal_C = C\n",
        "                optimal_gamma = gamma\n",
        "                optimal_classifier = clf\n",
        "                optimal_accuracy = accuracy\n",
        "\n",
        "    optimal_hyperparameters = {'C': optimal_C, 'gamma': optimal_gamma}\n",
        "    optimal_metrics = {'accuracy': optimal_accuracy, 'f1': optimal_f1}\n",
        "    return optimal_classifier, optimal_hyperparameters, optimal_metrics\n",
        "\n",
        "def grid_search_gaussian(X_train, Y_train,  X_valid, Y_valid):\n",
        "    #create csv file for the results name: results_gaussian.csv\n",
        "    with open('binary_svm/results_gaussian.csv', 'w') as f:\n",
        "        f.write('C, gamma, accuracy, precision, recall, f1\\n')\n",
        "\n",
        "    #SVM with C = 1 and gamma = 0.1\n",
        "    clf = SVC(kernel='rbf', C=1, gamma=0.1)\n",
        "\n",
        "    clf.fit(X_train, Y_train)\n",
        "    Y_pred = clf.predict(X_valid)\n",
        "    # f1 score\n",
        "    f1 = f1_score(Y_valid, Y_pred, average='micro')\n",
        "    optimal_f1 = f1\n",
        "    optimal_C = 1\n",
        "\n",
        "    optimal_gamma = 0.1\n",
        "    optimal_classifier = clf\n",
        "    optimal_accuracy = clf.score(X_valid, Y_valid)\n",
        "\n",
        "    #for testing\n",
        "    # c_range = [1]\n",
        "    # gamma_range = [0.1]\n",
        "\n",
        "    # c_range = np.logspace(-2, 10, 13)\n",
        "    # gamma_range = np.logspace(-9, 3, 13)\n",
        "    # c_range = [0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]\n",
        "    #\n",
        "    # gamma_range = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
        "    c_range = [0.001, 1, 100]\n",
        "\n",
        "    gamma_range = [ 0.01, 10, 1000]\n",
        "\n",
        "    for C in c_range:\n",
        "        for gamma in gamma_range:\n",
        "            clf = SVC(kernel='rbf', C=C, gamma=gamma)\n",
        "            clf.fit(X_train, Y_train)\n",
        "            Y_pred = clf.predict(X_valid)\n",
        "            # f1 score\n",
        "            f1 = f1_score(Y_valid, Y_pred, average='micro')\n",
        "            # accuracy\n",
        "            accuracy = clf.score(X_valid, Y_valid)\n",
        "            # precision\n",
        "            precision = precision_score(Y_valid, Y_pred, average='micro')\n",
        "            # recall\n",
        "            recall = recall_score(Y_valid, Y_pred, average='micro')\n",
        "            # write results to csv file\n",
        "            with open('binary_svm/results_gaussian.csv', 'a') as f:\n",
        "                f.write('{}, {}, {}, {}, {}, {}\\n'.format(C, gamma, accuracy, precision, recall, f1))\n",
        "\n",
        "            if f1 > optimal_f1:\n",
        "                optimal_f1 = f1\n",
        "                optimal_C = C\n",
        "                optimal_gamma = gamma\n",
        "                optimal_classifier = clf\n",
        "                optimal_accuracy = accuracy\n",
        "\n",
        "    optimal_hyperparameters = {'C': optimal_C, 'gamma': optimal_gamma}\n",
        "    optimal_metrics = {'accuracy': optimal_accuracy, 'f1': optimal_f1}\n",
        "    return optimal_classifier, optimal_hyperparameters, optimal_metrics\n",
        "\n",
        "def grid_search_linear(X_train, Y_train,  X_valid, Y_valid):\n",
        "    #create csv file for the results name: results_linear.csv\n",
        "    with open('binary_svm/results_linear.csv', 'w') as f:\n",
        "        f.write('C, accuracy, precision, recall, f1\\n')\n",
        "\n",
        "    #SVM with C = 1\n",
        "    clf = SVC(kernel='linear', C=1)\n",
        "    clf.fit(X_train, Y_train)\n",
        "    Y_pred = clf.predict(X_valid)\n",
        "    # f1 score\n",
        "    f1 = f1_score(Y_valid, Y_pred, average='micro')\n",
        "    optimal_f1 = f1\n",
        "    optimal_C = 1\n",
        "    optimal_degree = 2\n",
        "    optimal_gamma = 0.1\n",
        "    optimal_classifier = clf\n",
        "    optimal_accuracy = clf.score(X_valid, Y_valid)\n",
        "\n",
        "    # for testing\n",
        "    # c_range = [1]\n",
        "\n",
        "    # c_range = np.logspace(-2, 10, 13)\n",
        "    # c_range = [0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]\n",
        "    #\n",
        "    # gamma_range = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
        "    c_range = [0.001, 1, 100]\n",
        "\n",
        "    gamma_range = [ 0.01, 10, 1000]\n",
        "    for C in c_range:\n",
        "        clf = SVC(kernel='linear', C=C)\n",
        "        clf.fit(X_train, Y_train)\n",
        "        Y_pred = clf.predict(X_valid)\n",
        "        # f1 score\n",
        "        f1 = f1_score(Y_valid, Y_pred, average='micro')\n",
        "        # accuracy\n",
        "\n",
        "        accuracy = clf.score(X_valid, Y_valid)\n",
        "\n",
        "        # precision\n",
        "        precision = precision_score(Y_valid, Y_pred, average='micro')\n",
        "        # recall\n",
        "        recall = recall_score(Y_valid, Y_pred, average='micro')\n",
        "        # write results to csv file\n",
        "        with open('binary_svm/results_linear.csv', 'a') as f:\n",
        "            f.write('{}, {}, {}, {}, {}\\n'.format(C, accuracy, precision, recall, f1))\n",
        "\n",
        "        if f1 > optimal_f1:\n",
        "            optimal_f1 = f1\n",
        "            optimal_C = C\n",
        "            optimal_classifier = clf\n",
        "            optimal_accuracy = accuracy\n",
        "\n",
        "    optimal_hyperparameters = {'C': optimal_C}\n",
        "    optimal_metrics = {'accuracy': optimal_accuracy, 'f1': optimal_f1}\n",
        "    return optimal_classifier, optimal_hyperparameters, optimal_metrics\n",
        "\n",
        "\n",
        "\n",
        "#this function plots colormap where x and y coordinates of colourmap are the hyperparameters and colourmap is the accuracy of the classifier\n",
        "def plot_hyperparameter_accuracy(csv_file, midpoint, title, name , hyperparameters_to_plot= ['C', ' gamma']):\n",
        "    print()\n",
        "    hyperparameter_1 = hyperparameters_to_plot[0]\n",
        "    hyperparameter_2 = hyperparameters_to_plot[1]\n",
        "\n",
        "    # Draw heatmap of the validation accuracy as a function of gamma and C\n",
        "    #   read csv file\n",
        "\n",
        "    #plotting the validation accuracy vs hyperparameters colormap of linear kernel\n",
        "    #   read csv file and store value of accuracy in a 2d array\n",
        "    # one axis of array is gamma and other is C\n",
        "    #reading csv file\n",
        "    accuracy_list = []\n",
        "    c_list = []\n",
        "    gamma_list = []\n",
        "    with open(csv_file, 'r') as csvfile:\n",
        "        reader = csv.DictReader(csvfile)\n",
        "        line_count = 0\n",
        "        # the columns that we want to read are 'C', 'gamma', 'accuracy'\n",
        "        # the first row is the header\n",
        "        # the rest of the rows are the data\n",
        "        for row in reader:\n",
        "            if line_count == 0:\n",
        "                # this is the header\n",
        "                # print(f'Column names are {\", \".join(row)}')\n",
        "                line_count += 1\n",
        "            #save the accuracy values for all rows in a list\n",
        "            accuracy_list.append(row[' accuracy'])\n",
        "            c_list.append(row[hyperparameter_1])\n",
        "            gamma_list.append(row[hyperparameter_2])\n",
        "            line_count += 1\n",
        "    #print the list c_list ands gamma_list\n",
        "    print(c_list)\n",
        "    print(gamma_list)\n",
        "\n",
        "\n",
        "    #convert the list to numpy array\n",
        "    #convert list of strings to list of floats\n",
        "    accuracy_list = [float(i) for i in accuracy_list]\n",
        "    c_list = [float(i) for i in c_list]\n",
        "    gamma_list = [float(i) for i in gamma_list]\n",
        "    #convert list of floats to numpy array\n",
        "    accuracy_array = np.array(accuracy_list)\n",
        "    c_array = np.array(c_list)\n",
        "    gamma_array = np.array(gamma_list)\n",
        "    #reshape the array to 2d array\n",
        "\n",
        "    #define c_len to be no. of unique values of c_array\n",
        "    c_len = len(np.unique(c_array))\n",
        "    #define gamma_len to be no. of unique values of gamma_array\n",
        "    gamma_len = len(np.unique(gamma_array))\n",
        "\n",
        "    #c_array to have unique values of c_array\n",
        "    c_array = np.unique(c_array)\n",
        "    #gamma_array to have unique values of gamma_array\n",
        "    gamma_array = np.unique(gamma_array)\n",
        "    #c_array to be sorted in ascending order\n",
        "    c_array = np.sort(c_array)\n",
        "    #gamma_array to be sorted in ascending order\n",
        "    gamma_array = np.sort(gamma_array)\n",
        "\n",
        "    reshaped_array = np.reshape(accuracy_array, (len(c_array), len(gamma_array)))\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.subplots_adjust(left=0.2, right=0.95, bottom=0.15, top=0.95)\n",
        "    plt.imshow(\n",
        "        reshaped_array,\n",
        "        interpolation=\"nearest\",\n",
        "        cmap=plt.cm.hot,\n",
        "        norm=MidpointNormalize(vmin=0.2, midpoint=midpoint, vmax=1)\n",
        "    )\n",
        "    plt.xlabel(hyperparameter_2)\n",
        "    plt.ylabel(hyperparameter_1)\n",
        "    plt.colorbar()\n",
        "    plt.xticks(np.arange(len(gamma_array)), gamma_array, rotation=45)\n",
        "    plt.yticks(np.arange(len(c_array)), c_array)\n",
        "    plt.title(\"Validation accuracy \" + title + \" \"+ name)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def extract_accuracy(csv_file):\n",
        "    accuracy_list = []\n",
        "    c_list = []\n",
        "\n",
        "    with open(csv_file, 'r') as csvfile:\n",
        "        reader = csv.DictReader(csvfile)\n",
        "        line_count = 0\n",
        "        # the columns that we want to read are 'C', 'gamma', 'accuracy'\n",
        "        # the first row is the header\n",
        "        # the rest of the rows are the data\n",
        "        for row in reader:\n",
        "            if line_count == 0:\n",
        "                # this is the header\n",
        "                # print(f'Column names are {\", \".join(row)}')\n",
        "                line_count += 1\n",
        "            # save the accuracy values for all rows in a list\n",
        "            accuracy_list.append(row[' accuracy'])\n",
        "            c_list.append(row['C'])\n",
        "\n",
        "            line_count += 1\n",
        "\n",
        "    # convert list of strings to list of floats\n",
        "    accuracy_list = [float(i) for i in accuracy_list]\n",
        "    c_list = [float(i) for i in c_list]\n",
        "\n",
        "    # convert list of floats to numpy array\n",
        "    accuracy_array = np.array(accuracy_list)\n",
        "    c_array = np.array(c_list)\n",
        "\n",
        "\n",
        "\n",
        "    # c_array to have unique values of c_array\n",
        "    c_array = np.unique(c_array)\n",
        "\n",
        "    # c_array to be sorted in ascending order\n",
        "    c_array = np.sort(c_array)\n",
        "\n",
        "    return accuracy_array, c_array\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#plotting the results\n",
        "def plot_results():\n",
        "    name = \"PneumoniaMNIST\"\n",
        "\n",
        "    # plotting the colormap of sigmoide kernel\n",
        "    plot_hyperparameter_accuracy('binary_svm/results_sigmoid.csv' , 0.72 , 'Sigmoid Kernel', name)\n",
        "\n",
        "    #plotting the colormap of gaussian kernel\n",
        "    plot_hyperparameter_accuracy('binary_svm/results_gaussian.csv' , 0.74 , 'Gaussian Kernel', name)\n",
        "\n",
        "    #plotting the colormap of polynomial kernel\n",
        "    plot_hyperparameter_accuracy('binary_svm/results_poly_best.csv' , 0.7 , 'Polynomial Kernel', name , ['C', ' degree'])\n",
        "\n",
        "    #plotting the graph of linear kernel\n",
        "    accuracy_array, c_array = extract_accuracy('binary_svm/results_linear.csv')\n",
        "    #plot the accuracy vs c graph\n",
        "    plt.plot(c_array, accuracy_array)\n",
        "    plt.xlabel('C')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Accuracy vs C for Linear Kernel ' + name)\n",
        "    #show\n",
        "    plt.show()\n",
        "\n",
        "    # #plotting the graph of polynomial kernel\n",
        "    #\n",
        "    # accuracy_array, c_array = extract_accuracy('binary_svm/results_poly.csv')\n",
        "    # # plot the accuracy vs c graph\n",
        "    # plt.plot(c_array, accuracy_array)\n",
        "    # plt.xlabel('degree')\n",
        "    # plt.ylabel('Accuracy')\n",
        "    # plt.title('Accuracy vs degree for Linear Kernel ' + name)\n",
        "    # # show\n",
        "    # plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#plotting AUC curve of the best classifier\n",
        "def plot_AUC(classifier, X_test, Y_test):\n",
        "    name = \"PneumoniaMNIST\"\n",
        "    #get the predicted labels\n",
        "    Y_pred = classifier.predict(X_test)\n",
        "    #get the predicted probabilities\n",
        "    Y_prob = classifier.predict_proba(X_test)\n",
        "    #get the AUC\n",
        "    fpr, tpr, thresholds = roc_curve(Y_test, Y_prob[:,1])\n",
        "    #plot the AUC\n",
        "    plt.plot(fpr, tpr)\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('ROC Curve for ' + name)\n",
        "    #show\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def SVM(path):\n",
        "\n",
        "    #getting the data\n",
        "    X_train, Y_train, X_test, Y_test, X_valid, Y_valid = data_preprocessing(path)\n",
        "    # SVM\n",
        "    # clf = SVC(kernel='linear', C=1)\n",
        "    # clf.fit(X_train, Y_train)\n",
        "    # Y_pred = clf.predict(X_test)\n",
        "    # # print(classification_report(Y_test, Y_pred))\n",
        "    # print(y_pred)\n",
        "    # print(test1_labels)\n",
        "    # print(y_pred == test1_labels)\n",
        "    # print(np.sum(y_pred == test1_labels))\n",
        "    # print(np.sum(y_pred == test1_labels) / 624)\n",
        "    # print(np.sum(y_pred == test1_labels) / 624 * 100)\n",
        "\n",
        "    # print(\"Accuracy of SVM classifier on test set: {:.2f}%\".format(np.sum(Y_pred == Y_test) / 624 * 100))\n",
        "\n",
        "    # print(clf.score(X_test, test1_labels))\n",
        "    # print(clf.score(X_train, train1_labels))\n",
        "\n",
        "\n",
        "\n",
        "    #calling grid search poly\n",
        "    print(\"Grid Search for Polynomial Kernel\")\n",
        "    optimal_classifier, optimal_hyperparameters, optimal_metrics = grid_search_poly(X_train, Y_train, X_valid, Y_valid)\n",
        "    #printing the results\n",
        "    print(\"Optimal C: {}\".format(optimal_hyperparameters['C']))\n",
        "    print(\"Optimal degree: {}\".format(optimal_hyperparameters['degree']))\n",
        "    print(\"Optimal gamma: {}\".format(optimal_hyperparameters['gamma']))\n",
        "    print(\"Optimal f1: {}\".format(optimal_metrics['f1']))\n",
        "    print(\"Optimal accuracy: {}\".format(optimal_metrics['accuracy']))\n",
        "\n",
        "\n",
        "    # calling grid search sigmoid\n",
        "    print(\"Grid Search for Sigmoid Kernel\")\n",
        "    optimal_classifier, optimal_hyperparameters, optimal_metrics = grid_search_sigmoid(X_train, Y_train, X_valid, Y_valid)\n",
        "    #printing the results\n",
        "    print(\"Optimal C: {}\".format(optimal_hyperparameters['C']))\n",
        "    print(\"Optimal gamma: {}\".format(optimal_hyperparameters['gamma']))\n",
        "    print(\"Optimal f1: {}\".format(optimal_metrics['f1']))\n",
        "    print(\"Optimal accuracy: {}\".format(optimal_metrics['accuracy']))\n",
        "\n",
        "    # calling grid search gaussian\n",
        "    print(\"Grid Search for Gaussian Kernel\")\n",
        "    optimal_classifier, optimal_hyperparameters, optimal_metrics = grid_search_gaussian(X_train, Y_train, X_valid, Y_valid)\n",
        "    #printing the results\n",
        "    print(\"Optimal C: {}\".format(optimal_hyperparameters['C']))\n",
        "    print(\"Optimal gamma: {}\".format(optimal_hyperparameters['gamma']))\n",
        "    print(\"Optimal f1: {}\".format(optimal_metrics['f1']))\n",
        "    print(\"Optimal accuracy: {}\".format(optimal_metrics['accuracy']))\n",
        "\n",
        "    # # calling grid search linear\n",
        "    print(\"Grid Search for Linear Kernel\")\n",
        "    optimal_classifier, optimal_hyperparameters, optimal_metrics = grid_search_linear(X_train, Y_train, X_valid, Y_valid)\n",
        "    #printing the results\n",
        "    print(\"Optimal C: {}\".format(optimal_hyperparameters['C']))\n",
        "    print(\"Optimal f1: {}\".format(optimal_metrics['f1']))\n",
        "    print(\"Optimal accuracy: {}\".format(optimal_metrics['accuracy']))\n",
        "\n",
        "\n",
        "    # plotting\n",
        "    # plotting the colormap of polynomial kernel\n",
        "    # plot_hyperparameter_accuracy('binary_svm/results_poly.csv', 0.9, 'Polynomial Kernel', \"PneumoniaMNIST\", ['C', ' degree'])\n",
        "\n",
        "    # # plotting the graph of linear kernel\n",
        "    # accuracy_array, c_array = extract_accuracy('binary_svm/results_linear.csv')\n",
        "    #\n",
        "    # #print accuracy_array and c_array\n",
        "    # print(accuracy_array)\n",
        "    # print(c_array)\n",
        "    #\n",
        "    # # plot the accuracy vs c graph\n",
        "    # plt.plot(c_array, accuracy_array)\n",
        "    # plt.xlabel('C')\n",
        "    # plt.ylabel('Accuracy')\n",
        "    # plt.title('Accuracy vs C for Linear Kernel ' + \"PneumoniaMNIST\")\n",
        "    # # show\n",
        "    # plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#main function\n",
        "def main():\n",
        "\n",
        "    #if directory binary_svm does not exist, create it\n",
        "    if not os.path.exists('binary_svm'):\n",
        "        os.makedirs('binary_svm')\n",
        "\n",
        "\n",
        "    path = 'drive/MyDrive/Colab Notebooks/PRNN_A1_DATA'\n",
        "    # path = os.getcwd()\n",
        "\n",
        "    # path  = 'pneumoniamnist.npz'\n",
        "    SVM(path)\n",
        "    plot_results()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# if main\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yVSY7n5-lgBi"
      },
      "outputs": [],
      "source": [
        "#SVM multi class bloodmnist\n",
        "# we are implementing a simple SVM classifier for the  BloodMNIST dataset with 8 classes\n",
        "# the dataset is at the folder: bloodmnist.npz in the same directory as this file\n",
        "\n",
        "\n",
        "#global variables\n",
        "decision_function_shape = 'ovr'\n",
        "break_ties = False\n",
        "\n",
        "\n",
        "\n",
        "#importing libraries\n",
        "import csv\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score, precision_score, recall_score, roc_curve\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "#importing svm\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import Normalize\n",
        "\n",
        "\n",
        "from sklearn import svm\n",
        "import sklearn.model_selection as model_selection\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "\n",
        "class MidpointNormalize(Normalize):\n",
        "    def __init__(self, vmin=None, vmax=None, midpoint=None, clip=False):\n",
        "        self.midpoint = midpoint\n",
        "        Normalize.__init__(self, vmin, vmax, clip)\n",
        "\n",
        "    def __call__(self, value, clip=None):\n",
        "        x, y = [self.vmin, self.midpoint, self.vmax], [0, 0.5, 1]\n",
        "        return np.ma.masked_array(np.interp(value, x, y))\n",
        "\n",
        "\n",
        "\n",
        "def data_preprocessing(path):\n",
        "    data_path = path + '/bloodmnist.npz'\n",
        "    data = np.load(data_path)\n",
        "    # print(data.files)\n",
        "    # Reshape images and save in numpy arrays\n",
        "    train1_data = np.reshape(data['train_images'], (11959, 2352))\n",
        "    train1_labels = np.reshape(data['train_labels'], (11959))\n",
        "    #change data type to float of train1_data\n",
        "    train1_data = train1_data.astype(float)\n",
        "    #change datatype of train1_labels to int\n",
        "    train1_labels = train1_labels.astype(int)\n",
        "\n",
        "    #validation data\n",
        "    valid1_data = np.reshape(data['val_images'], (1712, 2352))\n",
        "    valid1_labels = np.reshape(data['val_labels'], (1712))\n",
        "    #change datatype of valid1_data to float\n",
        "    valid1_data = valid1_data.astype(float)\n",
        "    #change datatype of valid1_labels to int\n",
        "    valid1_labels = valid1_labels.astype(int)\n",
        "\n",
        "\n",
        "    test1_data = np.reshape(data['test_images'], (3421, 2352))\n",
        "    test1_labels = np.reshape(data['test_labels'], (3421))\n",
        "    #change data type to float of test1_data\n",
        "    test1_data = test1_data.astype(float)\n",
        "    #change datatype of test1_labels to int\n",
        "    test1_labels = test1_labels.astype(int)\n",
        "\n",
        "    # feature scaling\n",
        "    sc = StandardScaler()\n",
        "    X_train = sc.fit_transform(train1_data , train1_labels)\n",
        "    X_test = sc.fit_transform(test1_data, test1_labels)\n",
        "    X_valid = sc.fit_transform(valid1_data, valid1_labels)\n",
        "    # #change label of train1_labels to -1\n",
        "    # train1_labels[train1_labels == 0] = -1\n",
        "    Y_train = train1_labels\n",
        "    # #change label of test1_labels to -1\n",
        "    # test1_labels[test1_labels == 0] = -1\n",
        "    Y_test = test1_labels\n",
        "    # #change label of valid1_labels to -1\n",
        "    # valid1_labels[valid1_labels == 0] = -1\n",
        "    Y_valid = valid1_labels\n",
        "\n",
        "    #print y_train\n",
        "    # print(Y_train)\n",
        "\n",
        "    return X_train, Y_train, X_test, Y_test , X_valid, Y_valid\n",
        "\n",
        "\n",
        "# def validation_dataset(path):\n",
        "#     data = np.load(path)\n",
        "#     # print(data.files)\n",
        "#     # Reshape images and save in numpy arrays\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#  Different grid search methods based on types of kernels\n",
        "\n",
        "def grid_search_poly(X_train, Y_train,  X_valid, Y_valid ):\n",
        "\n",
        "    #create csv file for the results name: results_poly.csv\n",
        "    with open('multi_svm/results_poly.csv', 'w') as f:\n",
        "        f.write('C, degree, gamma, accuracy, precision, recall, f1\\n')\n",
        "\n",
        "\n",
        "    #SVM with C = 1 and degree = 2 and gamma = 0.1\n",
        "    clf = SVC(kernel='poly', C=1, degree=2)\n",
        "\n",
        "    clf.fit(X_train, Y_train)\n",
        "    Y_pred = clf.predict(X_valid)\n",
        "    #f1 score\n",
        "    f1 = f1_score(Y_valid, Y_pred, average='micro')\n",
        "    optimal_f1 = f1\n",
        "    optimal_C = 1\n",
        "    optimal_degree = 2\n",
        "\n",
        "    optimal_classifier = clf\n",
        "    optimal_accuracy = clf.score(X_valid, Y_valid)\n",
        "\n",
        "\n",
        "\n",
        "    #for testing\n",
        "    # c_range = [0.1, 1, 10]\n",
        "    # degree_range = [ 2 , 3]\n",
        "    # gamma_range = [0.1 ]\n",
        "\n",
        "    # c_range = np.logspace(-2, 10, 13)\n",
        "    # degree_range = [ 2, 3, 4, 5, 6 , 7, 8, 9, 10, 100, 1000, 10000]\n",
        "    # gamma_range = np.logspace(-9, 3, 13)\n",
        "\n",
        "    # c_range = [0.001,  1, 10, 100, 1000]\n",
        "    # degree_range = [ 2,  5, 25, 100]\n",
        "    # gamma_range = [ 0.1, 1, 10,  1000]\n",
        "\n",
        "    c_range =[    0.001, 1, 100]\n",
        "    degree_range = [2, 10, 100]\n",
        "    # gamma_range = [ 0.01, 10, 1000]\n",
        "    gamma_range = [0.1]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    for C in c_range:\n",
        "        for degree in degree_range:\n",
        "            for gamma in gamma_range:\n",
        "                clf = SVC(kernel='poly', C=C, degree=degree )\n",
        "                clf.fit(X_train, Y_train)\n",
        "                Y_pred = clf.predict(X_valid)\n",
        "                # f1 score\n",
        "                f1 = f1_score(Y_valid, Y_pred, average='micro')\n",
        "                # accuracy\n",
        "                accuracy = clf.score(X_valid, Y_valid)\n",
        "                # precision\n",
        "                precision = precision_score(Y_valid, Y_pred, average='micro')\n",
        "                # recall\n",
        "                recall = recall_score(Y_valid, Y_pred, average='micro')\n",
        "                # write results to csv file\n",
        "                with open('multi_svm/results_poly.csv', 'a') as f:\n",
        "                    f.write('{}, {}, {}, {}, {}, {}, {}\\n'.format(C, degree, gamma, accuracy, precision, recall, f1))\n",
        "\n",
        "                if f1 > optimal_f1:\n",
        "                    optimal_f1 = f1\n",
        "                    optimal_C = C\n",
        "                    optimal_degree = degree\n",
        "\n",
        "                    optimal_classifier = clf\n",
        "                    optimal_accuracy = accuracy\n",
        "\n",
        "\n",
        "\n",
        "    optimal_hyperparameters = {'C': optimal_C, 'degree': optimal_degree}\n",
        "    optimal_metrics = {'accuracy': optimal_accuracy, 'f1': optimal_f1}\n",
        "    return optimal_classifier, optimal_hyperparameters, optimal_metrics\n",
        "\n",
        "\n",
        "\n",
        "def grid_search_sigmoid(X_train, Y_train,  X_valid, Y_valid):\n",
        "    #create csv file for the results name: results_sigmoid.csv\n",
        "    with open('multi_svm/results_sigmoid.csv', 'w') as f:\n",
        "        f.write('C, gamma, accuracy, precision, recall, f1\\n')\n",
        "\n",
        "    #SVM with C = 1 and gamma = 0.1\n",
        "    clf = SVC(kernel='sigmoid', C=1, gamma=0.1)\n",
        "\n",
        "    clf.fit(X_train, Y_train)\n",
        "    Y_pred = clf.predict(X_valid)\n",
        "    # f1 score\n",
        "    f1 = f1_score(Y_valid, Y_pred, average='micro')\n",
        "    optimal_f1 = f1\n",
        "    optimal_C = 1\n",
        "\n",
        "    optimal_gamma = 0.1\n",
        "    optimal_classifier = clf\n",
        "    optimal_accuracy = clf.score(X_valid, Y_valid)\n",
        "\n",
        "    #for testing\n",
        "    # c_range = [1]\n",
        "    # gamma_range = [0.1]\n",
        "\n",
        "    # c_range = np.logspace(-2, 10, 13)\n",
        "    # gamma_range = np.logspace(-9, 3, 13)\n",
        "    # c_range = [0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]\n",
        "    #\n",
        "    # gamma_range = [ 0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
        "\n",
        "    # c_range = [0.001,  1, 10, 100, 1000]\n",
        "    #\n",
        "    # gamma_range = [ 0.1, 1, 10,  1000]\n",
        "\n",
        "    c_range = [0.001,  1, 100]\n",
        "\n",
        "    gamma_range = [ 0.01, 10, 1000]\n",
        "\n",
        "    for C in c_range:\n",
        "        for gamma in gamma_range:\n",
        "            clf = SVC(kernel='sigmoid', C=C, gamma=gamma)\n",
        "            clf.fit(X_train, Y_train)\n",
        "            Y_pred = clf.predict(X_valid)\n",
        "            # f1 score\n",
        "            f1 = f1_score(Y_valid, Y_pred, average='micro')\n",
        "            # accuracy\n",
        "            accuracy = clf.score(X_valid, Y_valid)\n",
        "            # precision\n",
        "            precision = precision_score(Y_valid, Y_pred, average='micro')\n",
        "            # recall\n",
        "            recall = recall_score(Y_valid, Y_pred, average='micro')\n",
        "            # write results to csv file\n",
        "            with open('multi_svm/results_sigmoid.csv', 'a') as f:\n",
        "                f.write('{}, {}, {}, {}, {}, {}\\n'.format(C, gamma, accuracy, precision, recall, f1))\n",
        "\n",
        "\n",
        "            if f1 > optimal_f1:\n",
        "                optimal_f1 = f1\n",
        "                optimal_C = C\n",
        "                optimal_gamma = gamma\n",
        "                optimal_classifier = clf\n",
        "                optimal_accuracy = accuracy\n",
        "\n",
        "    optimal_hyperparameters = {'C': optimal_C, 'gamma': optimal_gamma}\n",
        "    optimal_metrics = {'accuracy': optimal_accuracy, 'f1': optimal_f1}\n",
        "    return optimal_classifier, optimal_hyperparameters, optimal_metrics\n",
        "\n",
        "def grid_search_gaussian(X_train, Y_train,  X_valid, Y_valid):\n",
        "    #create csv file for the results name: results_gaussian.csv\n",
        "    with open('multi_svm/results_gaussian.csv', 'w') as f:\n",
        "        f.write('C, gamma, accuracy, precision, recall, f1\\n')\n",
        "\n",
        "    #SVM with C = 1 and gamma = 0.1\n",
        "    clf = SVC(kernel='rbf', C=1, gamma=0.1)\n",
        "\n",
        "    clf.fit(X_train, Y_train)\n",
        "    Y_pred = clf.predict(X_valid)\n",
        "    # f1 score\n",
        "    f1 = f1_score(Y_valid, Y_pred, average='micro')\n",
        "    optimal_f1 = f1\n",
        "    optimal_C = 1\n",
        "\n",
        "    optimal_gamma = 0.1\n",
        "    optimal_classifier = clf\n",
        "    optimal_accuracy = clf.score(X_valid, Y_valid)\n",
        "\n",
        "    #for testing\n",
        "    # c_range = [1]\n",
        "    # gamma_range = [0.1]\n",
        "\n",
        "    # c_range = np.logspace(-2, 10, 13)\n",
        "    # gamma_range = np.logspace(-9, 3, 13)\n",
        "    # c_range = [0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]\n",
        "    #\n",
        "    # gamma_range = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
        "\n",
        "    # c_range = [0.001,  1, 10, 100, 1000]\n",
        "    #\n",
        "    # gamma_range = [ 0.1, 1, 10,  1000]\n",
        "\n",
        "    c_range = [0.001,  1, 100]\n",
        "\n",
        "    gamma_range = [ 0.01, 10, 1000]\n",
        "\n",
        "    for C in c_range:\n",
        "        for gamma in gamma_range:\n",
        "            clf = SVC(kernel='rbf', C=C, gamma=gamma)\n",
        "            clf.fit(X_train, Y_train)\n",
        "            Y_pred = clf.predict(X_valid)\n",
        "            # f1 score\n",
        "            f1 = f1_score(Y_valid, Y_pred, average='micro')\n",
        "            # accuracy\n",
        "            accuracy = clf.score(X_valid, Y_valid)\n",
        "            # precision\n",
        "            precision = precision_score(Y_valid, Y_pred, average='micro')\n",
        "            # recall\n",
        "            recall = recall_score(Y_valid, Y_pred, average='micro')\n",
        "            # write results to csv file\n",
        "            with open('multi_svm/results_gaussian.csv', 'a') as f:\n",
        "                f.write('{}, {}, {}, {}, {}, {}\\n'.format(C, gamma, accuracy, precision, recall, f1))\n",
        "\n",
        "            if f1 > optimal_f1:\n",
        "                optimal_f1 = f1\n",
        "                optimal_C = C\n",
        "                optimal_gamma = gamma\n",
        "                optimal_classifier = clf\n",
        "                optimal_accuracy = accuracy\n",
        "\n",
        "    optimal_hyperparameters = {'C': optimal_C, 'gamma': optimal_gamma}\n",
        "    optimal_metrics = {'accuracy': optimal_accuracy, 'f1': optimal_f1}\n",
        "    return optimal_classifier, optimal_hyperparameters, optimal_metrics\n",
        "\n",
        "def grid_search_linear(X_train, Y_train,  X_valid, Y_valid):\n",
        "    #create csv file for the results name: results_linear.csv\n",
        "    with open('multi_svm/results_linear.csv', 'w') as f:\n",
        "        f.write('C, accuracy, precision, recall, f1\\n')\n",
        "\n",
        "    #SVM with C = 1\n",
        "    clf = SVC(kernel='linear', C=1)\n",
        "    clf.fit(X_train, Y_train)\n",
        "    Y_pred = clf.predict(X_valid)\n",
        "    # f1 score\n",
        "    f1 = f1_score(Y_valid, Y_pred, average='micro')\n",
        "    optimal_f1 = f1\n",
        "    optimal_C = 1\n",
        "    optimal_degree = 2\n",
        "    optimal_gamma = 0.1\n",
        "    optimal_classifier = clf\n",
        "    optimal_accuracy = clf.score(X_valid, Y_valid)\n",
        "\n",
        "    # for testing\n",
        "    # c_range = [1]\n",
        "\n",
        "    # c_range = np.logspace(-2, 10, 13)\n",
        "    # c_range = [0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]\n",
        "    #\n",
        "    # gamma_range = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
        "\n",
        "    # c_range = [0.001,  1, 10, 100, 1000]\n",
        "    #\n",
        "    # gamma_range = [ 0.1, 1, 10,  1000]\n",
        "\n",
        "    c_range = [0.001,  1, 100]\n",
        "\n",
        "    gamma_range = [ 0.01, 10, 1000]\n",
        "\n",
        "    for C in c_range:\n",
        "        clf = SVC(kernel='linear', C=C)\n",
        "        clf.fit(X_train, Y_train)\n",
        "        Y_pred = clf.predict(X_valid)\n",
        "        # f1 score\n",
        "        f1 = f1_score(Y_valid, Y_pred, average='micro')\n",
        "        # accuracy\n",
        "\n",
        "        accuracy = clf.score(X_valid, Y_valid)\n",
        "\n",
        "        # precision\n",
        "        precision = precision_score(Y_valid, Y_pred, average='micro')\n",
        "        # recall\n",
        "        recall = recall_score(Y_valid, Y_pred, average='micro')\n",
        "        # write results to csv file\n",
        "        with open('multi_svm/results_linear.csv', 'a') as f:\n",
        "            f.write('{}, {}, {}, {}, {}\\n'.format(C, accuracy, precision, recall, f1))\n",
        "\n",
        "        if f1 > optimal_f1:\n",
        "            optimal_f1 = f1\n",
        "            optimal_C = C\n",
        "            optimal_classifier = clf\n",
        "            optimal_accuracy = accuracy\n",
        "\n",
        "    optimal_hyperparameters = {'C': optimal_C}\n",
        "    optimal_metrics = {'accuracy': optimal_accuracy, 'f1': optimal_f1}\n",
        "    return optimal_classifier, optimal_hyperparameters, optimal_metrics\n",
        "\n",
        "\n",
        "\n",
        "#this function plots colormap where x and y coordinates of colourmap are the hyperparameters and colourmap is the accuracy of the classifier\n",
        "def plot_hyperparameter_accuracy(csv_file, midpoint, title, name , hyperparameters_to_plot= ['C', ' gamma']):\n",
        "    print()\n",
        "    hyperparameter_1 = hyperparameters_to_plot[0]\n",
        "    hyperparameter_2 = hyperparameters_to_plot[1]\n",
        "\n",
        "    # Draw heatmap of the validation accuracy as a function of gamma and C\n",
        "    #   read csv file\n",
        "\n",
        "    #plotting the validation accuracy vs hyperparameters colormap of linear kernel\n",
        "    #   read csv file and store value of accuracy in a 2d array\n",
        "    # one axis of array is gamma and other is C\n",
        "    #reading csv file\n",
        "    accuracy_list = []\n",
        "    c_list = []\n",
        "    gamma_list = []\n",
        "    with open(csv_file, 'r') as csvfile:\n",
        "        reader = csv.DictReader(csvfile)\n",
        "        line_count = 0\n",
        "        # the columns that we want to read are 'C', 'gamma', 'accuracy'\n",
        "        # the first row is the header\n",
        "        # the rest of the rows are the data\n",
        "        for row in reader:\n",
        "            if line_count == 0:\n",
        "                # this is the header\n",
        "                # print(f'Column names are {\", \".join(row)}')\n",
        "                line_count += 1\n",
        "            #save the accuracy values for all rows in a list\n",
        "            accuracy_list.append(row[' accuracy'])\n",
        "            c_list.append(row[hyperparameter_1])\n",
        "            gamma_list.append(row[hyperparameter_2])\n",
        "            line_count += 1\n",
        "    #print the list c_list ands gamma_list\n",
        "    print(c_list)\n",
        "    print(gamma_list)\n",
        "\n",
        "\n",
        "    #convert the list to numpy array\n",
        "    #convert list of strings to list of floats\n",
        "    accuracy_list = [float(i) for i in accuracy_list]\n",
        "    c_list = [float(i) for i in c_list]\n",
        "    gamma_list = [float(i) for i in gamma_list]\n",
        "    #convert list of floats to numpy array\n",
        "    accuracy_array = np.array(accuracy_list)\n",
        "    c_array = np.array(c_list)\n",
        "    gamma_array = np.array(gamma_list)\n",
        "    #reshape the array to 2d array\n",
        "\n",
        "    #define c_len to be no. of unique values of c_array\n",
        "    c_len = len(np.unique(c_array))\n",
        "    #define gamma_len to be no. of unique values of gamma_array\n",
        "    gamma_len = len(np.unique(gamma_array))\n",
        "\n",
        "    #c_array to have unique values of c_array\n",
        "    c_array = np.unique(c_array)\n",
        "    #gamma_array to have unique values of gamma_array\n",
        "    gamma_array = np.unique(gamma_array)\n",
        "    #c_array to be sorted in ascending order\n",
        "    c_array = np.sort(c_array)\n",
        "    #gamma_array to be sorted in ascending order\n",
        "    gamma_array = np.sort(gamma_array)\n",
        "\n",
        "    reshaped_array = np.reshape(accuracy_array, (len(c_array), len(gamma_array)))\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.subplots_adjust(left=0.2, right=0.95, bottom=0.15, top=0.95)\n",
        "    plt.imshow(\n",
        "        reshaped_array,\n",
        "        interpolation=\"nearest\",\n",
        "        cmap=plt.cm.hot,\n",
        "        norm=MidpointNormalize(vmin=0.2, midpoint=midpoint, vmax=1)\n",
        "    )\n",
        "    plt.xlabel(hyperparameter_2)\n",
        "    plt.ylabel(hyperparameter_1)\n",
        "    plt.colorbar()\n",
        "    plt.xticks(np.arange(len(gamma_array)), gamma_array, rotation=45)\n",
        "    plt.yticks(np.arange(len(c_array)), c_array)\n",
        "    plt.title(\"Validation accuracy \" + title + \" \"+ name)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def extract_accuracy(csv_file):\n",
        "    accuracy_list = []\n",
        "    c_list = []\n",
        "\n",
        "    with open(csv_file, 'r') as csvfile:\n",
        "        reader = csv.DictReader(csvfile)\n",
        "        line_count = 0\n",
        "        # the columns that we want to read are 'C', 'gamma', 'accuracy'\n",
        "        # the first row is the header\n",
        "        # the rest of the rows are the data\n",
        "        for row in reader:\n",
        "            if line_count == 0:\n",
        "                # this is the header\n",
        "                # print(f'Column names are {\", \".join(row)}')\n",
        "                line_count += 1\n",
        "            # save the accuracy values for all rows in a list\n",
        "            accuracy_list.append(row[' accuracy'])\n",
        "            c_list.append(row['C'])\n",
        "\n",
        "            line_count += 1\n",
        "\n",
        "    # convert list of strings to list of floats\n",
        "    accuracy_list = [float(i) for i in accuracy_list]\n",
        "    c_list = [float(i) for i in c_list]\n",
        "\n",
        "    # convert list of floats to numpy array\n",
        "    accuracy_array = np.array(accuracy_list)\n",
        "    c_array = np.array(c_list)\n",
        "\n",
        "\n",
        "\n",
        "    # c_array to have unique values of c_array\n",
        "    c_array = np.unique(c_array)\n",
        "\n",
        "    # c_array to be sorted in ascending order\n",
        "    c_array = np.sort(c_array)\n",
        "\n",
        "    return accuracy_array, c_array\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#plotting the results\n",
        "def plot_results():\n",
        "    name = \"BloodMNIST\"\n",
        "\n",
        "    #plotting the colormap of sigmoide kernel\n",
        "    plot_hyperparameter_accuracy('multi_svm/results_sigmoid.csv' , 0.4 , 'Sigmoid Kernel', name)\n",
        "\n",
        "    #plotting the colormap of gaussian kernel\n",
        "    plot_hyperparameter_accuracy('multi_svm/results_gaussian.csv' , 0.18 , 'Gaussian Kernel', name)\n",
        "\n",
        "    #plotting the colormap of polynomial kernel\n",
        "    # plot_hyperparameter_accuracy('multi_svm/results_poly.csv' , 0.7 , 'Polynomial Kernel', name , ['C', ' degree'])\n",
        "\n",
        "\n",
        "    #plotting the graph of linear kernel\n",
        "    accuracy_array, c_array = extract_accuracy('multi_svm/results_linear.csv')\n",
        "    #plot the accuracy vs c graph\n",
        "    plt.plot(c_array, accuracy_array)\n",
        "    plt.xlabel('C')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Accuracy vs C for Linear Kernel ' + name)\n",
        "    #show\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#plotting AUC curve of the best classifier\n",
        "def plot_AUC(classifier, X_test, Y_test):\n",
        "    name = \"BloodMNIST\"\n",
        "    #get the predicted labels\n",
        "    Y_pred = classifier.predict(X_test)\n",
        "    #get the predicted probabilities\n",
        "    Y_prob = classifier.predict_proba(X_test)\n",
        "    #get the AUC\n",
        "    fpr, tpr, thresholds = roc_curve(Y_test, Y_prob[:,1])\n",
        "    #plot the AUC\n",
        "    plt.plot(fpr, tpr)\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('ROC Curve for ' + name)\n",
        "    #show\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "def Multi_Class_SVM(path):\n",
        "    # getting the data\n",
        "    X_train, Y_train, X_test, Y_test, X_valid, Y_valid = data_preprocessing(path)\n",
        "    # SVM\n",
        "    # clf = SVC(kernel='linear', C=1)\n",
        "    # clf.fit(X_train, Y_train)\n",
        "    # Y_pred = clf.predict(X_test)\n",
        "    # # print(classification_report(Y_test, Y_pred))\n",
        "    # print(y_pred)\n",
        "    # print(test1_labels)\n",
        "    # print(y_pred == test1_labels)\n",
        "    # print(np.sum(y_pred == test1_labels))\n",
        "    # print(np.sum(y_pred == test1_labels) / 624)\n",
        "    # print(np.sum(y_pred == test1_labels) / 624 * 100)\n",
        "\n",
        "    # print(\"Accuracy of SVM classifier on test set: {:.2f}%\".format(np.sum(Y_pred == Y_test) / 624 * 100))\n",
        "\n",
        "    # print(clf.score(X_test, test1_labels))\n",
        "    # print(clf.score(X_train, train1_labels))\n",
        "\n",
        "    # calling grid search poly\n",
        "    print(\"Grid Search for Polynomial Kernel\")\n",
        "    optimal_classifier, optimal_hyperparameters, optimal_metrics = grid_search_poly(X_train, Y_train, X_valid, Y_valid)\n",
        "    # printing the results\n",
        "    print(\"Optimal C: {}\".format(optimal_hyperparameters['C']))\n",
        "    print(\"Optimal degree: {}\".format(optimal_hyperparameters['degree']))\n",
        "\n",
        "    print(\"Optimal f1: {}\".format(optimal_metrics['f1']))\n",
        "    print(\"Optimal accuracy: {}\".format(optimal_metrics['accuracy']))\n",
        "\n",
        "    # # calling grid search sigmoid\n",
        "    # print(\"Grid Search for Sigmoid Kernel\")\n",
        "    # optimal_classifier, optimal_hyperparameters, optimal_metrics = grid_search_sigmoid(X_train, Y_train, X_valid,\n",
        "    #                                                                                    Y_valid)\n",
        "    # # printing the results\n",
        "    # print(\"Optimal C: {}\".format(optimal_hyperparameters['C']))\n",
        "    # print(\"Optimal gamma: {}\".format(optimal_hyperparameters['gamma']))\n",
        "    # print(\"Optimal f1: {}\".format(optimal_metrics['f1']))\n",
        "    # print(\"Optimal accuracy: {}\".format(optimal_metrics['accuracy']))\n",
        "    #\n",
        "    # # calling grid search gaussian\n",
        "    # print(\"Grid Search for Gaussian Kernel\")\n",
        "    # optimal_classifier, optimal_hyperparameters, optimal_metrics = grid_search_gaussian(X_train, Y_train, X_valid,\n",
        "    #                                                                                     Y_valid)\n",
        "    # # printing the results\n",
        "    # print(\"Optimal C: {}\".format(optimal_hyperparameters['C']))\n",
        "    # print(\"Optimal gamma: {}\".format(optimal_hyperparameters['gamma']))\n",
        "    # print(\"Optimal f1: {}\".format(optimal_metrics['f1']))\n",
        "    # print(\"Optimal accuracy: {}\".format(optimal_metrics['accuracy']))\n",
        "    #\n",
        "    # # # calling grid search linear\n",
        "    # print(\"Grid Search for Linear Kernel\")\n",
        "    # optimal_classifier, optimal_hyperparameters, optimal_metrics = grid_search_linear(X_train, Y_train, X_valid,\n",
        "    #                                                                                   Y_valid)\n",
        "    # # printing the results\n",
        "    # print(\"Optimal C: {}\".format(optimal_hyperparameters['C']))\n",
        "    # print(\"Optimal f1: {}\".format(optimal_metrics['f1']))\n",
        "    # print(\"Optimal accuracy: {}\".format(optimal_metrics['accuracy']))\n",
        "\n",
        "    # plotting\n",
        "    # plotting the colormap of polynomial kernel\n",
        "    # plot_hyperparameter_accuracy('multi_svm/results_poly.csv', 0.9, 'Polynomial Kernel', \"BloodMNIST\", ['C', ' degree'])\n",
        "\n",
        "    # # plotting the graph of linear kernel\n",
        "    # accuracy_array, c_array = extract_accuracy('multi_svm/results_linear.csv')\n",
        "    #\n",
        "    # #print accuracy_array and c_array\n",
        "    # print(accuracy_array)\n",
        "    # print(c_array)\n",
        "    #\n",
        "    # # plot the accuracy vs c graph\n",
        "    # plt.plot(c_array, accuracy_array)\n",
        "    # plt.xlabel('C')\n",
        "    # plt.ylabel('Accuracy')\n",
        "    # plt.title('Accuracy vs C for Linear Kernel ' + \"BloodMNIST\")\n",
        "    # # show\n",
        "    # plt.show()\n",
        "\n",
        "\n",
        "#main function\n",
        "def main():\n",
        "    # if directory multi_svm does not exist, create it\n",
        "    if not os.path.exists('multi_svm'):\n",
        "        os.makedirs('multi_svm')\n",
        "    # path = 'drive/MyDrive/Colab Notebooks/PRNN_A1_DATA/bloodmnist.npz'\n",
        "    path = os.getcwd()\n",
        "\n",
        "    # path  = 'bloodmnist.npz'\n",
        "    Multi_Class_SVM(path)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# if main\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d7xPs_7plqz2"
      },
      "outputs": [],
      "source": [
        "#SVM audio data\n",
        "#this code will implement SVM for binary audio data\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# we are implementing a simple SVM classifier for the pneumonia MNIST dataset\n",
        "# the dataset is at the folder:  in the same directory as this file\n",
        "\n",
        "#result folder is: audio_data\n",
        "\n",
        "\n",
        "\n",
        "#importing libraries\n",
        "import csv\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score, precision_score, recall_score, roc_curve\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "#importing svm\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import Normalize\n",
        "import IPython.display as ipd  # To play sound in the notebook\n",
        "from scipy.io import wavfile # for reading wave files as numpy arrays\n",
        "import wave # opening .wav files\n",
        "import struct # for padding\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import matplotlib.pyplot as plt # visualizations\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os # operating system\n",
        "from os.path import join\n",
        "import time\n",
        "\n",
        "import os\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "#global variables\n",
        "#PLEASE SAVE PATH TO TIMIT DATASET IN PATH var\n",
        "path = 'darpa/'\n",
        "# path = 'drive/MyDrive/Colab Notebooks/PRNN_A1_DATA'\n",
        "tolerence = 0.1\n",
        "max_iteration = 4000\n",
        "\n",
        "\n",
        "RATE = 16000\n",
        "data_dir = path +\"data\"\n",
        "train_csv_file = path+\"train_data.csv\"\n",
        "test_csv_file = path+\"test_data.csv\"\n",
        "\n",
        "class MidpointNormalize(Normalize):\n",
        "    def __init__(self, vmin=None, vmax=None, midpoint=None, clip=False):\n",
        "        self.midpoint = midpoint\n",
        "        Normalize.__init__(self, vmin, vmax, clip)\n",
        "\n",
        "    def __call__(self, value, clip=None):\n",
        "        x, y = [self.vmin, self.midpoint, self.vmax], [0, 0.5, 1]\n",
        "        return np.ma.masked_array(np.interp(value, x, y))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def add_padding(data, length) :\n",
        "    padded_data = []\n",
        "    for row in data :\n",
        "        x1 = np.zeros(length)\n",
        "        if row[0].shape[0] > length:\n",
        "            x1 = row[0][:length]\n",
        "        else:\n",
        "            x1[:row[0].shape[0]] = row[0]\n",
        "        padded_data.append((x1, row[1]))\n",
        "    return padded_data\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def make_data_set(train_csv_file):\n",
        "    train_csv = get_good_audio_files(train_csv_file)\n",
        "    train_csv['filepath'] = train_csv.apply(lambda row: join_dirs(row), axis=1)\n",
        "    waves = train_csv['filepath']\n",
        "    audio_data = [read_audio(wave) for wave in waves]\n",
        "    wrds = [wave.replace('.WAV.wav', '') + '.PHN' for wave in waves]\n",
        "    word_data = [parse_phn_timestamps(wrd) for wrd in wrds]\n",
        "    train_data = [align_data(audio, wrd) for audio, wrd in zip(audio_data, word_data)]\n",
        "    train_data = [item for sublist in train_data for item in sublist]\n",
        "    return train_data\n",
        "\n",
        "\n",
        "def align_data(data, words, verbose=False):\n",
        "    aligned = []\n",
        "    print('len(data)', len(data)) if verbose else None\n",
        "    print('len(words)', len(words)) if verbose else None\n",
        "    print('data', data) if verbose else None\n",
        "    print('words', words) if verbose else None\n",
        "    for tup in words[1:-1]:\n",
        "        print('tup', tup) if verbose else None\n",
        "        start = int(tup[0])\n",
        "        end = int(tup[1])\n",
        "        word = tup[2]\n",
        "        assert start >= 0\n",
        "        assert end <= len(data)\n",
        "        aligned.append((data[start:end], word))\n",
        "    return aligned\n",
        "\n",
        "\n",
        "def parse_word_waves(time_aligned_words, audio_data, verbose=False):\n",
        "    return [align_data(data, words, verbose) for data, words in zip(audio_data, time_aligned_words)]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def read_audio(wave_path, verbose=False):\n",
        "    rate, data = wavfile.read(wave_path)\n",
        "    # make sure the rate of the file is the RATE that we want\n",
        "    assert rate == RATE\n",
        "    print(\"Sampling (frame) rate = \", rate) if verbose else None\n",
        "    print(\"Total samples (frames) = \", data.shape) if verbose else None\n",
        "    return data\n",
        "\n",
        "def join_dirs(row):\n",
        "    return os.path.join(data_dir,\n",
        "                       row['test_or_train'],\n",
        "                       row['dialect_region'],\n",
        "                       row['speaker_id'],\n",
        "                       row['filename'])\n",
        "\n",
        "def parse_phn_timestamps(wrd_path, verbose=False):\n",
        "    print('phn_path', wrd_path) if verbose else None\n",
        "    speaker_id = wrd_path.split('/')[-2]\n",
        "    sentence_id = wrd_path.split('/')[-1].replace('.PHN', '')\n",
        "    wrd_file = open(wrd_path)\n",
        "    content = wrd_file.read()\n",
        "    content = content.split('\\n')\n",
        "    content = [tuple(foo.split(' ') + [speaker_id, sentence_id]) for foo in content if foo != '']\n",
        "    wrd_file.close()\n",
        "    return content\n",
        "\n",
        "def get_good_audio_files(filename):\n",
        "    df = pd.read_csv(filename)\n",
        "    return df[df['is_converted_audio'] == True]\n",
        "\n",
        "def data_preprocessing():\n",
        "    print()\n",
        "\n",
        "    train_data = make_data_set(train_csv_file)\n",
        "    test_data = make_data_set(test_csv_file)\n",
        "\n",
        "    vowels = [\"iy\", \"ih\", \"eh\", \"ey\", \"ae\", \"aa\", \"aw\", \"ay\", \"ah\", \"ao\", \"oy\", \"ow\", \"uh\", \"uw\", \"ux\", \"er\", \"ax\",\n",
        "              \"ix\", \"axr\", \"ax-h\"]\n",
        "\n",
        "    train_data = add_padding(train_data, 100)\n",
        "    test_data = add_padding(test_data, 100)\n",
        "\n",
        "\n",
        "\n",
        "    trainX = np.array([i[0] for i in train_data])\n",
        "    trainY = np.array([int(i[1] in vowels) for i in train_data])\n",
        "\n",
        "    testX = np.array([i[0] for i in test_data])\n",
        "    testY = np.array([int(i[1] in vowels) for i in test_data])\n",
        "    #     train1_data ,valid1_data, train1_labels ,valid1_labels  = train_test_split(trainX, trainY,  test_size=0.2, random_state=42)\n",
        "    train1_data = trainX\n",
        "    train1_labels = trainY\n",
        "\n",
        "\n",
        "    test1_data = testX\n",
        "    test1_labels = testY\n",
        "\n",
        "    #split train data into train and validation\n",
        "    train1_data ,valid1_data, train1_labels ,valid1_labels  = train_test_split(train1_data, train1_labels,  test_size=0.2, random_state=42)\n",
        "\n",
        "    train1_data = train1_data.astype(float)\n",
        "    # change datatype of train1_labels to int\n",
        "    train1_labels = train1_labels.astype(int)\n",
        "\n",
        "    # validation data\n",
        "    valid1_data = valid1_data.astype(float)\n",
        "    # change datatype of valid1_labels to int\n",
        "    valid1_labels = valid1_labels.astype(int)\n",
        "\n",
        "    #     #change datatype of valid1_data to float\n",
        "    #     valid1_data = valid1_data.astype(float)\n",
        "    #     #change datatype of valid1_labels to int\n",
        "    #     valid1_labels = valid1_labels.astype(int)\n",
        "\n",
        "    test1_data = testX\n",
        "    test1_labels = testY\n",
        "    # change data type to float of test1_data\n",
        "    test1_data = test1_data.astype(float)\n",
        "    # change datatype of test1_labels to int\n",
        "    test1_labels = test1_labels.astype(int)\n",
        "\n",
        "    # feature scaling\n",
        "    sc = StandardScaler()\n",
        "    X_train = sc.fit_transform(train1_data, train1_labels)\n",
        "    X_test = sc.fit_transform(test1_data, test1_labels)\n",
        "    X_valid = sc.fit_transform(valid1_data, valid1_labels)\n",
        "\n",
        "    # change label of train1_labels to -1\n",
        "    train1_labels[train1_labels == 0] = -1\n",
        "    Y_train = train1_labels\n",
        "    # change label of valid1_labels to -1\n",
        "    valid1_labels[valid1_labels == 0] = -1\n",
        "    Y_valid = valid1_labels\n",
        "    # change label of test1_labels to -1\n",
        "    test1_labels[test1_labels == 0] = -1\n",
        "    Y_test = test1_labels\n",
        "\n",
        "\n",
        "    return X_train, Y_train, X_test, Y_test, X_valid, Y_valid\n",
        "\n",
        "\n",
        "#\n",
        "    #\n",
        "    # train1_data ,valid1_data, train1_labels ,valid1_labels  = train_test_split(trainX, trainY,  test_size=0.2, random_state=42)\n",
        "    #\n",
        "    # train1_data = train1_data.astype(float)\n",
        "    # #change datatype of train1_labels to int\n",
        "    # train1_labels = train1_labels.astype(int)\n",
        "    #\n",
        "    # #validation data\n",
        "    #\n",
        "    # #change datatype of valid1_data to float\n",
        "    # valid1_data = valid1_data.astype(float)\n",
        "    # #change datatype of valid1_labels to int\n",
        "    # valid1_labels = valid1_labels.astype(int)\n",
        "    #\n",
        "    #\n",
        "    # test1_data = testX\n",
        "    # test1_labels = testY\n",
        "    # #change data type to float of test1_data\n",
        "    # test1_data = test1_data.astype(float)\n",
        "    # #change datatype of test1_labels to int\n",
        "    # test1_labels = test1_labels.astype(int)\n",
        "    #\n",
        "    # # feature scaling\n",
        "    # sc = StandardScaler()\n",
        "    # X_train = sc.fit_transform(train1_data , train1_labels)\n",
        "    # X_test = sc.fit_transform(test1_data, test1_labels)\n",
        "    # X_valid = sc.fit_transform(valid1_data, valid1_labels)\n",
        "    # #change label of train1_labels to -1\n",
        "    # train1_labels[train1_labels == 0] = -1\n",
        "    # Y_train = train1_labels\n",
        "    # #change label of test1_labels to -1\n",
        "    # test1_labels[test1_labels == 0] = -1\n",
        "    # Y_test = test1_labels\n",
        "    # #change label of valid1_labels to -1\n",
        "    # valid1_labels[valid1_labels == 0] = -1\n",
        "    # Y_valid = valid1_labels\n",
        "    #\n",
        "    # return X_train, Y_train, X_test, Y_test , X_valid, Y_valid\n",
        "\n",
        "\n",
        "\n",
        "# def validation_dataset(path):\n",
        "#     data = np.load(path)\n",
        "#     # print(data.files)\n",
        "#     # Reshape images and save in numpy arrays\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def grid_search_poly(X_train, Y_train,  X_valid, Y_valid ):\n",
        "\n",
        "    #create csv file for the results name: results_poly.csv\n",
        "    with open('audio_data/results_poly.csv', 'w') as f:\n",
        "        f.write('C, degree, gamma, accuracy, precision, recall, f1\\n')\n",
        "\n",
        "\n",
        "    #SVM with C = 1 and degree = 2 and gamma = 0.1\n",
        "    clf = SVC(kernel='poly', C=1, degree=2)\n",
        "\n",
        "    clf.fit(X_train, Y_train)\n",
        "    Y_pred = clf.predict(X_valid)\n",
        "    #f1 score\n",
        "    f1 = f1_score(Y_valid, Y_pred, average='micro')\n",
        "    optimal_f1 = f1\n",
        "    optimal_C = 1\n",
        "    optimal_degree = 2\n",
        "\n",
        "    optimal_classifier = clf\n",
        "    optimal_accuracy = clf.score(X_valid, Y_valid)\n",
        "\n",
        "\n",
        "\n",
        "    #for testing\n",
        "    # c_range = [ 1, ]\n",
        "    # degree_range = [2]\n",
        "    # gamma_range = [0.1]\n",
        "\n",
        "    # c_range = [0.1, 1, 10]\n",
        "    # degree_range = [ 2 , 3]\n",
        "    # gamma_range = [0.1 ]\n",
        "\n",
        "    c_range = np.logspace(-2, 10, 13)\n",
        "    degree_range = [ 2, 3, 4, 5, 6 , 7, 8, 9, 10, 100, 1000, 10000]\n",
        "    gamma_range = np.logspace(-9, 3, 13)\n",
        "\n",
        "    # c_range = [0.001,  1, 100]\n",
        "    # degree_range = [ 2, 10, 100]\n",
        "    # # gamma_range = [ 0.01, 10, 1000]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    for C in c_range:\n",
        "        for degree in degree_range:\n",
        "            for gamma in gamma_range:\n",
        "                clf = SVC(kernel='poly', C=C, degree=degree )\n",
        "                clf.fit(X_train, Y_train)\n",
        "                Y_pred = clf.predict(X_valid)\n",
        "                # f1 score\n",
        "                f1 = f1_score(Y_valid, Y_pred, average='micro')\n",
        "                # accuracy\n",
        "                accuracy = clf.score(X_valid, Y_valid)\n",
        "                # precision\n",
        "                precision = precision_score(Y_valid, Y_pred, average='micro')\n",
        "                # recall\n",
        "                recall = recall_score(Y_valid, Y_pred, average='micro')\n",
        "                # write results to csv file\n",
        "                with open('audio_data/results_poly.csv', 'a') as f:\n",
        "                    f.write('{}, {}, {}, {}, {}, {}, {}\\n'.format(C, degree, gamma, accuracy, precision, recall, f1))\n",
        "\n",
        "                if f1 > optimal_f1:\n",
        "                    optimal_f1 = f1\n",
        "                    optimal_C = C\n",
        "                    optimal_degree = degree\n",
        "\n",
        "                    optimal_classifier = clf\n",
        "                    optimal_accuracy = accuracy\n",
        "\n",
        "\n",
        "\n",
        "    optimal_hyperparameters = {'C': optimal_C, 'degree': optimal_degree}\n",
        "    optimal_metrics = {'accuracy': optimal_accuracy, 'f1': optimal_f1}\n",
        "    return optimal_classifier, optimal_hyperparameters, optimal_metrics\n",
        "\n",
        "\n",
        "\n",
        "def grid_search_sigmoid(X_train, Y_train,  X_valid, Y_valid):\n",
        "    #create csv file for the results name: results_sigmoid.csv\n",
        "    with open('audio_data/results_sigmoid.csv', 'w') as f:\n",
        "        f.write('C, gamma, accuracy, precision, recall, f1\\n')\n",
        "\n",
        "    #SVM with C = 1 and gamma = 0.1\n",
        "    clf = SVC(kernel='sigmoid', C=1, gamma=0.1 , tol=tolerence, max_iter=max_iteration)\n",
        "\n",
        "    clf.fit(X_train, Y_train)\n",
        "    Y_pred = clf.predict(X_valid)\n",
        "    # f1 score\n",
        "    f1 = f1_score(Y_valid, Y_pred, average='micro')\n",
        "    optimal_f1 = f1\n",
        "    optimal_C = 1\n",
        "\n",
        "    optimal_gamma = 0.1\n",
        "    optimal_classifier = clf\n",
        "    optimal_accuracy = clf.score(X_valid, Y_valid)\n",
        "\n",
        "    #for testing\n",
        "    # c_range = [1]\n",
        "    # gamma_range = [0.1]\n",
        "\n",
        "    c_range = np.logspace(-2, 10, 13)\n",
        "    gamma_range = np.logspace(-9, 3, 13)\n",
        "    c_range = [0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]\n",
        "    #\n",
        "    # gamma_range = [ 0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
        "\n",
        "    # c_range = [0.001, 1, 100]\n",
        "    #\n",
        "    # gamma_range = [ 0.01, 10, 1000]\n",
        "\n",
        "    for C in c_range:\n",
        "        for gamma in gamma_range:\n",
        "            clf = SVC(kernel='sigmoid', C=C, gamma=gamma , tol=tolerence, max_iter=max_iteration)\n",
        "            clf.fit(X_train, Y_train)\n",
        "            Y_pred = clf.predict(X_valid)\n",
        "            # f1 score\n",
        "            f1 = f1_score(Y_valid, Y_pred, average='micro')\n",
        "            # accuracy\n",
        "            accuracy = clf.score(X_valid, Y_valid)\n",
        "            # precision\n",
        "            precision = precision_score(Y_valid, Y_pred, average='micro')\n",
        "            # recall\n",
        "            recall = recall_score(Y_valid, Y_pred, average='micro')\n",
        "            # write results to csv file\n",
        "            with open('audio_data/results_sigmoid.csv', 'a') as f:\n",
        "                f.write('{}, {}, {}, {}, {}, {}\\n'.format(C, gamma, accuracy, precision, recall, f1))\n",
        "\n",
        "\n",
        "            if f1 > optimal_f1:\n",
        "                optimal_f1 = f1\n",
        "                optimal_C = C\n",
        "                optimal_gamma = gamma\n",
        "                optimal_classifier = clf\n",
        "                optimal_accuracy = accuracy\n",
        "\n",
        "    optimal_hyperparameters = {'C': optimal_C, 'gamma': optimal_gamma}\n",
        "    optimal_metrics = {'accuracy': optimal_accuracy, 'f1': optimal_f1}\n",
        "    return optimal_classifier, optimal_hyperparameters, optimal_metrics\n",
        "\n",
        "def grid_search_gaussian(X_train, Y_train,  X_valid, Y_valid):\n",
        "    #create csv file for the results name: results_gaussian.csv\n",
        "    with open('audio_data/results_gaussian.csv', 'w') as f:\n",
        "        f.write('C, gamma, accuracy, precision, recall, f1\\n')\n",
        "\n",
        "    #SVM with C = 1 and gamma = 0.1\n",
        "    clf = SVC(kernel='rbf', C=1, gamma=0.1)\n",
        "\n",
        "    clf.fit(X_train, Y_train)\n",
        "    Y_pred = clf.predict(X_valid)\n",
        "    # f1 score\n",
        "    f1 = f1_score(Y_valid, Y_pred, average='micro')\n",
        "    optimal_f1 = f1\n",
        "    optimal_C = 1\n",
        "\n",
        "    optimal_gamma = 0.1\n",
        "    optimal_classifier = clf\n",
        "    optimal_accuracy = clf.score(X_valid, Y_valid)\n",
        "\n",
        "    #for testing\n",
        "    # c_range = [1]\n",
        "    # gamma_range = [0.1]\n",
        "\n",
        "    c_range = np.logspace(-2, 10, 13)\n",
        "    gamma_range = np.logspace(-9, 3, 13)\n",
        "    c_range = [0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]\n",
        "    #\n",
        "    # gamma_range = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
        "    # c_range = [0.001, 1, 100]\n",
        "    #\n",
        "    # gamma_range = [ 0.01, 10, 1000]\n",
        "\n",
        "    for C in c_range:\n",
        "        for gamma in gamma_range:\n",
        "            clf = SVC(kernel='rbf', C=C, gamma=gamma)\n",
        "            clf.fit(X_train, Y_train)\n",
        "            Y_pred = clf.predict(X_valid)\n",
        "            # f1 score\n",
        "            f1 = f1_score(Y_valid, Y_pred, average='micro')\n",
        "            # accuracy\n",
        "            accuracy = clf.score(X_valid, Y_valid)\n",
        "            # precision\n",
        "            precision = precision_score(Y_valid, Y_pred, average='micro')\n",
        "            # recall\n",
        "            recall = recall_score(Y_valid, Y_pred, average='micro')\n",
        "            # write results to csv file\n",
        "            with open('audio_data/results_gaussian.csv', 'a') as f:\n",
        "                f.write('{}, {}, {}, {}, {}, {}\\n'.format(C, gamma, accuracy, precision, recall, f1))\n",
        "\n",
        "            if f1 > optimal_f1:\n",
        "                optimal_f1 = f1\n",
        "                optimal_C = C\n",
        "                optimal_gamma = gamma\n",
        "                optimal_classifier = clf\n",
        "                optimal_accuracy = accuracy\n",
        "\n",
        "    optimal_hyperparameters = {'C': optimal_C, 'gamma': optimal_gamma}\n",
        "    optimal_metrics = {'accuracy': optimal_accuracy, 'f1': optimal_f1}\n",
        "    return optimal_classifier, optimal_hyperparameters, optimal_metrics\n",
        "\n",
        "def grid_search_linear(X_train, Y_train,  X_valid, Y_valid):\n",
        "    #create csv file for the results name: results_linear.csv\n",
        "    with open('audio_data/results_linear.csv', 'w') as f:\n",
        "        f.write('C, accuracy, precision, recall, f1\\n')\n",
        "\n",
        "    #SVM with C = 1\n",
        "    clf = SVC(kernel='linear', C=1)\n",
        "    clf.fit(X_train, Y_train)\n",
        "    Y_pred = clf.predict(X_valid)\n",
        "    # f1 score\n",
        "    f1 = f1_score(Y_valid, Y_pred, average='micro')\n",
        "    optimal_f1 = f1\n",
        "    optimal_C = 1\n",
        "    optimal_degree = 2\n",
        "    optimal_gamma = 0.1\n",
        "    optimal_classifier = clf\n",
        "    optimal_accuracy = clf.score(X_valid, Y_valid)\n",
        "\n",
        "    # for testing\n",
        "    # c_range = [1]\n",
        "\n",
        "    c_range = np.logspace(-2, 10, 13)\n",
        "    # c_range = [0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]\n",
        "    #\n",
        "    # gamma_range = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
        "    # c_range = [0.001, 1, 100]\n",
        "    #\n",
        "    # gamma_range = [ 0.01, 10, 1000]\n",
        "\n",
        "    for C in c_range:\n",
        "        clf = SVC(kernel='linear', C=C)\n",
        "        clf.fit(X_train, Y_train)\n",
        "        Y_pred = clf.predict(X_valid)\n",
        "        # f1 score\n",
        "        f1 = f1_score(Y_valid, Y_pred, average='micro')\n",
        "        # accuracy\n",
        "\n",
        "        accuracy = clf.score(X_valid, Y_valid)\n",
        "\n",
        "        # precision\n",
        "        precision = precision_score(Y_valid, Y_pred, average='micro')\n",
        "        # recall\n",
        "        recall = recall_score(Y_valid, Y_pred, average='micro')\n",
        "        # write results to csv file\n",
        "        with open('audio_data/results_linear.csv', 'a') as f:\n",
        "            f.write('{}, {}, {}, {}, {}\\n'.format(C, accuracy, precision, recall, f1))\n",
        "\n",
        "        if f1 > optimal_f1:\n",
        "            optimal_f1 = f1\n",
        "            optimal_C = C\n",
        "            optimal_classifier = clf\n",
        "            optimal_accuracy = accuracy\n",
        "\n",
        "    optimal_hyperparameters = {'C': optimal_C}\n",
        "    optimal_metrics = {'accuracy': optimal_accuracy, 'f1': optimal_f1}\n",
        "    return optimal_classifier, optimal_hyperparameters, optimal_metrics\n",
        "\n",
        "\n",
        "\n",
        "#this function plots colormap where x and y coordinates of colourmap are the hyperparameters and colourmap is the accuracy of the classifier\n",
        "def plot_hyperparameter_accuracy(csv_file, midpoint, title, name , hyperparameters_to_plot= ['C', ' gamma']):\n",
        "    print()\n",
        "    hyperparameter_1 = hyperparameters_to_plot[0]\n",
        "    hyperparameter_2 = hyperparameters_to_plot[1]\n",
        "\n",
        "    # Draw heatmap of the validation accuracy as a function of gamma and C\n",
        "    #   read csv file\n",
        "\n",
        "    #plotting the validation accuracy vs hyperparameters colormap of linear kernel\n",
        "    #   read csv file and store value of accuracy in a 2d array\n",
        "    # one axis of array is gamma and other is C\n",
        "    #reading csv file\n",
        "    accuracy_list = []\n",
        "    c_list = []\n",
        "    gamma_list = []\n",
        "    with open(csv_file, 'r') as csvfile:\n",
        "        reader = csv.DictReader(csvfile)\n",
        "        line_count = 0\n",
        "        # the columns that we want to read are 'C', 'gamma', 'accuracy'\n",
        "        # the first row is the header\n",
        "        # the rest of the rows are the data\n",
        "        for row in reader:\n",
        "            if line_count == 0:\n",
        "                # this is the header\n",
        "                # print(f'Column names are {\", \".join(row)}')\n",
        "                line_count += 1\n",
        "            #save the accuracy values for all rows in a list\n",
        "            accuracy_list.append(row[' accuracy'])\n",
        "            c_list.append(row[hyperparameter_1])\n",
        "            gamma_list.append(row[hyperparameter_2])\n",
        "            line_count += 1\n",
        "    #print the list c_list ands gamma_list\n",
        "    print(c_list)\n",
        "    print(gamma_list)\n",
        "\n",
        "\n",
        "    #convert the list to numpy array\n",
        "    #convert list of strings to list of floats\n",
        "    accuracy_list = [float(i) for i in accuracy_list]\n",
        "    c_list = [float(i) for i in c_list]\n",
        "    gamma_list = [float(i) for i in gamma_list]\n",
        "    #convert list of floats to numpy array\n",
        "    accuracy_array = np.array(accuracy_list)\n",
        "    c_array = np.array(c_list)\n",
        "    gamma_array = np.array(gamma_list)\n",
        "    #reshape the array to 2d array\n",
        "\n",
        "    #define c_len to be no. of unique values of c_array\n",
        "    c_len = len(np.unique(c_array))\n",
        "    #define gamma_len to be no. of unique values of gamma_array\n",
        "    gamma_len = len(np.unique(gamma_array))\n",
        "\n",
        "    #c_array to have unique values of c_array\n",
        "    c_array = np.unique(c_array)\n",
        "    #gamma_array to have unique values of gamma_array\n",
        "    gamma_array = np.unique(gamma_array)\n",
        "    #c_array to be sorted in ascending order\n",
        "    c_array = np.sort(c_array)\n",
        "    #gamma_array to be sorted in ascending order\n",
        "    gamma_array = np.sort(gamma_array)\n",
        "\n",
        "    reshaped_array = np.reshape(accuracy_array, (len(c_array), len(gamma_array)))\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.subplots_adjust(left=0.2, right=0.95, bottom=0.15, top=0.95)\n",
        "    plt.imshow(\n",
        "        reshaped_array,\n",
        "        interpolation=\"nearest\",\n",
        "        cmap=plt.cm.hot,\n",
        "        norm=MidpointNormalize(vmin=0.2, midpoint=midpoint, vmax=1)\n",
        "    )\n",
        "    plt.xlabel(hyperparameter_2)\n",
        "    plt.ylabel(hyperparameter_1)\n",
        "    plt.colorbar()\n",
        "    plt.xticks(np.arange(len(gamma_array)), gamma_array, rotation=45)\n",
        "    plt.yticks(np.arange(len(c_array)), c_array)\n",
        "    plt.title(\"Validation accuracy \" + title + \" \"+ name)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def extract_accuracy(csv_file):\n",
        "    accuracy_list = []\n",
        "    c_list = []\n",
        "\n",
        "    with open(csv_file, 'r') as csvfile:\n",
        "        reader = csv.DictReader(csvfile)\n",
        "        line_count = 0\n",
        "        # the columns that we want to read are 'C', 'gamma', 'accuracy'\n",
        "        # the first row is the header\n",
        "        # the rest of the rows are the data\n",
        "        for row in reader:\n",
        "            if line_count == 0:\n",
        "                # this is the header\n",
        "                # print(f'Column names are {\", \".join(row)}')\n",
        "                line_count += 1\n",
        "            # save the accuracy values for all rows in a list\n",
        "            accuracy_list.append(row[' accuracy'])\n",
        "            c_list.append(row['C'])\n",
        "\n",
        "            line_count += 1\n",
        "\n",
        "    # convert list of strings to list of floats\n",
        "    accuracy_list = [float(i) for i in accuracy_list]\n",
        "    c_list = [float(i) for i in c_list]\n",
        "\n",
        "    # convert list of floats to numpy array\n",
        "    accuracy_array = np.array(accuracy_list)\n",
        "    c_array = np.array(c_list)\n",
        "\n",
        "\n",
        "\n",
        "    # c_array to have unique values of c_array\n",
        "    c_array = np.unique(c_array)\n",
        "\n",
        "    # c_array to be sorted in ascending order\n",
        "    c_array = np.sort(c_array)\n",
        "\n",
        "    return accuracy_array, c_array\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#plotting the results\n",
        "def plot_results():\n",
        "    name = \"TIMIT\"\n",
        "\n",
        "    # plotting the colormap of sigmoide kernel\n",
        "    plot_hyperparameter_accuracy('audio_data/results_sigmoid.csv' , 0.5 , 'Sigmoid Kernel', name)\n",
        "\n",
        "    #plotting the colormap of gaussian kernel\n",
        "    plot_hyperparameter_accuracy('audio_data/results_gaussian.csv' , 0.5 , 'Gaussian Kernel', name)\n",
        "\n",
        "    #plotting the colormap of polynomial kernel\n",
        "    plot_hyperparameter_accuracy('audio_data/results_poly_best.csv' , 0.5 , 'Polynomial Kernel', name , ['C', ' degree'])\n",
        "\n",
        "    #plotting the graph of linear kernel\n",
        "    accuracy_array, c_array = extract_accuracy('audio_data/results_linear.csv')\n",
        "    #plot the accuracy vs c graph\n",
        "    plt.plot(c_array, accuracy_array)\n",
        "    plt.xlabel('C')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Accuracy vs C for Linear Kernel ' + name)\n",
        "    #show\n",
        "    plt.show()\n",
        "\n",
        "    # #plotting the graph of polynomial kernel\n",
        "    #\n",
        "    # accuracy_array, c_array = extract_accuracy('audio_data/results_poly.csv')\n",
        "    # # plot the accuracy vs c graph\n",
        "    # plt.plot(c_array, accuracy_array)\n",
        "    # plt.xlabel('degree')\n",
        "    # plt.ylabel('Accuracy')\n",
        "    # plt.title('Accuracy vs degree for Linear Kernel ' + name)\n",
        "    # # show\n",
        "    # plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#plotting AUC curve of the best classifier\n",
        "def plot_AUC(classifier, X_test, Y_test):\n",
        "    name = \"TIMIT\"\n",
        "    #get the predicted labels\n",
        "    Y_pred = classifier.predict(X_test)\n",
        "    #get the predicted probabilities\n",
        "    Y_prob = classifier.predict_proba(X_test)\n",
        "    #get the AUC\n",
        "    fpr, tpr, thresholds = roc_curve(Y_test, Y_prob[:,1])\n",
        "    #plot the AUC\n",
        "    plt.plot(fpr, tpr)\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('ROC Curve for ' + name)\n",
        "    #show\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def SVM():\n",
        "\n",
        "    #getting the data\n",
        "    X_train, Y_train, X_test, Y_test, X_valid, Y_valid = data_preprocessing()\n",
        "    # SVM\n",
        "    # clf = SVC(kernel='linear', C=1)\n",
        "    # clf.fit(X_train, Y_train)\n",
        "    # Y_pred = clf.predict(X_test)\n",
        "    # # print(classification_report(Y_test, Y_pred))\n",
        "    # print(y_pred)\n",
        "    # print(test1_labels)\n",
        "    # print(y_pred == test1_labels)\n",
        "    # print(np.sum(y_pred == test1_labels))\n",
        "    # print(np.sum(y_pred == test1_labels) / 624)\n",
        "    # print(np.sum(y_pred == test1_labels) / 624 * 100)\n",
        "\n",
        "    # print(\"Accuracy of SVM classifier on test set: {:.2f}%\".format(np.sum(Y_pred == Y_test) / 624 * 100))\n",
        "\n",
        "    # print(clf.score(X_test, test1_labels))\n",
        "    # print(clf.score(X_train, train1_labels))\n",
        "\n",
        "    #\n",
        "    #\n",
        "    #calling grid search poly\n",
        "    print(\"Grid Search for Polynomial Kernel\")\n",
        "    optimal_classifier, optimal_hyperparameters, optimal_metrics = grid_search_poly(X_train, Y_train, X_valid, Y_valid)\n",
        "    #printing the results\n",
        "    print(\"Optimal C: {}\".format(optimal_hyperparameters['C']))\n",
        "    print(\"Optimal degree: {}\".format(optimal_hyperparameters['degree']))\n",
        "    print(\"Optimal gamma: {}\".format(optimal_hyperparameters['gamma']))\n",
        "    print(\"Optimal f1: {}\".format(optimal_metrics['f1']))\n",
        "    print(\"Optimal accuracy: {}\".format(optimal_metrics['accuracy']))\n",
        "\n",
        "\n",
        "    # calling grid search sigmoid\n",
        "    print(\"Grid Search for Sigmoid Kernel\")\n",
        "    optimal_classifier, optimal_hyperparameters, optimal_metrics = grid_search_sigmoid(X_train, Y_train, X_valid, Y_valid)\n",
        "    #printing the results\n",
        "    print(\"Optimal C: {}\".format(optimal_hyperparameters['C']))\n",
        "    print(\"Optimal gamma: {}\".format(optimal_hyperparameters['gamma']))\n",
        "    print(\"Optimal f1: {}\".format(optimal_metrics['f1']))\n",
        "    print(\"Optimal accuracy: {}\".format(optimal_metrics['accuracy']))\n",
        "\n",
        "    # calling grid search gaussian\n",
        "    print(\"Grid Search for Gaussian Kernel\")\n",
        "    optimal_classifier, optimal_hyperparameters, optimal_metrics = grid_search_gaussian(X_train, Y_train, X_valid, Y_valid)\n",
        "    #printing the results\n",
        "    print(\"Optimal C: {}\".format(optimal_hyperparameters['C']))\n",
        "    print(\"Optimal gamma: {}\".format(optimal_hyperparameters['gamma']))\n",
        "    print(\"Optimal f1: {}\".format(optimal_metrics['f1']))\n",
        "    print(\"Optimal accuracy: {}\".format(optimal_metrics['accuracy']))\n",
        "\n",
        "    # # calling grid search linear\n",
        "    print(\"Grid Search for Linear Kernel\")\n",
        "    optimal_classifier, optimal_hyperparameters, optimal_metrics = grid_search_linear(X_train, Y_train, X_valid, Y_valid)\n",
        "    #printing the results\n",
        "    print(\"Optimal C: {}\".format(optimal_hyperparameters['C']))\n",
        "    print(\"Optimal f1: {}\".format(optimal_metrics['f1']))\n",
        "    print(\"Optimal accuracy: {}\".format(optimal_metrics['accuracy']))\n",
        "\n",
        "\n",
        "    # plotting\n",
        "    # plotting the colormap of polynomial kernel\n",
        "    # plot_hyperparameter_accuracy('audio_data/results_poly.csv', 0.9, 'Polynomial Kernel', \"PneumoniaMNIST\", ['C', ' degree'])\n",
        "\n",
        "    # # plotting the graph of linear kernel\n",
        "    # accuracy_array, c_array = extract_accuracy('audio_data/results_linear.csv')\n",
        "    #\n",
        "    # #print accuracy_array and c_array\n",
        "    # print(accuracy_array)\n",
        "    # print(c_array)\n",
        "    #\n",
        "    # # plot the accuracy vs c graph\n",
        "    # plt.plot(c_array, accuracy_array)\n",
        "    # plt.xlabel('C')\n",
        "    # plt.ylabel('Accuracy')\n",
        "    # plt.title('Accuracy vs C for Linear Kernel ' + \"PneumoniaMNIST\")\n",
        "    # # show\n",
        "    # plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#main function\n",
        "def main():\n",
        "\n",
        "    #if directory audio_data does not exist, create it\n",
        "    if not os.path.exists('audio_data'):\n",
        "        os.makedirs('audio_data')\n",
        "\n",
        "\n",
        "    path = 'drive/MyDrive/Colab Notebooks/PRNN_A1_DATA'\n",
        "    # path = os.getcwd()\n",
        "\n",
        "    # data_preprocessing()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    SVM()\n",
        "    plot_results()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# if main\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VMPEhftwl3Bx"
      },
      "outputs": [],
      "source": [
        "# we are implementing a simple SVM classifier for the pneumonia MNIST dataset\n",
        "# the dataset is at the folder:  object_localization.npy in the same directory as this file\n",
        "\n",
        "#result folder is: binary_svr\n",
        "\n",
        "\n",
        "\n",
        "#importing libraries\n",
        "import csv\n",
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score, precision_score, recall_score, roc_curve\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "#importing svm\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import Normalize\n",
        "\n",
        "import os\n",
        "import random\n",
        "import math\n",
        "from datetime import datetime\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Rectangle\n",
        "from sklearn.model_selection import train_test_split\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "#Import libraries for processing\n",
        "import numpy as np\n",
        "from skimage import color\n",
        "from skimage.transform import rescale\n",
        "\n",
        "#global vars\n",
        "\n",
        "# path = 'drive/MyDrive/Colab Notebooks/PRNN_A1_DATA'\n",
        "path = os.getcwd()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class MidpointNormalize(Normalize):\n",
        "    def __init__(self, vmin=None, vmax=None, midpoint=None, clip=False):\n",
        "        self.midpoint = midpoint\n",
        "        Normalize.__init__(self, vmin, vmax, clip)\n",
        "\n",
        "    def __call__(self, value, clip=None):\n",
        "        x, y = [self.vmin, self.midpoint, self.vmax], [0, 0.5, 1]\n",
        "        return np.ma.masked_array(np.interp(value, x, y))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Compute Mean Square Error\n",
        "def compute_mse(test_data, test_label,test_output) :\n",
        "    mse = [0,0,0,0]\n",
        "    for i in range(len(test_data)):\n",
        "      for j in range(4):\n",
        "        mse[j]+= (test_output[i][j] - test_label[i][j])*(test_output[i][j] - test_label[i][j])\n",
        "\n",
        "    for i in range(len(mse)):\n",
        "      mse[i] = mse[i]/len(test_data)\n",
        "\n",
        "    print(\"Mean squared error: \",sum(mse)/len(mse))\n",
        "\n",
        "\n",
        "\n",
        "#Compute Mean Absolute Error\n",
        "def compute_mae(test_data, test_label,test_output) :\n",
        "    mae = [0,0,0,0]\n",
        "    for i in range(len(test_data)):\n",
        "      for j in range(4):\n",
        "        mae[j]+= abs(test_output[i][j] - test_label[i][j])\n",
        "\n",
        "    for i in range(len(mae)):\n",
        "      mae[i] = mae[i]/len(test_data)\n",
        "\n",
        "    print(\"Mean absolute error: \",sum(mae)/len(mae))\n",
        "\n",
        "\n",
        "#Compute Mean Intersection over Union Value\n",
        "def compute_mIoU(test_data, test_label,test_output) :\n",
        "    miou = 0\n",
        "    for i in range(len(test_data)):\n",
        "      x1 = max(test_output[i][0],test_label[i][0])\n",
        "      x2 = min(test_output[i][2],test_label[i][2])\n",
        "      y1 = max(test_output[i][1],test_label[i][1])\n",
        "      y2 = min(test_output[i][3],test_label[i][3])\n",
        "      int_area = (x2 - x1)*(y2 - y1)\n",
        "      test_area = (test_output[i][2] - test_output[i][0])*(test_output[i][3] - test_output[i][1])\n",
        "      op_area = (test_label[i][2] - test_label[i][0])*(test_label[i][3] - test_label[i][1])\n",
        "      iou = int_area / (test_area + op_area - int_area)\n",
        "      miou += iou\n",
        "\n",
        "    print(\"Mean Intersection over Union error: \",miou/len(test_data))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# preprocessing the data\n",
        "def data_preprocessing():\n",
        "    data_path = path + '/object_localization.npy'\n",
        "\n",
        "    data = np.load(data_path,allow_pickle=True,encoding='latin1')\n",
        "\n",
        "    num_images = data.shape[0]\n",
        "    # 80% of the data to be training data\n",
        "    num_train_data = int(num_images * 0.8)\n",
        "\n",
        "    # Convert the image to grayscale and downscale it\n",
        "    r_data = []\n",
        "    labels = []\n",
        "    for i in range(num_images):\n",
        "        g_img = color.rgb2gray(data[i][0])\n",
        "        # print(g_img.shape)\n",
        "        ds_img = rescale(g_img, 0.37)\n",
        "        ds_img = ds_img.flatten()\n",
        "        ds_img = [*ds_img, 1]\n",
        "        # print(ds_img.shape)\n",
        "        r_data.append(ds_img)\n",
        "        labels.append(data[i][2] * 0.37)\n",
        "\n",
        "    # Split the training and test data\n",
        "    train_data = np.array(r_data[:num_train_data])\n",
        "    test_data = np.array(r_data[num_train_data:])\n",
        "    train_label = np.array(labels[:num_train_data])\n",
        "    test_label = labels[num_train_data:]\n",
        "\n",
        "    #split train data into train and validation data\n",
        "    train_data, val_data, train_label, val_label = train_test_split(train_data, train_label, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "    #converting everything to numpy array\n",
        "    train_data = np.array(train_data)\n",
        "    train_label = np.array(train_label)\n",
        "    val_data = np.array(val_data)\n",
        "    val_label = np.array(val_label)\n",
        "    test_data = np.array(test_data)\n",
        "    test_label = np.array(test_label)\n",
        "\n",
        "    # print(data.files)\n",
        "    # Reshape images and save in numpy arrays\n",
        "    train1_data = train_data\n",
        "    train1_labels = train_label\n",
        "    # change data type to float of train1_data\n",
        "    train1_data = train1_data.astype(float)\n",
        "    # change datatype of train1_labels to int\n",
        "\n",
        "    train1_labels = train1_labels.astype(int)\n",
        "\n",
        "    # validation data\n",
        "    valid1_data = val_data\n",
        "    valid1_labels = val_label\n",
        "    # change datatype of valid1_data to float\n",
        "    valid1_data = valid1_data.astype(float)\n",
        "    # change datatype of valid1_labels to int\n",
        "    valid1_labels = valid1_labels.astype(int)\n",
        "\n",
        "    test1_data = test_data\n",
        "    test1_labels = test_label\n",
        "    # change data type to float of test1_data\n",
        "    test1_data = test1_data.astype(float)\n",
        "    # change datatype of test1_labels to int\n",
        "    test1_labels = test1_labels.astype(int)\n",
        "\n",
        "    # feature scaling\n",
        "    sc = StandardScaler()\n",
        "    X_train = sc.fit_transform(train1_data, train1_labels)\n",
        "    X_test = sc.fit_transform(test1_data, test1_labels)\n",
        "    X_valid = sc.fit_transform(valid1_data, valid1_labels)\n",
        "\n",
        "\n",
        "    #print the shape of Y_train\n",
        "\n",
        "    return X_train, Y_train, X_test, Y_test, X_valid, Y_valid\n",
        "\n",
        "\n",
        "\n",
        "    # # save np.load\n",
        "    # np_load_old = np.load\n",
        "    #\n",
        "    # # modify the default parameters of np.load\n",
        "    # np.load = lambda *a, **k: np_load_old(*a, allow_pickle=True, **k)\n",
        "    #\n",
        "    # # call load_data with allow_pickle implicitly set to true\n",
        "    # # (train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)\n",
        "    # data = np.load(data_path )\n",
        "    #\n",
        "    # # restore np.load for future normal usage\n",
        "    # np.load = np_load_old\n",
        "    #\n",
        "    # # data = pickle.load(data_path, encoding='iso-8859-1')\n",
        "    # # print(data.files)\n",
        "    # # Reshape images and save in numpy arrays\n",
        "    # # train1_data = np.reshape(data['train_images'], (4708, 784))\n",
        "    # # train1_labels = np.reshape(data['train_labels'], (4708))\n",
        "    #\n",
        "    # #print\n",
        "    # print(data.files)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def grid_search_poly(X_train, Y_train,  X_valid, Y_valid ):\n",
        "\n",
        "    #create csv file for the results name: results_poly.csv\n",
        "    with open('binary_svr/results_poly.csv', 'w') as f:\n",
        "        f.write('C, degree, gamma, accuracy, precision, recall, f1\\n')\n",
        "\n",
        "\n",
        "    #SVM with C = 1 and degree = 2 and gamma = 0.1\n",
        "    clf = SVC(kernel='poly', C=1, degree=2)\n",
        "\n",
        "    clf.fit(X_train, Y_train)\n",
        "    Y_pred = clf.predict(X_valid)\n",
        "    #f1 score\n",
        "    f1 = f1_score(Y_valid, Y_pred, average='micro')\n",
        "    optimal_f1 = f1\n",
        "    optimal_C = 1\n",
        "    optimal_degree = 2\n",
        "\n",
        "    optimal_classifier = clf\n",
        "    optimal_accuracy = clf.score(X_valid, Y_valid)\n",
        "\n",
        "\n",
        "\n",
        "    #for testing\n",
        "    c_range = [0.1, 1, 10]\n",
        "    degree_range = [ 2 , 3]\n",
        "    gamma_range = [0.1 ]\n",
        "\n",
        "    # c_range = np.logspace(-2, 10, 13)\n",
        "    # degree_range = [ 2, 3, 4, 5, 6 , 7, 8, 9, 10, 100, 1000, 10000]\n",
        "    # gamma_range = np.logspace(-9, 3, 13)\n",
        "\n",
        "    # c_range = [0.001,  1, 100]\n",
        "    # degree_range = [ 2, 10, 100]\n",
        "    # gamma_range = [ 0.01, 10, 1000]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    for C in c_range:\n",
        "        for degree in degree_range:\n",
        "            for gamma in gamma_range:\n",
        "                clf = SVC(kernel='poly', C=C, degree=degree )\n",
        "                clf.fit(X_train, Y_train)\n",
        "                Y_pred = clf.predict(X_valid)\n",
        "                # f1 score\n",
        "                f1 = f1_score(Y_valid, Y_pred, average='micro')\n",
        "                # accuracy\n",
        "                accuracy = clf.score(X_valid, Y_valid)\n",
        "                # precision\n",
        "                precision = precision_score(Y_valid, Y_pred, average='micro')\n",
        "                # recall\n",
        "                recall = recall_score(Y_valid, Y_pred, average='micro')\n",
        "                # write results to csv file\n",
        "                with open('binary_svr/results_poly.csv', 'a') as f:\n",
        "                    f.write('{}, {}, {}, {}, {}, {}, {}\\n'.format(C, degree, gamma, accuracy, precision, recall, f1))\n",
        "\n",
        "                if f1 > optimal_f1:\n",
        "                    optimal_f1 = f1\n",
        "                    optimal_C = C\n",
        "                    optimal_degree = degree\n",
        "\n",
        "                    optimal_classifier = clf\n",
        "                    optimal_accuracy = accuracy\n",
        "\n",
        "\n",
        "\n",
        "    optimal_hyperparameters = {'C': optimal_C, 'degree': optimal_degree}\n",
        "    optimal_metrics = {'accuracy': optimal_accuracy, 'f1': optimal_f1}\n",
        "    return optimal_classifier, optimal_hyperparameters, optimal_metrics\n",
        "\n",
        "\n",
        "\n",
        "def grid_search_sigmoid(X_train, Y_train,  X_valid, Y_valid):\n",
        "    #create csv file for the results name: results_sigmoid.csv\n",
        "    with open('binary_svr/results_sigmoid.csv', 'w') as f:\n",
        "        f.write('C, gamma, accuracy, precision, recall, f1\\n')\n",
        "\n",
        "    #SVM with C = 1 and gamma = 0.1\n",
        "    clf = SVC(kernel='sigmoid', C=1, gamma=0.1)\n",
        "\n",
        "    clf.fit(X_train, Y_train)\n",
        "    Y_pred = clf.predict(X_valid)\n",
        "    # f1 score\n",
        "    f1 = f1_score(Y_valid, Y_pred, average='micro')\n",
        "    optimal_f1 = f1\n",
        "    optimal_C = 1\n",
        "\n",
        "    optimal_gamma = 0.1\n",
        "    optimal_classifier = clf\n",
        "    optimal_accuracy = clf.score(X_valid, Y_valid)\n",
        "\n",
        "    #for testing\n",
        "    c_range = [1]\n",
        "    gamma_range = [0.1]\n",
        "\n",
        "    # c_range = np.logspace(-2, 10, 13)\n",
        "    # gamma_range = np.logspace(-9, 3, 13)\n",
        "    # c_range = [0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]\n",
        "    #\n",
        "    # gamma_range = [ 0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
        "\n",
        "    # c_range = [0.001, 1, 100]\n",
        "    #\n",
        "    # gamma_range = [ 0.01, 10, 1000]\n",
        "\n",
        "    for C in c_range:\n",
        "        for gamma in gamma_range:\n",
        "            clf = SVC(kernel='sigmoid', C=C, gamma=gamma)\n",
        "            clf.fit(X_train, Y_train)\n",
        "            Y_pred = clf.predict(X_valid)\n",
        "            # f1 score\n",
        "            f1 = f1_score(Y_valid, Y_pred, average='micro')\n",
        "            # accuracy\n",
        "            accuracy = clf.score(X_valid, Y_valid)\n",
        "            # precision\n",
        "            precision = precision_score(Y_valid, Y_pred, average='micro')\n",
        "            # recall\n",
        "            recall = recall_score(Y_valid, Y_pred, average='micro')\n",
        "            # write results to csv file\n",
        "            with open('binary_svr/results_sigmoid.csv', 'a') as f:\n",
        "                f.write('{}, {}, {}, {}, {}, {}\\n'.format(C, gamma, accuracy, precision, recall, f1))\n",
        "\n",
        "\n",
        "            if f1 > optimal_f1:\n",
        "                optimal_f1 = f1\n",
        "                optimal_C = C\n",
        "                optimal_gamma = gamma\n",
        "                optimal_classifier = clf\n",
        "                optimal_accuracy = accuracy\n",
        "\n",
        "    optimal_hyperparameters = {'C': optimal_C, 'gamma': optimal_gamma}\n",
        "    optimal_metrics = {'accuracy': optimal_accuracy, 'f1': optimal_f1}\n",
        "    return optimal_classifier, optimal_hyperparameters, optimal_metrics\n",
        "\n",
        "def grid_search_gaussian(X_train, Y_train,  X_valid, Y_valid):\n",
        "    #create csv file for the results name: results_gaussian.csv\n",
        "    with open('binary_svr/results_gaussian.csv', 'w') as f:\n",
        "        f.write('C, gamma, accuracy, precision, recall, f1\\n')\n",
        "\n",
        "    #SVM with C = 1 and gamma = 0.1\n",
        "    clf = SVC(kernel='rbf', C=1, gamma=0.1)\n",
        "\n",
        "    clf.fit(X_train, Y_train)\n",
        "    Y_pred = clf.predict(X_valid)\n",
        "    # f1 score\n",
        "    f1 = f1_score(Y_valid, Y_pred, average='micro')\n",
        "    optimal_f1 = f1\n",
        "    optimal_C = 1\n",
        "\n",
        "    optimal_gamma = 0.1\n",
        "    optimal_classifier = clf\n",
        "    optimal_accuracy = clf.score(X_valid, Y_valid)\n",
        "\n",
        "    #for testing\n",
        "    # c_range = [1]\n",
        "    # gamma_range = [0.1]\n",
        "\n",
        "    # c_range = np.logspace(-2, 10, 13)\n",
        "    # gamma_range = np.logspace(-9, 3, 13)\n",
        "    # c_range = [0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]\n",
        "    #\n",
        "    # gamma_range = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
        "    c_range = [0.001, 1, 100]\n",
        "\n",
        "    gamma_range = [ 0.01, 10, 1000]\n",
        "\n",
        "    for C in c_range:\n",
        "        for gamma in gamma_range:\n",
        "            clf = SVC(kernel='rbf', C=C, gamma=gamma)\n",
        "            clf.fit(X_train, Y_train)\n",
        "            Y_pred = clf.predict(X_valid)\n",
        "            # f1 score\n",
        "            f1 = f1_score(Y_valid, Y_pred, average='micro')\n",
        "            # accuracy\n",
        "            accuracy = clf.score(X_valid, Y_valid)\n",
        "            # precision\n",
        "            precision = precision_score(Y_valid, Y_pred, average='micro')\n",
        "            # recall\n",
        "            recall = recall_score(Y_valid, Y_pred, average='micro')\n",
        "            # write results to csv file\n",
        "            with open('binary_svr/results_gaussian.csv', 'a') as f:\n",
        "                f.write('{}, {}, {}, {}, {}, {}\\n'.format(C, gamma, accuracy, precision, recall, f1))\n",
        "\n",
        "            if f1 > optimal_f1:\n",
        "                optimal_f1 = f1\n",
        "                optimal_C = C\n",
        "                optimal_gamma = gamma\n",
        "                optimal_classifier = clf\n",
        "                optimal_accuracy = accuracy\n",
        "\n",
        "    optimal_hyperparameters = {'C': optimal_C, 'gamma': optimal_gamma}\n",
        "    optimal_metrics = {'accuracy': optimal_accuracy, 'f1': optimal_f1}\n",
        "    return optimal_classifier, optimal_hyperparameters, optimal_metrics\n",
        "\n",
        "def grid_search_linear(X_train, Y_train,  X_valid, Y_valid):\n",
        "    #create csv file for the results name: results_linear.csv\n",
        "    with open('binary_svr/results_linear.csv', 'w') as f:\n",
        "        f.write('C, accuracy, precision, recall, f1\\n')\n",
        "\n",
        "    #SVM with C = 1\n",
        "    clf = SVC(kernel='linear', C=1)\n",
        "    clf.fit(X_train, Y_train)\n",
        "    Y_pred = clf.predict(X_valid)\n",
        "    # f1 score\n",
        "    f1 = f1_score(Y_valid, Y_pred, average='micro')\n",
        "    optimal_f1 = f1\n",
        "    optimal_C = 1\n",
        "    optimal_degree = 2\n",
        "    optimal_gamma = 0.1\n",
        "    optimal_classifier = clf\n",
        "    optimal_accuracy = clf.score(X_valid, Y_valid)\n",
        "\n",
        "    # for testing\n",
        "    # c_range = [1]\n",
        "\n",
        "    # c_range = np.logspace(-2, 10, 13)\n",
        "    # c_range = [0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]\n",
        "    #\n",
        "    # gamma_range = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
        "    c_range = [0.001, 1, 100]\n",
        "\n",
        "    gamma_range = [ 0.01, 10, 1000]\n",
        "    for C in c_range:\n",
        "        clf = SVC(kernel='linear', C=C)\n",
        "        clf.fit(X_train, Y_train)\n",
        "        Y_pred = clf.predict(X_valid)\n",
        "        # f1 score\n",
        "        f1 = f1_score(Y_valid, Y_pred, average='micro')\n",
        "        # accuracy\n",
        "\n",
        "        accuracy = clf.score(X_valid, Y_valid)\n",
        "\n",
        "        # precision\n",
        "        precision = precision_score(Y_valid, Y_pred, average='micro')\n",
        "        # recall\n",
        "        recall = recall_score(Y_valid, Y_pred, average='micro')\n",
        "        # write results to csv file\n",
        "        with open('binary_svr/results_linear.csv', 'a') as f:\n",
        "            f.write('{}, {}, {}, {}, {}\\n'.format(C, accuracy, precision, recall, f1))\n",
        "\n",
        "        if f1 > optimal_f1:\n",
        "            optimal_f1 = f1\n",
        "            optimal_C = C\n",
        "            optimal_classifier = clf\n",
        "            optimal_accuracy = accuracy\n",
        "\n",
        "    optimal_hyperparameters = {'C': optimal_C}\n",
        "    optimal_metrics = {'accuracy': optimal_accuracy, 'f1': optimal_f1}\n",
        "    return optimal_classifier, optimal_hyperparameters, optimal_metrics\n",
        "\n",
        "\n",
        "\n",
        "#this function plots colormap where x and y coordinates of colourmap are the hyperparameters and colourmap is the accuracy of the classifier\n",
        "def plot_hyperparameter_accuracy(csv_file, midpoint, title, name , hyperparameters_to_plot= ['C', ' gamma']):\n",
        "    print()\n",
        "    hyperparameter_1 = hyperparameters_to_plot[0]\n",
        "    hyperparameter_2 = hyperparameters_to_plot[1]\n",
        "\n",
        "    # Draw heatmap of the validation accuracy as a function of gamma and C\n",
        "    #   read csv file\n",
        "\n",
        "    #plotting the validation accuracy vs hyperparameters colormap of linear kernel\n",
        "    #   read csv file and store value of accuracy in a 2d array\n",
        "    # one axis of array is gamma and other is C\n",
        "    #reading csv file\n",
        "    accuracy_list = []\n",
        "    c_list = []\n",
        "    gamma_list = []\n",
        "    with open(csv_file, 'r') as csvfile:\n",
        "        reader = csv.DictReader(csvfile)\n",
        "        line_count = 0\n",
        "        # the columns that we want to read are 'C', 'gamma', 'accuracy'\n",
        "        # the first row is the header\n",
        "        # the rest of the rows are the data\n",
        "        for row in reader:\n",
        "            if line_count == 0:\n",
        "                # this is the header\n",
        "                # print(f'Column names are {\", \".join(row)}')\n",
        "                line_count += 1\n",
        "            #save the accuracy values for all rows in a list\n",
        "            accuracy_list.append(row[' accuracy'])\n",
        "            c_list.append(row[hyperparameter_1])\n",
        "            gamma_list.append(row[hyperparameter_2])\n",
        "            line_count += 1\n",
        "    #print the list c_list ands gamma_list\n",
        "    print(c_list)\n",
        "    print(gamma_list)\n",
        "\n",
        "\n",
        "    #convert the list to numpy array\n",
        "    #convert list of strings to list of floats\n",
        "    accuracy_list = [float(i) for i in accuracy_list]\n",
        "    c_list = [float(i) for i in c_list]\n",
        "    gamma_list = [float(i) for i in gamma_list]\n",
        "    #convert list of floats to numpy array\n",
        "    accuracy_array = np.array(accuracy_list)\n",
        "    c_array = np.array(c_list)\n",
        "    gamma_array = np.array(gamma_list)\n",
        "    #reshape the array to 2d array\n",
        "\n",
        "    #define c_len to be no. of unique values of c_array\n",
        "    c_len = len(np.unique(c_array))\n",
        "    #define gamma_len to be no. of unique values of gamma_array\n",
        "    gamma_len = len(np.unique(gamma_array))\n",
        "\n",
        "    #c_array to have unique values of c_array\n",
        "    c_array = np.unique(c_array)\n",
        "    #gamma_array to have unique values of gamma_array\n",
        "    gamma_array = np.unique(gamma_array)\n",
        "    #c_array to be sorted in ascending order\n",
        "    c_array = np.sort(c_array)\n",
        "    #gamma_array to be sorted in ascending order\n",
        "    gamma_array = np.sort(gamma_array)\n",
        "\n",
        "    reshaped_array = np.reshape(accuracy_array, (len(c_array), len(gamma_array)))\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.subplots_adjust(left=0.2, right=0.95, bottom=0.15, top=0.95)\n",
        "    plt.imshow(\n",
        "        reshaped_array,\n",
        "        interpolation=\"nearest\",\n",
        "        cmap=plt.cm.hot,\n",
        "        norm=MidpointNormalize(vmin=0.2, midpoint=midpoint, vmax=1)\n",
        "    )\n",
        "    plt.xlabel(hyperparameter_2)\n",
        "    plt.ylabel(hyperparameter_1)\n",
        "    plt.colorbar()\n",
        "    plt.xticks(np.arange(len(gamma_array)), gamma_array, rotation=45)\n",
        "    plt.yticks(np.arange(len(c_array)), c_array)\n",
        "    plt.title(\"Validation accuracy \" + title + \" \"+ name)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def extract_accuracy(csv_file):\n",
        "    accuracy_list = []\n",
        "    c_list = []\n",
        "\n",
        "    with open(csv_file, 'r') as csvfile:\n",
        "        reader = csv.DictReader(csvfile)\n",
        "        line_count = 0\n",
        "        # the columns that we want to read are 'C', 'gamma', 'accuracy'\n",
        "        # the first row is the header\n",
        "        # the rest of the rows are the data\n",
        "        for row in reader:\n",
        "            if line_count == 0:\n",
        "                # this is the header\n",
        "                # print(f'Column names are {\", \".join(row)}')\n",
        "                line_count += 1\n",
        "            # save the accuracy values for all rows in a list\n",
        "            accuracy_list.append(row[' accuracy'])\n",
        "            c_list.append(row['C'])\n",
        "\n",
        "            line_count += 1\n",
        "\n",
        "    # convert list of strings to list of floats\n",
        "    accuracy_list = [float(i) for i in accuracy_list]\n",
        "    c_list = [float(i) for i in c_list]\n",
        "\n",
        "    # convert list of floats to numpy array\n",
        "    accuracy_array = np.array(accuracy_list)\n",
        "    c_array = np.array(c_list)\n",
        "\n",
        "\n",
        "\n",
        "    # c_array to have unique values of c_array\n",
        "    c_array = np.unique(c_array)\n",
        "\n",
        "    # c_array to be sorted in ascending order\n",
        "    c_array = np.sort(c_array)\n",
        "\n",
        "    return accuracy_array, c_array\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#plotting the results\n",
        "def plot_results():\n",
        "    name = \"PneumoniaMNIST\"\n",
        "\n",
        "    # plotting the colormap of sigmoide kernel\n",
        "    plot_hyperparameter_accuracy('binary_svr/results_sigmoid.csv' , 0.72 , 'Sigmoid Kernel', name)\n",
        "\n",
        "    #plotting the colormap of gaussian kernel\n",
        "    plot_hyperparameter_accuracy('binary_svr/results_gaussian.csv' , 0.74 , 'Gaussian Kernel', name)\n",
        "\n",
        "    #plotting the colormap of polynomial kernel\n",
        "    plot_hyperparameter_accuracy('binary_svr/results_poly_best.csv' , 0.7 , 'Polynomial Kernel', name , ['C', ' degree'])\n",
        "\n",
        "    #plotting the graph of linear kernel\n",
        "    accuracy_array, c_array = extract_accuracy('binary_svr/results_linear.csv')\n",
        "    #plot the accuracy vs c graph\n",
        "    plt.plot(c_array, accuracy_array)\n",
        "    plt.xlabel('C')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Accuracy vs C for Linear Kernel ' + name)\n",
        "    #show\n",
        "    plt.show()\n",
        "\n",
        "    # #plotting the graph of polynomial kernel\n",
        "    #\n",
        "    # accuracy_array, c_array = extract_accuracy('binary_svr/results_poly.csv')\n",
        "    # # plot the accuracy vs c graph\n",
        "    # plt.plot(c_array, accuracy_array)\n",
        "    # plt.xlabel('degree')\n",
        "    # plt.ylabel('Accuracy')\n",
        "    # plt.title('Accuracy vs degree for Linear Kernel ' + name)\n",
        "    # # show\n",
        "    # plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#plotting AUC curve of the best classifier\n",
        "def plot_AUC(classifier, X_test, Y_test):\n",
        "    name = \"PneumoniaMNIST\"\n",
        "    #get the predicted labels\n",
        "    Y_pred = classifier.predict(X_test)\n",
        "    #get the predicted probabilities\n",
        "    Y_prob = classifier.predict_proba(X_test)\n",
        "    #get the AUC\n",
        "    fpr, tpr, thresholds = roc_curve(Y_test, Y_prob[:,1])\n",
        "    #plot the AUC\n",
        "    plt.plot(fpr, tpr)\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('ROC Curve for ' + name)\n",
        "    #show\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def SVM(path):\n",
        "\n",
        "    #getting the data\n",
        "    X_train, Y_train, X_test, Y_test, X_valid, Y_valid = data_preprocessing(path)\n",
        "    # SVM\n",
        "    # clf = SVC(kernel='linear', C=1)\n",
        "    # clf.fit(X_train, Y_train)\n",
        "    # Y_pred = clf.predict(X_test)\n",
        "    # # print(classification_report(Y_test, Y_pred))\n",
        "    # print(y_pred)\n",
        "    # print(test1_labels)\n",
        "    # print(y_pred == test1_labels)\n",
        "    # print(np.sum(y_pred == test1_labels))\n",
        "    # print(np.sum(y_pred == test1_labels) / 624)\n",
        "    # print(np.sum(y_pred == test1_labels) / 624 * 100)\n",
        "\n",
        "    # print(\"Accuracy of SVM classifier on test set: {:.2f}%\".format(np.sum(Y_pred == Y_test) / 624 * 100))\n",
        "\n",
        "    # print(clf.score(X_test, test1_labels))\n",
        "    # print(clf.score(X_train, train1_labels))\n",
        "\n",
        "\n",
        "\n",
        "    # #calling grid search poly\n",
        "    # print(\"Grid Search for Polynomial Kernel\")\n",
        "    # optimal_classifier, optimal_hyperparameters, optimal_metrics = grid_search_poly(X_train, Y_train, X_valid, Y_valid)\n",
        "    # #printing the results\n",
        "    # print(\"Optimal C: {}\".format(optimal_hyperparameters['C']))\n",
        "    # print(\"Optimal degree: {}\".format(optimal_hyperparameters['degree']))\n",
        "    # print(\"Optimal gamma: {}\".format(optimal_hyperparameters['gamma']))\n",
        "    # print(\"Optimal f1: {}\".format(optimal_metrics['f1']))\n",
        "    # print(\"Optimal accuracy: {}\".format(optimal_metrics['accuracy']))\n",
        "\n",
        "\n",
        "    # calling grid search sigmoid\n",
        "    print(\"Grid Search for Sigmoid Kernel\")\n",
        "    optimal_classifier, optimal_hyperparameters, optimal_metrics = grid_search_sigmoid(X_train, Y_train, X_valid, Y_valid)\n",
        "    #printing the results\n",
        "    print(\"Optimal C: {}\".format(optimal_hyperparameters['C']))\n",
        "    print(\"Optimal gamma: {}\".format(optimal_hyperparameters['gamma']))\n",
        "    print(\"Optimal f1: {}\".format(optimal_metrics['f1']))\n",
        "    print(\"Optimal accuracy: {}\".format(optimal_metrics['accuracy']))\n",
        "\n",
        "    # # calling grid search gaussian\n",
        "    # print(\"Grid Search for Gaussian Kernel\")\n",
        "    # optimal_classifier, optimal_hyperparameters, optimal_metrics = grid_search_gaussian(X_train, Y_train, X_valid, Y_valid)\n",
        "    # #printing the results\n",
        "    # print(\"Optimal C: {}\".format(optimal_hyperparameters['C']))\n",
        "    # print(\"Optimal gamma: {}\".format(optimal_hyperparameters['gamma']))\n",
        "    # print(\"Optimal f1: {}\".format(optimal_metrics['f1']))\n",
        "    # print(\"Optimal accuracy: {}\".format(optimal_metrics['accuracy']))\n",
        "    # \n",
        "    # # # calling grid search linear\n",
        "    # print(\"Grid Search for Linear Kernel\")\n",
        "    # optimal_classifier, optimal_hyperparameters, optimal_metrics = grid_search_linear(X_train, Y_train, X_valid, Y_valid)\n",
        "    # #printing the results\n",
        "    # print(\"Optimal C: {}\".format(optimal_hyperparameters['C']))\n",
        "    # print(\"Optimal f1: {}\".format(optimal_metrics['f1']))\n",
        "    # print(\"Optimal accuracy: {}\".format(optimal_metrics['accuracy']))\n",
        "\n",
        "\n",
        "    # plotting\n",
        "    # plotting the colormap of polynomial kernel\n",
        "    # plot_hyperparameter_accuracy('binary_svr/results_poly.csv', 0.9, 'Polynomial Kernel', \"PneumoniaMNIST\", ['C', ' degree'])\n",
        "\n",
        "    # # plotting the graph of linear kernel\n",
        "    # accuracy_array, c_array = extract_accuracy('binary_svr/results_linear.csv')\n",
        "    #\n",
        "    # #print accuracy_array and c_array\n",
        "    # print(accuracy_array)\n",
        "    # print(c_array)\n",
        "    #\n",
        "    # # plot the accuracy vs c graph\n",
        "    # plt.plot(c_array, accuracy_array)\n",
        "    # plt.xlabel('C')\n",
        "    # plt.ylabel('Accuracy')\n",
        "    # plt.title('Accuracy vs C for Linear Kernel ' + \"PneumoniaMNIST\")\n",
        "    # # show\n",
        "    # plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#main function\n",
        "import os\n",
        "\n",
        "\n",
        "def main():\n",
        "\n",
        "    #if directory binary_svr does not exist, create it\n",
        "    # if not os.path.exists('binary_svr'):\n",
        "    #     os.makedirs('binary_svr')\n",
        "\n",
        "\n",
        "\n",
        "    data_preprocessing()\n",
        "\n",
        "\n",
        "    # SVM()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# if main\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xEscv7E0l79j"
      },
      "outputs": [],
      "source": [
        "#MLP \n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\"\"\"For Dataset-2 we use output activation = Softmax, Loss = Categorical Cross-entropy\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "def process_data(path):\n",
        "\n",
        "    dataset = np.load(path)\n",
        "    train_x, train_y = dataset['train_images']/255.0, dataset['train_labels']\n",
        "    val_x, val_y = dataset['val_images']/255.0, dataset['val_labels']\n",
        "    test_x, test_y = dataset['test_images']/255.0, dataset['test_labels']\n",
        "    temp = test_x\n",
        "\n",
        "    train_x = train_x.reshape((train_x.shape[0], train_x.shape[1]*train_x.shape[2]*train_x.shape[3]))\n",
        "    val_x = val_x.reshape((val_x.shape[0], val_x.shape[1]*val_x.shape[2]*val_x.shape[3]))\n",
        "    test_x = test_x.reshape((test_x.shape[0], test_x.shape[1]*test_x.shape[2]*test_x.shape[3]))\n",
        "\n",
        "    return  train_x, tf.reshape(tf.one_hot(train_y, depth=8), [train_x.shape[0], 8]), \\\n",
        "    test_x, tf.reshape(tf.one_hot(test_y, depth=8), [test_y.shape[0], 8]), \\\n",
        "    val_x, tf.reshape(tf.one_hot(val_y, depth=8), [val_y.shape[0], 8]), temp\n",
        "\n",
        "def MLP_train(x, y, val_x, val_y):\n",
        "    model = tf.keras.Sequential([tf.keras.layers.Dense(80000, activation='relu'),   \n",
        "                                 tf.keras.layers.Dense(8, activation='softmax')])\n",
        "    model.compile(optimizer='rmsprop',\n",
        "                  loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),\n",
        "                  metrics=['accuracy'])\n",
        "    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
        "    history = model.fit(x, y, epochs=40, batch_size=32, callbacks=[callback], validation_data=(val_x, val_y))\n",
        "    return history, model\n",
        "\n",
        "def plot_accuracy(history):\n",
        "    plt.plot(history.history['accuracy'])\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "    return\n",
        "\n",
        "def plot_example_predictions(test_images, test_inputs, test_labels, model):\n",
        "    rows = 2\n",
        "    cols = 4\n",
        "    fig, axes = plt.subplots(nrows=rows, ncols=cols)\n",
        "    for i in range(rows*cols):\n",
        "        temp = test_inputs[i]\n",
        "        temp = temp[np.newaxis is None,:]\n",
        "        pred = model.predict(temp)\n",
        "        plt.subplot(int(str(rows) + str(cols) + str(i+1)))\n",
        "        plt.imshow((test_images[i, :, :]))\n",
        "        title = str(\"\\nPredicted \\nclass =\" + str(np.argmax(pred))+\",\\nTrue class = \" + str(np.argmax(test_labels[i])))\n",
        "        plt.title(title)\n",
        "    fig.tight_layout()\n",
        "    plt.savefig(\"d2_test_predictions.png\")\n",
        "    plt.show()\n",
        "    return\n",
        "\n",
        "def main():\n",
        "\n",
        "    path = '/content/drive/MyDrive/PRNN/bloodmnist.npz'\n",
        "    train_x, train_y, test_x, test_y, val_x, val_y, test_images = process_data(path)\n",
        "    \n",
        "    history, trained_model = MLP_train(train_x, train_y, val_x, val_y)\n",
        "    test_loss, test_acc = trained_model.evaluate(test_x,  test_y, verbose=2)\n",
        "    print(\"Test Accuracy = \", test_acc)\n",
        "\n",
        "    plot_accuracy(history)\n",
        "    plot_example_predictions(test_images, test_x, test_y, trained_model)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from numba import jit\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage.transform import downscale_local_mean, rescale\n",
        "\n",
        "\n",
        "def process_data(path):\n",
        "\n",
        "    dataset = np.load(path)\n",
        "    train_x, train_y = dataset['train_images']/255.0, dataset['train_labels']\n",
        "    val_x, val_y = dataset['val_images']/255.0, dataset['val_labels']\n",
        "    test_x, test_y = dataset['test_images']/255.0, dataset['test_labels']\n",
        "\n",
        "    scale = 4\n",
        "\n",
        "    train_x_ = np.zeros((train_x.shape[0], train_x.shape[1]**2//scale**2))\n",
        "    val_x_ = np.zeros((val_x.shape[0], val_x.shape[1]**2//scale**2))\n",
        "    test_x_ = np.zeros((test_x.shape[0], test_x.shape[1]**2//scale**2))\n",
        "\n",
        "    for i in range(train_x.shape[0]):\n",
        "        temp = downscale_local_mean(train_x[i, :, :], (scale, scale))\n",
        "        train_x_[i, :] = temp.reshape(train_x.shape[1]**2//scale**2)\n",
        "        #train_x_[i, :] = train_x_[i, :]/np.max(train_x_[i, :])\n",
        "\n",
        "    for i in range(val_x.shape[0]):\n",
        "        temp = downscale_local_mean(val_x[i, :, :], (scale, scale))\n",
        "        val_x_[i, :] = temp.reshape(val_x.shape[1]**2//scale**2)\n",
        "        #val_x_[i, :] = val_x_[i, :]/np.max(val_x_[i, :])\n",
        "\n",
        "    for i in range(test_x.shape[0]):\n",
        "        temp = downscale_local_mean(test_x[i, :, :], (scale, scale))\n",
        "        test_x_[i, :] = temp.reshape(test_x.shape[1]**2//scale**2)\n",
        "        #test_x_[i, :] = test_x_[i, :]/np.max(test_x_[i, :])\n",
        "\n",
        "    return train_x_, train_y, test_x_, test_y, val_x_, val_y, test_x\n",
        "\n",
        "\n",
        "@jit(nopython=True)\n",
        "def ReLU(x):\n",
        "    return x * (x > 0)\n",
        "\n",
        "\n",
        "@jit(nopython=True)\n",
        "def dReLU(x):\n",
        "    return 1.0 * (x > 0)\n",
        "\n",
        "\n",
        "@jit(nopython=True)\n",
        "def sigmoid(x):\n",
        "    return 1.0/(1.0+np.exp(-x))\n",
        "\n",
        "\n",
        "def forward_pass(h0, theta):\n",
        "    h0 = h0.reshape((h0.shape[0], 1))\n",
        "    W1, b1, W2, b2 = theta\n",
        "\n",
        "    a1 = b1 + np.matmul(W1, h0)\n",
        "    h1 = ReLU(a1)\n",
        "    a2 = b2 + np.matmul(W2, h1)\n",
        "    y_hat = sigmoid(a2)\n",
        "    outputs = [h1, a1, a2, y_hat]\n",
        "    return outputs\n",
        "\n",
        "\n",
        "def backward_pass(h0, h1, a1, a2, y_hat, y, W2):\n",
        "    grad_a2_L = y_hat - y\n",
        "    grad_W2_L = np.outer(grad_a2_L, h1.T)\n",
        "    grad_b2_L = grad_a2_L\n",
        "    grad_h1_L = np.matmul(W2.T, grad_a2_L)\n",
        "    grad_a1_L = grad_h1_L * dReLU(a1)\n",
        "    grad_W1_L = np.outer(grad_a1_L, h0.T)\n",
        "    grad_b1_L = grad_a1_L\n",
        "\n",
        "    gradients = [grad_W1_L, grad_W2_L, grad_b2_L, grad_b1_L]\n",
        "    return gradients\n",
        "\n",
        "\n",
        "def grad_descent(train_x, train_y, parameters):\n",
        "    W1, b1, W2, b2 = parameters\n",
        "    t = 0\n",
        "    max_iters = 100\n",
        "    lr = 1e-4\n",
        "    no_correct = 0\n",
        "    tp = 0\n",
        "    tn = 0\n",
        "    fp = 0\n",
        "    fn = 0\n",
        "    f1 = 0.0\n",
        "    acc = 0.0\n",
        "\n",
        "    for i in range(train_x.shape[0]):\n",
        "        h0 = train_x[i]\n",
        "        y = train_y[i]\n",
        "\n",
        "        while(t < max_iters):\n",
        "            h1, a1, a2, y_hat = forward_pass(h0, parameters)\n",
        "            grad_W1_L, grad_W2_L, grad_b2_L, grad_b1_L = backward_pass(h0, h1, a1, a2, y_hat, y, W2)\n",
        "            W1 = W1 - lr*grad_W1_L\n",
        "            W2 = W2 - lr*grad_W2_L\n",
        "            b2 = b2 - lr*grad_b2_L\n",
        "            b1 = b1 - lr*grad_b1_L\n",
        "            t += 1\n",
        "        \n",
        "        if(np.round(y_hat) == 1):\n",
        "            if(train_y[i] == 1):\n",
        "                tp += 1\n",
        "            else:\n",
        "                fp += 1\n",
        "        else:\n",
        "            if(train_y[i] == 0):\n",
        "                tn += 1\n",
        "            else:\n",
        "                fn += 1\n",
        "\n",
        "    acc = (tp+tn)/(tp+tn+fp+fn)\n",
        "    f1 = tp/(tp+0.5*(fp+fn))\n",
        "    #print(\"\\nTraining Accuracy = \", acc)\n",
        "    return W1, b1, W2, b2, acc, f1\n",
        "\n",
        "def train_loop(train_x, train_y, val_x, val_y, epochs):\n",
        "\n",
        "    h0 = train_x[0]\n",
        "    y = train_y[0]\n",
        "\n",
        "    input_dim = h0.shape[0]\n",
        "    hidden_layer_dim = 2*input_dim\n",
        "    output_dim = y.shape[0]\n",
        "\n",
        "    accuracy = []\n",
        "\n",
        "    W1 = np.random.rand(hidden_layer_dim, input_dim)*np.random.normal(size=1)\n",
        "    b1 = np.random.rand(hidden_layer_dim, 1)\n",
        "    W2 = np.random.rand(output_dim, hidden_layer_dim)\n",
        "    b2 = np.random.rand(output_dim, 1)\n",
        "    for i in tqdm(range(epochs)):\n",
        "        #if(i % 2==0):\n",
        "        W1, b1, W2, b2, acc, f1 = grad_descent(train_x[i:i-1+train_x.shape[0]//epochs, :], train_y[i:i-1+train_x.shape[0]//epochs, :], [W1, b1, W2, b2])\n",
        "        #else:\n",
        "        #    W1, b1, W2, b2, acc, f1 = grad_descent(val_x[i:i-1+train_x.shape[0]//epochs, :], val_y[i:i-1+train_x.shape[0]//epochs, :], [W1, b1, W2, b2])\n",
        "        accuracy.append(acc)\n",
        "    \n",
        "    #print(\"F1 score = \", f1)\n",
        "    plot_accuracy(accuracy)\n",
        "    return W1, b1, W2, b2\n",
        "\n",
        "def evaluate(test_x, test_y, parameters, name_data):\n",
        "    tp = 0\n",
        "    tn = 0\n",
        "    fp = 0\n",
        "    fn = 0\n",
        "    f1 = 0.0\n",
        "    acc = 0.0\n",
        "\n",
        "    for i in range(test_x.shape[0]):\n",
        "        _, _, _, y_hat = forward_pass(test_x[i], parameters)\n",
        "        if(np.round(y_hat) == 1):\n",
        "            if(test_y[i] == 1):\n",
        "                tp += 1\n",
        "            else:\n",
        "                fp += 1\n",
        "        else:\n",
        "            if(test_y[i] == 0):\n",
        "                tn += 1\n",
        "            else:\n",
        "                fn += 1\n",
        "\n",
        "    acc = (tp+tn)/(tp+tn+fp+fn)\n",
        "    f1 = tp/(tp+0.5*(fp+fn))\n",
        "    print(\"\\n\\n\", name_data, \"accuracy = \", acc)\n",
        "    print(\"\\n\", name_data, \"F1-Score = \", f1)\n",
        "    return\n",
        "\n",
        "def plot_accuracy(acc):\n",
        "    plt.plot(acc[:-5])\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.ylim(0.0,1.0)\n",
        "    plt.grid()\n",
        "    plt.title(\"Training Accuracy\")\n",
        "    plt.show()\n",
        "    return\n",
        "\n",
        "def main():\n",
        "    path = '/content//drive/MyDrive/PRNN/pneumoniamnist.npz'\n",
        "    train_x, train_y, test_x, test_y, val_x, val_y, test_images = process_data(path)\n",
        "    W1, b1, W2, b2 = train_loop(train_x, train_y, val_x, val_y, 200)\n",
        "    evaluate(test_x, test_y, [W1, b1, W2, b2], 'Test')\n",
        "    return\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "\n",
        "\"\"\"Backpropagation of single layer MLP with L2 regularization\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from numba import jit\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage.transform import downscale_local_mean, rescale\n",
        "\n",
        "\n",
        "def process_data(path):\n",
        "\n",
        "    dataset = np.load(path)\n",
        "    train_x, train_y = dataset['train_images']/255.0, dataset['train_labels']\n",
        "    val_x, val_y = dataset['val_images']/255.0, dataset['val_labels']\n",
        "    test_x, test_y = dataset['test_images']/255.0, dataset['test_labels']\n",
        "\n",
        "    scale = 4\n",
        "\n",
        "    train_x_ = np.zeros((train_x.shape[0], train_x.shape[1]**2//scale**2))\n",
        "    val_x_ = np.zeros((val_x.shape[0], val_x.shape[1]**2//scale**2))\n",
        "    test_x_ = np.zeros((test_x.shape[0], test_x.shape[1]**2//scale**2))\n",
        "\n",
        "    for i in range(train_x.shape[0]):\n",
        "        temp = downscale_local_mean(train_x[i, :, :], (scale, scale))\n",
        "        train_x_[i, :] = temp.reshape(train_x.shape[1]**2//scale**2)\n",
        "        #train_x_[i, :] = train_x_[i, :]/np.max(train_x_[i, :])\n",
        "\n",
        "    for i in range(val_x.shape[0]):\n",
        "        temp = downscale_local_mean(val_x[i, :, :], (scale, scale))\n",
        "        val_x_[i, :] = temp.reshape(val_x.shape[1]**2//scale**2)\n",
        "        #val_x_[i, :] = val_x_[i, :]/np.max(val_x_[i, :])\n",
        "\n",
        "    for i in range(test_x.shape[0]):\n",
        "        temp = downscale_local_mean(test_x[i, :, :], (scale, scale))\n",
        "        test_x_[i, :] = temp.reshape(test_x.shape[1]**2//scale**2)\n",
        "        #test_x_[i, :] = test_x_[i, :]/np.max(test_x_[i, :])\n",
        "\n",
        "    return train_x_, train_y, test_x_, test_y, val_x_, val_y, test_x\n",
        "\n",
        "\n",
        "@jit(nopython=True)\n",
        "def ReLU(x):\n",
        "    return x * (x > 0)\n",
        "\n",
        "\n",
        "@jit(nopython=True)\n",
        "def dReLU(x):\n",
        "    return 1.0 * (x > 0)\n",
        "\n",
        "\n",
        "@jit(nopython=True)\n",
        "def sigmoid(x):\n",
        "    return 1.0/(1.0+np.exp(-x))\n",
        "\n",
        "\n",
        "def forward_pass(h0, theta):\n",
        "    h0 = h0.reshape((h0.shape[0], 1))\n",
        "    W1, b1, W2, b2 = theta\n",
        "\n",
        "    a1 = b1 + np.matmul(W1, h0)\n",
        "    h1 = ReLU(a1)\n",
        "    a2 = b2 + np.matmul(W2, h1)\n",
        "    y_hat = sigmoid(a2)\n",
        "    outputs = [h1, a1, a2, y_hat]\n",
        "    return outputs\n",
        "\n",
        "\n",
        "def backward_pass(h0, h1, a1, a2, y_hat, y, W2):\n",
        "    grad_a2_L = y_hat - y\n",
        "    grad_W2_L = np.outer(grad_a2_L, h1.T)\n",
        "    grad_b2_L = grad_a2_L\n",
        "    grad_h1_L = np.matmul(W2.T, grad_a2_L)\n",
        "    grad_a1_L = grad_h1_L * dReLU(a1)\n",
        "    grad_W1_L = np.outer(grad_a1_L, h0.T)\n",
        "    grad_b1_L = grad_a1_L\n",
        "\n",
        "    gradients = [grad_W1_L, grad_W2_L, grad_b2_L, grad_b1_L]\n",
        "    return gradients\n",
        "\n",
        "\n",
        "def grad_descent(train_x, train_y, parameters, lambda_):\n",
        "    W1, b1, W2, b2 = parameters\n",
        "    t = 0\n",
        "    max_iters = 100\n",
        "    lr = 1e-4\n",
        "    no_correct = 0\n",
        "    tp = 0\n",
        "    tn = 0\n",
        "    fp = 0\n",
        "    fn = 0\n",
        "    f1 = 0.0\n",
        "    acc = 0.0\n",
        "\n",
        "    for i in range(train_x.shape[0]):\n",
        "        h0 = train_x[i]\n",
        "        y = train_y[i]\n",
        "\n",
        "        while(t < max_iters):\n",
        "            h1, a1, a2, y_hat = forward_pass(h0, parameters)\n",
        "            grad_W1_L, grad_W2_L, grad_b2_L, grad_b1_L = backward_pass(h0, h1, a1, a2, y_hat, y, W2)\n",
        "            W1 = (1-lr*lambda_)*W1 - lr*grad_W1_L\n",
        "            W2 = (1-lr*lambda_)*W2 - lr*grad_W2_L\n",
        "            b2 = (1-lr*lambda_)*b2 - lr*grad_b2_L\n",
        "            b1 = (1-lr*lambda_)*b1 - lr*grad_b1_L\n",
        "            t += 1\n",
        "        \n",
        "        if(np.round(y_hat) == 1):\n",
        "            if(train_y[i] == 1):\n",
        "                tp += 1\n",
        "            else:\n",
        "                fp += 1\n",
        "        else:\n",
        "            if(train_y[i] == 0):\n",
        "                tn += 1\n",
        "            else:\n",
        "                fn += 1\n",
        "\n",
        "    acc = (tp+tn)/(tp+tn+fp+fn)\n",
        "    f1 = tp/(tp+0.5*(fp+fn))\n",
        "    #print(\"\\nTraining Accuracy = \", acc)\n",
        "    return W1, b1, W2, b2, acc, f1\n",
        "\n",
        "def train_loop(train_x, train_y, val_x, val_y, epochs):\n",
        "\n",
        "    h0 = train_x[0]\n",
        "    y = train_y[0]\n",
        "\n",
        "    input_dim = h0.shape[0]\n",
        "    hidden_layer_dim = 2*input_dim\n",
        "    output_dim = y.shape[0]\n",
        "\n",
        "    accuracy = []\n",
        "\n",
        "    W1 = np.random.rand(hidden_layer_dim, input_dim)*np.random.normal(100, 5)\n",
        "    b1 = np.random.rand(hidden_layer_dim, 1)*np.random.normal(10, 2)\n",
        "    W2 = np.random.rand(output_dim, hidden_layer_dim)*np.random.normal(-10, 2)\n",
        "    b2 = np.random.rand(output_dim, 1)*np.random.normal(size=1)\n",
        "    for i in tqdm(range(epochs)):\n",
        "        W1, b1, W2, b2, acc, f1 = grad_descent(train_x[i:i-1+train_x.shape[0]//epochs, :], train_y[i:i-1+train_x.shape[0]//epochs, :], [W1, b1, W2, b2], 0.1)\n",
        "        accuracy.append(acc)\n",
        "    \n",
        "    plot_accuracy(accuracy)\n",
        "    return W1, b1, W2, b2\n",
        "\n",
        "def evaluate(test_x, test_y, parameters, name_data):\n",
        "    tp = 0\n",
        "    tn = 0\n",
        "    fp = 0\n",
        "    fn = 0\n",
        "    f1 = 0.0\n",
        "    acc = 0.0\n",
        "\n",
        "    for i in range(test_x.shape[0]):\n",
        "        _, _, _, y_hat = forward_pass(test_x[i], parameters)\n",
        "        if(np.round(y_hat) == 1):\n",
        "            if(test_y[i] == 1):\n",
        "                tp += 1\n",
        "            else:\n",
        "                fp += 1\n",
        "        else:\n",
        "            if(test_y[i] == 0):\n",
        "                tn += 1\n",
        "            else:\n",
        "                fn += 1\n",
        "\n",
        "    acc = (tp+tn)/(tp+tn+fp+fn)\n",
        "    f1 = tp/(tp+0.5*(fp+fn))\n",
        "    print(\"\\n\\n\", name_data, \"accuracy = \", acc)\n",
        "    print(\"\\n\", name_data, \"F1-Score = \", f1)\n",
        "    return\n",
        "\n",
        "def plot_accuracy(acc):\n",
        "    plt.plot(acc[:-5])\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.ylim(0.0,1.0)\n",
        "    plt.grid()\n",
        "    plt.title(\"Training Accuracy\")\n",
        "    plt.show()\n",
        "    return\n",
        "\n",
        "def main():\n",
        "    path = '/content//drive/MyDrive/PRNN/pneumoniamnist.npz'\n",
        "    train_x, train_y, test_x, test_y, val_x, val_y, test_images = process_data(path)\n",
        "    W1, b1, W2, b2 = train_loop(train_x, train_y, val_x, val_y, 200)\n",
        "    evaluate(test_x, test_y, [W1, b1, W2, b2], 'Test')\n",
        "    return\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "\n",
        "\"\"\"Backpropagation of single layer MLP with L1 regularization\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from numba import jit\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage.transform import downscale_local_mean, rescale\n",
        "\n",
        "\n",
        "def process_data(path):\n",
        "\n",
        "    dataset = np.load(path)\n",
        "    train_x, train_y = dataset['train_images']/255.0, dataset['train_labels']\n",
        "    val_x, val_y = dataset['val_images']/255.0, dataset['val_labels']\n",
        "    test_x, test_y = dataset['test_images']/255.0, dataset['test_labels']\n",
        "\n",
        "    scale = 4\n",
        "\n",
        "    train_x_ = np.zeros((train_x.shape[0], train_x.shape[1]**2//scale**2))\n",
        "    val_x_ = np.zeros((val_x.shape[0], val_x.shape[1]**2//scale**2))\n",
        "    test_x_ = np.zeros((test_x.shape[0], test_x.shape[1]**2//scale**2))\n",
        "\n",
        "    for i in range(train_x.shape[0]):\n",
        "        temp = downscale_local_mean(train_x[i, :, :], (scale, scale))\n",
        "        train_x_[i, :] = temp.reshape(train_x.shape[1]**2//scale**2)\n",
        "        #train_x_[i, :] = train_x_[i, :]/np.max(train_x_[i, :])\n",
        "\n",
        "    for i in range(val_x.shape[0]):\n",
        "        temp = downscale_local_mean(val_x[i, :, :], (scale, scale))\n",
        "        val_x_[i, :] = temp.reshape(val_x.shape[1]**2//scale**2)\n",
        "        #val_x_[i, :] = val_x_[i, :]/np.max(val_x_[i, :])\n",
        "\n",
        "    for i in range(test_x.shape[0]):\n",
        "        temp = downscale_local_mean(test_x[i, :, :], (scale, scale))\n",
        "        test_x_[i, :] = temp.reshape(test_x.shape[1]**2//scale**2)\n",
        "        #test_x_[i, :] = test_x_[i, :]/np.max(test_x_[i, :])\n",
        "\n",
        "    return train_x_, train_y, test_x_, test_y, val_x_, val_y, test_x\n",
        "\n",
        "\n",
        "@jit(nopython=True)\n",
        "def ReLU(x):\n",
        "    return x * (x > 0)\n",
        "\n",
        "\n",
        "@jit(nopython=True)\n",
        "def dReLU(x):\n",
        "    return 1.0 * (x > 0)\n",
        "\n",
        "\n",
        "@jit(nopython=True)\n",
        "def sigmoid(x):\n",
        "    return 1.0/(1.0+np.exp(-x))\n",
        "\n",
        "\n",
        "def forward_pass(h0, theta):\n",
        "    h0 = h0.reshape((h0.shape[0], 1))\n",
        "    W1, b1, W2, b2 = theta\n",
        "\n",
        "    a1 = b1 + np.matmul(W1, h0)\n",
        "    h1 = ReLU(a1)\n",
        "    a2 = b2 + np.matmul(W2, h1)\n",
        "    y_hat = sigmoid(a2)\n",
        "    outputs = [h1, a1, a2, y_hat]\n",
        "    return outputs\n",
        "\n",
        "\n",
        "def backward_pass(h0, h1, a1, a2, y_hat, y, W2):\n",
        "    grad_a2_L = y_hat - y\n",
        "    grad_W2_L = np.outer(grad_a2_L, h1.T)\n",
        "    grad_b2_L = grad_a2_L\n",
        "    grad_h1_L = np.matmul(W2.T, grad_a2_L)\n",
        "    grad_a1_L = grad_h1_L * dReLU(a1)\n",
        "    grad_W1_L = np.outer(grad_a1_L, h0.T)\n",
        "    grad_b1_L = grad_a1_L\n",
        "\n",
        "    gradients = [grad_W1_L, grad_W2_L, grad_b2_L, grad_b1_L]\n",
        "    return gradients\n",
        "\n",
        "\n",
        "def grad_descent(train_x, train_y, parameters, lambda_):\n",
        "    W1, b1, W2, b2 = parameters\n",
        "    t = 0\n",
        "    max_iters = 100\n",
        "    lr = 1e-4\n",
        "    no_correct = 0\n",
        "    tp = 0\n",
        "    tn = 0\n",
        "    fp = 0\n",
        "    fn = 0\n",
        "    f1 = 0.0\n",
        "    acc = 0.0\n",
        "\n",
        "    for i in range(train_x.shape[0]):\n",
        "        h0 = train_x[i]\n",
        "        y = train_y[i]\n",
        "\n",
        "        while(t < max_iters):\n",
        "            h1, a1, a2, y_hat = forward_pass(h0, parameters)\n",
        "            grad_W1_L, grad_W2_L, grad_b2_L, grad_b1_L = backward_pass(h0, h1, a1, a2, y_hat, y, W2)\n",
        "            W1 = W1 + (-lr*lambda_)*np.sign(W1) - lr*grad_W1_L\n",
        "            W2 = W2 + (-lr*lambda_)*np.sign(W2) - lr*grad_W2_L\n",
        "            b2 = b2 + (-lr*lambda_)*np.sign(b2) - lr*grad_b2_L\n",
        "            b1 = b1 + (-lr*lambda_)*np.sign(b1) - lr*grad_b1_L\n",
        "            t += 1\n",
        "            t += 1\n",
        "        \n",
        "        if(np.round(y_hat) == 1):\n",
        "            if(train_y[i] == 1):\n",
        "                tp += 1\n",
        "            else:\n",
        "                fp += 1\n",
        "        else:\n",
        "            if(train_y[i] == 0):\n",
        "                tn += 1\n",
        "            else:\n",
        "                fn += 1\n",
        "\n",
        "    acc = (tp+tn)/(tp+tn+fp+fn)\n",
        "    f1 = tp/(tp+0.5*(fp+fn))\n",
        "    #print(\"\\nTraining Accuracy = \", acc)\n",
        "    return W1, b1, W2, b2, acc, f1\n",
        "\n",
        "def train_loop(train_x, train_y, val_x, val_y, epochs):\n",
        "\n",
        "    h0 = train_x[0]\n",
        "    y = train_y[0]\n",
        "\n",
        "    input_dim = h0.shape[0]\n",
        "    hidden_layer_dim = 2*input_dim\n",
        "    output_dim = y.shape[0]\n",
        "\n",
        "    accuracy = []\n",
        "\n",
        "    W1 = np.random.rand(hidden_layer_dim, input_dim)*np.random.normal(100, 5)\n",
        "    b1 = np.random.rand(hidden_layer_dim, 1)*np.random.normal(10, 2)\n",
        "    W2 = np.random.rand(output_dim, hidden_layer_dim)*np.random.normal(-10, 2)\n",
        "    b2 = np.random.rand(output_dim, 1)*np.random.normal(size=1)\n",
        "    for i in tqdm(range(epochs)):\n",
        "        W1, b1, W2, b2, acc, f1 = grad_descent(train_x[i:i-1+train_x.shape[0]//epochs, :], train_y[i:i-1+train_x.shape[0]//epochs, :], [W1, b1, W2, b2], 0.1)\n",
        "        accuracy.append(acc)\n",
        "    \n",
        "    plot_accuracy(accuracy)\n",
        "    return W1, b1, W2, b2\n",
        "\n",
        "def evaluate(test_x, test_y, parameters, name_data):\n",
        "    tp = 0\n",
        "    tn = 0\n",
        "    fp = 0\n",
        "    fn = 0\n",
        "    f1 = 0.0\n",
        "    acc = 0.0\n",
        "\n",
        "    for i in range(test_x.shape[0]):\n",
        "        _, _, _, y_hat = forward_pass(test_x[i], parameters)\n",
        "        if(np.round(y_hat) == 1):\n",
        "            if(test_y[i] == 1):\n",
        "                tp += 1\n",
        "            else:\n",
        "                fp += 1\n",
        "        else:\n",
        "            if(test_y[i] == 0):\n",
        "                tn += 1\n",
        "            else:\n",
        "                fn += 1\n",
        "\n",
        "    acc = (tp+tn)/(tp+tn+fp+fn)\n",
        "    f1 = tp/(tp+0.5*(fp+fn))\n",
        "    print(\"\\n\\n\", name_data, \"accuracy = \", acc)\n",
        "    print(\"\\n\", name_data, \"F1-Score = \", f1)\n",
        "    return\n",
        "\n",
        "def plot_accuracy(acc):\n",
        "    plt.plot(acc[:-5])\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.grid()\n",
        "    plt.ylim(0.0,1.0)\n",
        "    plt.title(\"Training Accuracy\")\n",
        "    plt.show()\n",
        "    return\n",
        "\n",
        "def main():\n",
        "    path = '/content//drive/MyDrive/PRNN/pneumoniamnist.npz'\n",
        "    train_x, train_y, test_x, test_y, val_x, val_y, test_images = process_data(path)\n",
        "    W1, b1, W2, b2 = train_loop(train_x, train_y, val_x, val_y, 200)\n",
        "    evaluate(test_x, test_y, [W1, b1, W2, b2], 'Test')\n",
        "    return\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "\n",
        "\"\"\"Backpropagation of single layer MLP with Dropout\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from numba import jit\n",
        "from skimage.transform import downscale_local_mean, rescale\n",
        "\n",
        "\n",
        "def process_data(path):\n",
        "\n",
        "    dataset = np.load(path)\n",
        "    train_x, train_y = dataset['train_images']/255.0, dataset['train_labels']\n",
        "    val_x, val_y = dataset['val_images']/255.0, dataset['val_labels']\n",
        "    test_x, test_y = dataset['test_images']/255.0, dataset['test_labels']\n",
        "\n",
        "    scale = 4\n",
        "\n",
        "    train_x_ = np.zeros((train_x.shape[0], train_x.shape[1]**2//scale**2))\n",
        "    val_x_ = np.zeros((val_x.shape[0], val_x.shape[1]**2//scale**2))\n",
        "    test_x_ = np.zeros((test_x.shape[0], test_x.shape[1]**2//scale**2))\n",
        "\n",
        "    for i in range(train_x.shape[0]):\n",
        "        temp = downscale_local_mean(train_x[i, :, :], (scale, scale))\n",
        "        train_x_[i, :] = temp.reshape(train_x.shape[1]**2//scale**2)\n",
        "        #train_x_[i, :] = train_x_[i, :]/np.max(train_x_[i, :])\n",
        "\n",
        "    for i in range(val_x.shape[0]):\n",
        "        temp = downscale_local_mean(val_x[i, :, :], (scale, scale))\n",
        "        val_x_[i, :] = temp.reshape(val_x.shape[1]**2//scale**2)\n",
        "        #val_x_[i, :] = val_x_[i, :]/np.max(val_x_[i, :])\n",
        "\n",
        "    for i in range(test_x.shape[0]):\n",
        "        temp = downscale_local_mean(test_x[i, :, :], (scale, scale))\n",
        "        test_x_[i, :] = temp.reshape(test_x.shape[1]**2//scale**2)\n",
        "        #test_x_[i, :] = test_x_[i, :]/np.max(test_x_[i, :])\n",
        "\n",
        "    return train_x_, train_y, test_x_, test_y, val_x_, val_y, test_x\n",
        "\n",
        "\n",
        "@jit(nopython=True)\n",
        "def ReLU(x):\n",
        "    return x * (x > 0)\n",
        "\n",
        "\n",
        "@jit(nopython=True)\n",
        "def dReLU(x):\n",
        "    return 1.0 * (x > 0)\n",
        "\n",
        "\n",
        "@jit(nopython=True)\n",
        "def sigmoid(x):\n",
        "    return 1.0/(1.0+np.exp(-x))\n",
        "\n",
        "\n",
        "def forward_pass(h0, theta):\n",
        "    p = 0.9\n",
        "    h0 = h0.reshape((h0.shape[0], 1))\n",
        "    W1, b1, W2, b2 = theta\n",
        "\n",
        "    a1 = b1 + np.matmul(W1, h0)\n",
        "    rand1 = np.random.rand(a1.shape[0], 1) < p\n",
        "    a1 = a1 * rand1\n",
        "    h1 = ReLU(a1)\n",
        "    a2 = b2 + np.matmul(W2, h1)\n",
        "    #print(a2.shape)\n",
        "    #rand2 = np.random.rand(a2.shape[0], 1) < p\n",
        "    #a2 = a2 * rand1\n",
        "    #print(a2.shape)\n",
        "    y_hat = sigmoid(a2)\n",
        "    outputs = [h1, a1, a2, y_hat]\n",
        "    return outputs\n",
        "\n",
        "\n",
        "def backward_pass(h0, h1, a1, a2, y_hat, y, W2):\n",
        "    grad_a2_L = y_hat - y\n",
        "    grad_W2_L = np.outer(grad_a2_L, h1.T)\n",
        "    grad_b2_L = grad_a2_L\n",
        "    grad_h1_L = np.matmul(W2.T, grad_a2_L)\n",
        "    grad_a1_L = grad_h1_L * dReLU(a1)\n",
        "    grad_W1_L = np.outer(grad_a1_L, h0.T)\n",
        "    grad_b1_L = grad_a1_L\n",
        "\n",
        "    gradients = [grad_W1_L, grad_W2_L, grad_b2_L, grad_b1_L]\n",
        "    return gradients\n",
        "\n",
        "\n",
        "def grad_descent(train_x, train_y, parameters):\n",
        "    W1, b1, W2, b2 = parameters\n",
        "    t = 0\n",
        "    max_iters = 100\n",
        "    lr = 1e-2\n",
        "    no_correct = 0\n",
        "\n",
        "    for i in range(train_x.shape[0]):\n",
        "        h0 = train_x[i]\n",
        "        y = train_y[i]\n",
        "\n",
        "        while(t < max_iters):\n",
        "            h1, a1, a2, y_hat = forward_pass(h0, parameters)\n",
        "            grad_W1_L, grad_W2_L, grad_b2_L, grad_b1_L = backward_pass(h0, h1, a1, a2, y_hat, y, W2)\n",
        "            W1 = W1 - lr*grad_W1_L\n",
        "            W2 = W2 - lr*grad_W2_L\n",
        "            b2 = b2 - lr*grad_b2_L\n",
        "            b1 = b1 - lr*grad_b1_L\n",
        "            t += 1\n",
        "        \n",
        "        if(y_hat == train_y[i]):\n",
        "            no_correct += 1\n",
        "    acc = no_correct/train_x.shape[0]\n",
        "    print(\"\\nTraining Accuracy = \", acc)\n",
        "    return W1, b1, W2, b2\n",
        "\n",
        "def train_loop(train_x, train_y, val_x, val_y, epochs):\n",
        "\n",
        "    h0 = train_x[0]\n",
        "    y = train_y[0]\n",
        "\n",
        "    input_dim = h0.shape[0]\n",
        "    hidden_layer_dim = 2*input_dim\n",
        "    output_dim = y.shape[0]\n",
        "\n",
        "    W1 = np.random.rand(hidden_layer_dim, input_dim)\n",
        "    b1 = np.random.rand(hidden_layer_dim, 1)\n",
        "    W2 = np.random.rand(output_dim, hidden_layer_dim)\n",
        "    b2 = np.random.rand(output_dim, 1)\n",
        "    for i in tqdm(range(epochs)):\n",
        "        if(i % 2==0):\n",
        "            W1, b1, W2, b2 = grad_descent(train_x, train_y, [W1, b1, W2, b2])\n",
        "        else:\n",
        "            W1, b1, W2, b2 = grad_descent(val_x, val_y, [W1, b1, W2, b2])\n",
        "    return W1, b1, W2, b2\n",
        "\n",
        "def evaluate(test_x, test_y, parameters, name_data):\n",
        "    no_correct = 0\n",
        "    for i in range(test_x.shape[0]):\n",
        "        _, _, _, y_hat = forward_pass(test_x[i], parameters)\n",
        "        if(y_hat == test_y[i]):\n",
        "            no_correct += 1\n",
        "    acc = no_correct/test_x.shape[0]\n",
        "    print(\"\\n\\n\", name_data, \"accuracy = \", acc)\n",
        "    return\n",
        "\n",
        "\n",
        "def main():\n",
        "    path = '/content/drive/MyDrive/PRNN/pneumoniamnist.npz'\n",
        "    train_x, train_y, test_x, test_y, val_x, val_y, test_images = process_data(path)\n",
        "    W1, b1, W2, b2 = train_loop(train_x, train_y, val_x, val_y, epochs=100)\n",
        "    evaluate(test_x, test_y, [W1, b1, W2, b2], 'Test')\n",
        "    return\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "\n",
        "\"\"\"# Backpropagation with Dropout\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "#import datasets.mnist.loader as mnist\n",
        "import matplotlib.pylab as plt\n",
        "from sklearn .preprocessing import OneHotEncoder\n",
        " \n",
        " \n",
        "class ANN:\n",
        "    def __init__(self, layers_size,p):\n",
        "        self.layers_size = layers_size\n",
        "        #print(layers_size)\n",
        "        self.parameters = {}\n",
        "        self.L = len(self.layers_size)\n",
        "        self.n = 0\n",
        "        self.p=p\n",
        "        self.costs = []\n",
        " \n",
        "    def sigmoid(self, Z):\n",
        "        return 1 / (1 + np.exp(-Z))\n",
        " \n",
        "    def softmax(self, Z):\n",
        "        expZ = np.exp(Z - np.max(Z))\n",
        "        return expZ / expZ.sum(axis=0, keepdims=True)\n",
        " \n",
        "    def initialize_parameters(self):\n",
        "        np.random.seed(1)\n",
        "        print(self.layers_size)\n",
        "        for l in range(1, len(self.layers_size)):\n",
        "            self.parameters[\"W\" + str(l)] = np.random.randn(self.layers_size[l], self.layers_size[l - 1]) / np.sqrt(\n",
        "                self.layers_size[l - 1])\n",
        "            self.parameters[\"b\" + str(l)] = np.zeros((self.layers_size[l], 1))\n",
        "            #print(\"W\" + str(l)+\" shape \")\n",
        "            #print(self.parameters[\"W\" + str(l)].shape)\n",
        "            #print(\"b\" + str(l)+\" shape \")\n",
        "            #print(self.parameters[\"b\" + str(l)].shape)\n",
        "            \n",
        "            \n",
        " \n",
        "    def forward(self, X):\n",
        "        store = {}\n",
        " \n",
        "        A = X.T\n",
        "        for l in range(self.L - 1):\n",
        "            #print(\"W\" + str(l + 1)+\"shape is\")\n",
        "            #print(self.parameters[\"W\" + str(l + 1)].shape)\n",
        "            #print(\"A \"+\"shape is\")\n",
        "            #print(A.shape)\n",
        "            #print(\"W\" + str(l+1)+\" dot A shape \")\n",
        "            #print(self.parameters[\"W\" + str(l + 1)].dot(A).shape)\n",
        "            #print(\"b\" + str(l+1)+\" shape \")\n",
        "            #print(self.parameters[\"b\" + str(l+1)].shape)\n",
        "            \n",
        "            Z = self.parameters[\"W\" + str(l + 1)].dot(A) + self.parameters[\"b\" + str(l + 1)]\n",
        "            #print(\"Z shape \")\n",
        "            #print(Z.shape)\n",
        "            A = self.sigmoid(Z)\n",
        "            store[\"A\" + str(l + 1)] = A\n",
        "            store[\"W\" + str(l + 1)] = self.parameters[\"W\" + str(l + 1)]\n",
        "            store[\"Z\" + str(l + 1)] = Z\n",
        "            #print(\"Stored A,W,Z for index \"+str(l + 1))\n",
        " \n",
        "        Z = self.parameters[\"W\" + str(self.L)].dot(A) + self.parameters[\"b\" + str(self.L)]\n",
        "        A = self.softmax(Z)\n",
        "        store[\"A\" + str(self.L)] = A\n",
        "        store[\"W\" + str(self.L)] = self.parameters[\"W\" + str(self.L)]\n",
        "        store[\"Z\" + str(self.L)] = Z\n",
        "        #print(\"A shape is \")\n",
        "        #print(A.shape)\n",
        "        #print(\"Stored A,W,Z for index \"+str(self.L))\n",
        "        return A, store\n",
        "    \n",
        "    def forward2(self, X):\n",
        "        store = {}\n",
        " \n",
        "        A = X.T\n",
        "        for l in range(self.L - 1):\n",
        "            #print(\"W\" + str(l + 1)+\"shape is\")\n",
        "            #print(self.parameters[\"W\" + str(l + 1)].shape)\n",
        "            #print(\"A \"+\"shape is\")\n",
        "            #print(A.shape)\n",
        "            #print(\"W\" + str(l+1)+\" dot A shape \")\n",
        "            #print(self.parameters[\"W\" + str(l + 1)].dot(A).shape)\n",
        "            #print(\"b\" + str(l+1)+\" shape \")\n",
        "            #print(self.parameters[\"b\" + str(l+1)].shape)\n",
        "            \n",
        "            Z = self.parameters[\"W\" + str(l + 1)].dot(A) + self.parameters[\"b\" + str(l + 1)]\n",
        "            #print(\"Z shape \")\n",
        "            #print(Z.shape)\n",
        "            A = self.sigmoid(Z)\n",
        "            store[\"A\" + str(l + 1)] = A\n",
        "            store[\"W\" + str(l + 1)] = self.parameters[\"W\" + str(l + 1)]*np.ones(self.parameters[\"W\" + str(l + 1)].shape)*self.p\n",
        "            store[\"Z\" + str(l + 1)] = Z\n",
        "            #print(\"Stored A,W,Z for index \"+str(l + 1))\n",
        " \n",
        "        Z = self.parameters[\"W\" + str(self.L)].dot(A) + self.parameters[\"b\" + str(self.L)]\n",
        "        A = self.softmax(Z)\n",
        "        store[\"A\" + str(self.L)] = A\n",
        "        store[\"W\" + str(self.L)] = self.parameters[\"W\" + str(self.L)]\n",
        "        store[\"Z\" + str(self.L)] = Z\n",
        "        #print(\"A shape is \")\n",
        "        #print(A.shape)\n",
        "        #print(\"Stored A,W,Z for index \"+str(self.L))\n",
        "        return A, store\n",
        " \n",
        "    def sigmoid_derivative(self, Z):\n",
        "        s = 1 / (1 + np.exp(-Z))\n",
        "        return s * (1 - s)\n",
        "    \n",
        " \n",
        "    def backward(self, X, Y, lam,store):\n",
        " \n",
        "        derivatives = {}\n",
        " \n",
        "        store[\"A0\"] = X.T\n",
        " \n",
        "        A = store[\"A\" + str(self.L)]\n",
        "        dZ = A - Y.T\n",
        "        #print(\"self.n \",self.n)\n",
        "        dW = dZ.dot(store[\"A\" + str(self.L - 1)].T) / self.n +(lam/self.n)*store['W'+str(self.L)]\n",
        "        db = np.sum(dZ, axis=1, keepdims=True) / self.n\n",
        "        dAPrev = store[\"W\" + str(self.L)].T.dot(dZ)\n",
        "        \n",
        "        \n",
        "        #print(\"Shape of dZ is\",dZ.shape)\n",
        "        #print(\"Shape of dW is\",dW.shape)\n",
        "        #print(\"Shape of db is\",db.shape)\n",
        "        #print(\"Shape of dAPrev is\",dAPrev.shape)\n",
        " \n",
        "        derivatives[\"dW\" + str(self.L)] = dW\n",
        "        derivatives[\"db\" + str(self.L)] = db\n",
        "        #print(\"shape of derivative of dW \"+str(self.L)+\" is \",derivatives[\"dW\" + str(self.L)].shape)\n",
        "        #print(\"shape of derivative of db \"+str(self.L)+\" is \",derivatives[\"db\" + str(self.L)].shape)\n",
        "        for l in range(self.L - 1, 0, -1):\n",
        "            dZ = dAPrev * self.sigmoid_derivative(store[\"Z\" + str(l)])\n",
        "            dW = 1. / self.n * dZ.dot(store[\"A\" + str(l - 1)].T)+(lam/self.n)*store['W'+str(l)]\n",
        "            db = 1. / self.n * np.sum(dZ, axis=1, keepdims=True)\n",
        "            if l > 1:\n",
        "                dAPrev = store[\"W\" + str(l)].T.dot(dZ)\n",
        " \n",
        "            derivatives[\"dW\" + str(l)] = dW\n",
        "            derivatives[\"db\" + str(l)] = db\n",
        "            #print(\"shape of derivative of dW \"+str(l)+\" is \",derivatives[\"dW\" + str(l)].shape)\n",
        "            #print(\"shape of derivative of db \"+str(l)+\" is \",derivatives[\"db\" + str(l)].shape)\n",
        "        \n",
        "        #print(\"derivatives shape is \")\n",
        "        #print(derivatives)\n",
        "        return derivatives\n",
        " \n",
        "    def fit(self, X, Y, lam,learning_rate=0.01, n_iterations=2500):\n",
        "        np.random.seed(1)\n",
        " \n",
        "        self.n = X.shape[0]\n",
        " \n",
        "        self.layers_size.insert(0, X.shape[1])\n",
        " \n",
        "        self.initialize_parameters()\n",
        "        for loop in range(n_iterations):\n",
        "            A, store = self.forward(X)\n",
        "            cost = -np.mean(Y * np.log(A.T+ 1e-8))\n",
        "            #print(\"Y shape is \",Y.shape)\n",
        "            #print(\"np.log(A.T+ 1e-8) shape is \",np.log(A.T+ 1e-8).shape )\n",
        "            #print(\"(Y * np.log(A.T+ 1e-8)).shape is \",(Y * np.log(A.T+ 1e-8)).shape)\n",
        "            #print(\"cost\",cost)\n",
        "            derivatives = self.backward(X, Y, lam,store)\n",
        " \n",
        "            for l in range(1, self.L + 1):\n",
        "                self.parameters[\"W\" + str(l)] = self.parameters[\"W\" + str(l)] - learning_rate * derivatives[\n",
        "                    \"dW\" + str(l)]*self.droputMatrix(self.p,derivatives[\n",
        "                    \"dW\" + str(l)])\n",
        "                self.parameters[\"b\" + str(l)] = self.parameters[\"b\" + str(l)] - learning_rate * derivatives[\n",
        "                    \"db\" + str(l)]*self.droputMatrix(self.p,derivatives[\n",
        "                    \"db\" + str(l)])\n",
        " \n",
        "            if loop % 100 == 0:\n",
        "                print(\"Cost: \", cost, \"Train Accuracy:\", self.predict(X, Y))\n",
        " \n",
        "            if loop % 10 == 0:\n",
        "                self.costs.append(cost)\n",
        " \n",
        "    def predict(self, X, Y):\n",
        "        A, cache = self.forward2(X)\n",
        "        y_hat = np.argmax(A, axis=0)\n",
        "        Y = np.argmax(Y, axis=1)\n",
        "        accuracy = (y_hat == Y).mean()\n",
        "        return accuracy * 100\n",
        " \n",
        "    def plot_cost(self):\n",
        "        plt.figure()\n",
        "        plt.plot(np.arange(len(self.costs)), self.costs)\n",
        "        plt.xlabel(\"epochs\")\n",
        "        plt.ylabel(\"cost\")\n",
        "        plt.show()\n",
        "        \n",
        "    def droputMatrix(self,p,Mat):\n",
        "        noOfOnes=int(Mat.shape[0]*Mat.shape[1]*p)\n",
        "        noOfZeros=Mat.shape[0]*Mat.shape[1]-noOfOnes\n",
        "        ones=np.ones(noOfOnes)\n",
        "        zeroes=np.zeros(noOfZeros)\n",
        "        total=np.concatenate((ones,zeroes))\n",
        "        np.random.shuffle(total)\n",
        "        total=total.reshape((Mat.shape[0],Mat.shape[1]))\n",
        "        return total\n",
        "    \n",
        "        \n",
        "        \n",
        " \n",
        " \n",
        "def pre_process_data(train_x, train_y, test_x, test_y):\n",
        "    # Normalize\n",
        "    train_x = train_x / 255.\n",
        "    test_x = test_x / 255.\n",
        " \n",
        "    enc = OneHotEncoder(sparse=False, categories='auto')\n",
        "    train_y = enc.fit_transform(train_y.reshape(len(train_y), -1))\n",
        " \n",
        "    test_y = enc.transform(test_y.reshape(len(test_y), -1))\n",
        " \n",
        "    return train_x.reshape(len(train_x),28*28), train_y, test_x.reshape(len(test_x),28*28), test_y\n",
        " \n",
        " \n",
        "if __name__ == '__main__':\n",
        "    \n",
        " \n",
        "    train_x, train_y, test_x, test_y = pre_process_data(Xtrain,ytrain,Xtest,ytest)\n",
        "    #print(train_y)\n",
        "    #print(\"train_x's shape: \" + str(train_x.shape))\n",
        "    #print(\"test_x's shape: \" + str(test_x.shape))\n",
        "    #print(\"train_y's shape: \" + str(train_y.shape))\n",
        "    #print(\"test_y's shape: \" + str(test_y.shape))\n",
        " \n",
        "    layers_dims = [50, 25, 2]\n",
        " \n",
        "    ann = ANN(layers_dims,0.4)\n",
        "    ann.fit(train_x, train_y, lam=0,learning_rate=0.1, n_iterations=10000)\n",
        "    print(\"Train Accuracy:\", ann.predict(train_x, train_y))\n",
        "    print(\"Test Accuracy:\", ann.predict(test_x, test_y))\n",
        "    ann.plot_cost()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jx2-4y_AoY4i"
      },
      "outputs": [],
      "source": [
        "#CNN for pneumoniamnist\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
        "\n",
        "# Commented out IPython magic to ensure Python compatibility.\n",
        "import matplotlib.pyplot as plt  #for visualization\n",
        "# %matplotlib inline\n",
        "\n",
        "data = np.load(\"../input/pneumoniamnist/pneumoniamnist.npz\" )\n",
        "train_dataset = data[\"train_images\"]\n",
        "train_labels = data[\"train_labels\"]\n",
        "\n",
        "valid_dataset = data[\"val_images\"]\n",
        "valid_labels = data[\"val_labels\"]\n",
        "\n",
        "test_dataset = data[\"test_images\"]\n",
        "test_labels = data[\"test_labels\"]\n",
        "\n",
        "print(train_dataset.shape)\n",
        "print(train_labels.shape)\n",
        "print(valid_dataset.shape)\n",
        "print(valid_labels.shape)\n",
        "print(test_dataset.shape)\n",
        "print(test_labels.shape)\n",
        "\n",
        "#Normalize the data\n",
        "train_dataset = (train_dataset-127)/255\n",
        "valid_dataset = (valid_dataset-127)/255\n",
        "test_dataset = (test_dataset-127)/255\n",
        "\n",
        "#One Hot Encoding the label\n",
        "neural_train_labels = np.zeros((train_labels.shape[0],2))\n",
        "neural_valid_labels = np.zeros((valid_labels.shape[0],2))\n",
        "neural_test_labels = np.zeros((test_labels.shape[0],2))\n",
        "\n",
        "for i,value in enumerate(train_labels):\n",
        "    neural_train_labels[i,value] = 1\n",
        "\n",
        "for i,value in enumerate(valid_labels):\n",
        "    neural_valid_labels[i,value] = 1\n",
        "\n",
        "for i,value in enumerate(test_labels):\n",
        "    neural_test_labels[i,value] = 1\n",
        "\n",
        "print(\"Train labels = {}\".format(neural_train_labels.shape))\n",
        "print(\"Valid labels = {}\".format(neural_valid_labels.shape))\n",
        "print(\"Test labels  = {}\".format(neural_test_labels.shape))\n",
        "\n",
        "image_size = 28\n",
        "num_labels = 2\n",
        "num_channels = 1 # grayscale\n",
        "\n",
        "def reformat(dataset):\n",
        "    dataset = dataset.reshape(-1,image_size,image_size,num_channels).astype(np.float32)\n",
        "    return dataset\n",
        "\n",
        "train_dataset = reformat(train_dataset)\n",
        "valid_dataset = reformat(valid_dataset)\n",
        "test_dataset = reformat(test_dataset)\n",
        "\n",
        "print('Training set   = {}'.format(train_dataset.shape, neural_train_labels.shape))\n",
        "print('Validation set = {}'.format(valid_dataset.shape, neural_valid_labels.shape))\n",
        "print('Test set       = {}'.format(test_dataset.shape, neural_test_labels.shape))\n",
        "\n",
        "##HYPER_PARAMETERS\n",
        "#pixels\n",
        "image_size = 28\n",
        "\n",
        "#greyscale\n",
        "num_channels = 1\n",
        "\n",
        "#patch size\n",
        "patch_size = 3\n",
        "\n",
        "#depth\n",
        "depth = 4\n",
        "\n",
        "#hidden layers\n",
        "hidden1 = 256\n",
        "\n",
        "#hyperparameters\n",
        "learning_rate = 0.1\n",
        "\n",
        "#regularization\n",
        "beta = 0.01\n",
        "\n",
        "#target_labels\n",
        "num_classes = 2\n",
        "\n",
        "#Activation layers used in between and final\n",
        "\n",
        "#sigmoid\n",
        "def sigmoid(X):\n",
        "    return 1/(1+np.exp(-1*X))\n",
        "\n",
        "#sigmoid_derivative\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1-x)\n",
        "\n",
        "#Relu activation function\n",
        "def relu(x):\n",
        "    return x * (x > 0)\n",
        "\n",
        "#relu_derivative\n",
        "def relu_derivative(x):\n",
        "    return 1. * (x > 0)\n",
        "\n",
        "#softmax\n",
        "def softmax(X):\n",
        "    exp_X = np.exp(X)\n",
        "    sum_exp_X = np.sum(exp_X,1).reshape(-1,1)  #col-wise sum\n",
        "    exp_X = exp_X/sum_exp_X\n",
        "    return exp_X\n",
        "\n",
        "def initialize_parameters():\n",
        "    #initialize weights values with 0 mean and 0.5 standard deviation.\n",
        "    mean = 0\n",
        "    std = 0.5\n",
        "    \n",
        "    #conv layer weights\n",
        "    conv_layer1_weights = np.random.normal(mean,std,(patch_size,patch_size,num_channels,depth))\n",
        "    conv_layer1_biases = np.zeros([1,depth])\n",
        "    conv_layer2_weights = np.random.normal(mean,std,(patch_size,patch_size,depth,depth*4))\n",
        "    conv_layer2_biases = np.zeros([1,depth*4])\n",
        "    \n",
        "    #fully-connected weights\n",
        "    full_layer1_weights = np.random.normal(mean,std,(((image_size//4-1) * (image_size//4-1) * depth * 4),hidden1))\n",
        "    full_layer1_biases = np.zeros([hidden1])\n",
        "    full_layer2_weights = np.random.normal(mean,std,(hidden1,num_classes))\n",
        "    full_layer2_biases = np.zeros([num_classes])\n",
        "    \n",
        "    parameters = dict()\n",
        "    parameters['cw1'] = conv_layer1_weights\n",
        "    parameters['cb1'] = conv_layer1_biases\n",
        "    parameters['cw2'] = conv_layer2_weights\n",
        "    parameters['cb2'] = conv_layer2_biases\n",
        "    parameters['fw1'] = full_layer1_weights\n",
        "    parameters['fb1'] = full_layer1_biases\n",
        "    parameters['fw2'] = full_layer2_weights\n",
        "    parameters['fb2'] = full_layer2_biases\n",
        "    \n",
        "    return parameters\n",
        "\n",
        "#Convolution operation i.e multiplying Image with the weights.\n",
        "#stride hardcoded = 2\n",
        "#padding  = 0\n",
        "def conv_multiply(image,weights):\n",
        "    hsize = (image.shape[0]-weights.shape[0])//2 + 1\n",
        "    vsize = (image.shape[1]-weights.shape[1])//2 + 1\n",
        "    logits = np.zeros([hsize,vsize,weights.shape[3]])\n",
        "    for d in range(weights.shape[3]):\n",
        "        row = 0\n",
        "        for rpos in range(0,(image.shape[0]-patch_size+1),2):\n",
        "            col=0\n",
        "            for cpos in range(0,(image.shape[1]-patch_size+1),2):\n",
        "                logits[row,col,d] = np.sum(np.multiply(image[rpos:rpos+patch_size, cpos:cpos+patch_size, :],weights[:,:,:,d]))\n",
        "                col += 1\n",
        "            row+=1\n",
        "    return logits\n",
        "\n",
        "#stride = 1\n",
        "def conv_multiply_stride1(image,weights):\n",
        "    hsize = (image.shape[0]-weights.shape[0])//1 + 1\n",
        "    vsize = (image.shape[1]-weights.shape[1])//1 + 1\n",
        "    logits = np.zeros([hsize,vsize,weights.shape[3]])\n",
        "    for d in range(weights.shape[3]):\n",
        "        row = 0\n",
        "        for rpos in range(0,(image.shape[0]-patch_size+1),1):\n",
        "            col=0\n",
        "            for cpos in range(0,(image.shape[1]-patch_size+1),1):\n",
        "                logits[row,col,d] = np.sum(np.multiply(image[rpos:rpos+patch_size, cpos:cpos+patch_size, :],weights[:,:,:,d]))\n",
        "                col += 1\n",
        "            row+=1\n",
        "    return logits\n",
        "\n",
        "#FORWARD PROPAGATION\n",
        "def forward_propagation(dataset,parameters):\n",
        "    #convolution layers activations\n",
        "    m = dataset.shape[0]\n",
        "    \n",
        "    #get the parameters\n",
        "    cw1 = parameters['cw1']\n",
        "    cb1 = parameters['cb1']\n",
        "    cw2 = parameters['cw2']\n",
        "    cb2 = parameters['cb2']\n",
        "    \n",
        "    fw1 = parameters['fw1']\n",
        "    fb1 = parameters['fb1']\n",
        "    fw2 = parameters['fw2']\n",
        "    fb2 = parameters['fb2']\n",
        "    \n",
        "    #to store the intermediate activations for backward propagation\n",
        "    cache = dict()\n",
        "    \n",
        "    conv_activation1 = list()\n",
        "    conv_activation2 = list()\n",
        "    \n",
        "    #image by image convolutional forward propagation\n",
        "    for i in range(m):\n",
        "        image = dataset[i]\n",
        "        logits = conv_multiply(image,cw1) + cb1\n",
        "        ca1 = sigmoid(logits)\n",
        "        ca2 = sigmoid(conv_multiply(ca1,cw2) + cb2).reshape((image_size // 4 -1) * (image_size // 4 -1) * depth * 4)\n",
        "        \n",
        "        conv_activation1.append(ca1)\n",
        "        conv_activation2.append(ca2)\n",
        "        \n",
        "    #convert into numpy array\n",
        "    conv_activation1 = np.array(conv_activation1).reshape(m,image_size // 2 -1, image_size // 2 -1, depth)\n",
        "    conv_activation2 = np.array(conv_activation2).reshape(m,image_size // 4 -1, image_size // 4 -1, depth * 4)\n",
        "        \n",
        "    #expand the conv_activation2 into (m,num_features) \n",
        "    #num_features = (image_size // 4 * image_size // 4 * depth * 4)\n",
        "    temp_activation = np.array(conv_activation2).reshape(m,(image_size // 4 -1) * (image_size // 4-1) * depth * 4)\n",
        "    \n",
        "    #fully connected layers activations\n",
        "    full_activation1 = np.matmul(temp_activation,fw1) + fb1\n",
        "    full_activation1 = sigmoid(full_activation1)\n",
        "    full_activation2 = np.matmul(full_activation1,fw2) + fb2\n",
        "    output = softmax(full_activation2)\n",
        "    \n",
        "    cache['ca1'] = conv_activation1\n",
        "    cache['ca2'] = conv_activation2\n",
        "    cache['fa1'] = full_activation1\n",
        "    cache['output'] = output\n",
        "    return cache,output\n",
        "\n",
        "#calculate conv deltas or errors only for one example\n",
        "def conv_delta(next_error,weights):\n",
        "    delta = np.zeros([next_error.shape[0]*2+1,next_error.shape[1]*2+1,next_error.shape[2]//4])\n",
        "    for d in range(weights.shape[3]):\n",
        "        row = 0\n",
        "        for rpos in range(0,delta.shape[0]-patch_size+1,2):\n",
        "            col=0\n",
        "            for cpos in range(0,delta.shape[2]-patch_size+1,2):\n",
        "                delta[rpos:rpos+patch_size,cpos:cpos+patch_size,:] += weights[:,:,:,d]*next_error[row,col,d]\n",
        "                col+=1\n",
        "            row +=1\n",
        "    return delta\n",
        "\n",
        "#conv partial derivatives only for single example\n",
        "def conv_derivatives(delta,activation):\n",
        "    partial_derivatives = np.zeros([patch_size,patch_size,activation.shape[2],delta.shape[2]])\n",
        "    for d2 in range(0,partial_derivatives.shape[3]):\n",
        "        row=0\n",
        "        for rpos in range(0,activation.shape[0]-patch_size+1,2):\n",
        "            col = 0\n",
        "            for cpos in range(0,activation.shape[1]-patch_size+1,2):\n",
        "                partial_derivatives[:,:,:,d2] += np.multiply(activation[rpos:rpos+patch_size, cpos:cpos+patch_size, :],delta[row,col,d2])\n",
        "                col += 1\n",
        "            row += 1\n",
        "    return partial_derivatives\n",
        "\n",
        "def backward_propagation(dataset,labels,cache,parameters):\n",
        "    #get activations\n",
        "    output = cache['output']\n",
        "    fa1 = cache['fa1']\n",
        "    ca2 = cache['ca2']\n",
        "    ca1 = cache['ca1']\n",
        "    \n",
        "    temp_act = np.array(ca2).reshape(-1,(image_size // 4-1) * (image_size // 4 -1)* depth * 4)\n",
        "    \n",
        "    #get parameters\n",
        "    cw1 = parameters['cw1']\n",
        "    cw2 = parameters['cw2']\n",
        "    fw1 = parameters['fw1']\n",
        "    fw2 = parameters['fw2']\n",
        "    \n",
        "    \n",
        "    #cal errors fully connected\n",
        "    error_fa2 = output - labels\n",
        "    error_fa1 = np.matmul(error_fa2,fw2.T)\n",
        "    error_fa1 = np.multiply(error_fa1,sigmoid_derivative(fa1))\n",
        "    error_temp = np.matmul(error_fa1,fw1.T)\n",
        "    error_temp = np.multiply(error_temp,sigmoid_derivative(temp_act))\n",
        "    \n",
        "    m = dataset.shape[0]\n",
        "    \n",
        "    #cal errors conv layers\n",
        "    error_ca2 = np.array(error_temp).reshape(-1,image_size//4-1,image_size//4-1,depth*4)\n",
        "    error_ca1 = np.zeros(ca1.shape)\n",
        "    ## Image by Image error\n",
        "    for i in range(m):\n",
        "        error = conv_delta(error_ca2[i],cw2)\n",
        "        error = np.multiply(error,sigmoid_derivative(ca1[i]))\n",
        "        error_ca1 += error\n",
        "    \n",
        "    \n",
        "    #calculate partial derivatives\n",
        "    #fully connected layers\n",
        "    fd2 = (np.matmul(fa1.T,error_fa2) + beta*fw2)/m\n",
        "    fd1 = (np.matmul(temp_act.T,error_fa1) + beta*fw1)/m\n",
        "    \n",
        "    #conv layers\n",
        "    cd2 = np.zeros(cw2.shape)\n",
        "    cd1 = np.zeros(cw1.shape)\n",
        "    \n",
        "    ##Image by Image derivatives\n",
        "    for i in range(m):\n",
        "        cd2 = cd2 + conv_derivatives(error_ca2[i],ca1[i])\n",
        "        cd1 = cd1 + conv_derivatives(error_ca1[i],dataset[i])\n",
        "    cd2 = (cd2 + beta*cw2)/m\n",
        "    cd1 = (cd1 + beta*cw1)/m\n",
        "    \n",
        "    \n",
        "    #store the derivatives in dict\n",
        "    derivatives = dict()\n",
        "    \n",
        "    derivatives['cd1'] = cd1\n",
        "    derivatives['cd2'] = cd2\n",
        "    derivatives['fd1'] = fd1\n",
        "    derivatives['fd2'] = fd2\n",
        "    \n",
        "    return derivatives\n",
        "\n",
        "def update_parameters(derivatives,parameters):\n",
        "    #get parameters\n",
        "    cw1 = parameters['cw1']\n",
        "    cw2 = parameters['cw2']\n",
        "    fw1 = parameters['fw1']\n",
        "    fw2 = parameters['fw2']\n",
        "    \n",
        "    #get derivatives\n",
        "    cd1 = derivatives['cd1']\n",
        "    cd2 = derivatives['cd2']\n",
        "    fd1 = derivatives['fd1']\n",
        "    fd2 = derivatives['fd2']\n",
        "    \n",
        "    #update\n",
        "    cw1 = cw1 - learning_rate*cd1\n",
        "    cw2 = cw2 - learning_rate*cd2\n",
        "    fw1 = fw1 - learning_rate*fd1\n",
        "    fw2 = fw2 - learning_rate*fd2\n",
        "    \n",
        "    #update the dict\n",
        "    parameters['cw1'] = cw1\n",
        "    parameters['cw2'] = cw2\n",
        "    parameters['fw1'] = fw1\n",
        "    parameters['fw2'] = fw2\n",
        "    \n",
        "    return parameters\n",
        "\n",
        "def cal_loss_accuracy(true_labels,predictions,parameters):\n",
        "    #get parameters\n",
        "    cw1 = parameters['cw1']\n",
        "    cw2 = parameters['cw2']\n",
        "    fw1 = parameters['fw1']\n",
        "    fw2 = parameters['fw2']\n",
        "    \n",
        "    m = len(true_labels)\n",
        "    \n",
        "    #cal loss\n",
        "    loss = -1*(np.sum(np.multiply(np.log(predictions),true_labels),1) + np.sum(np.multiply(np.log(1-predictions),1-true_labels),1))\n",
        "    loss = np.sum(loss)\n",
        "    loss = loss + beta*(np.sum(cw1**2) + np.sum(cw2**2) + np.sum(fw1**2) + np.sum(fw2**2))\n",
        "    loss = loss/m\n",
        "    \n",
        "    #cal accuracy\n",
        "    accuracy = np.sum(np.argmax(true_labels,1)==np.argmax(predictions,1))/m\n",
        "    \n",
        "    return loss,accuracy\n",
        "\n",
        "#train function\n",
        "def train(train_dataset,train_labels,batch_size=32,iters=101,stride=2):\n",
        "    \n",
        "    #initialize the parameters\n",
        "    parameters = initialize_parameters()\n",
        "    \n",
        "    cw1 = parameters['cw1']\n",
        "    cb1 = parameters['cb1']\n",
        "    cw2 = parameters['cw2']\n",
        "    cb2 = parameters['cb2']\n",
        "    \n",
        "    fw1 = parameters['fw1']\n",
        "    fb1 = parameters['fb1']\n",
        "    fw2 = parameters['fw2']\n",
        "    fb2 = parameters['fb2']\n",
        "    \n",
        "    J = []  #store the loss o every batch\n",
        "    A = []  #store the accuracy of every batch\n",
        "    \n",
        "    \n",
        "    #training process.\n",
        "    for step in range(iters):\n",
        "        #get the batch data.\n",
        "        start = (step*batch_size)%(train_dataset.shape[0])\n",
        "        end = start + batch_size\n",
        "        \n",
        "        batch_dataset = train_dataset[start:end,:,:,:]\n",
        "        batch_labels = train_labels[start:end,:]\n",
        "        \n",
        "        #forward propagation\n",
        "        cache,output = forward_propagation(batch_dataset,parameters)\n",
        "        \n",
        "        #cal_loss and accuracy\n",
        "        loss,accuracy = cal_loss_accuracy(batch_labels,output,parameters)\n",
        "        \n",
        "        #calculate the derivatives\n",
        "        derivatives = backward_propagation(batch_dataset,batch_labels,cache,parameters)\n",
        "        \n",
        "        #update the parameters\n",
        "        parameters = update_parameters(derivatives,parameters)\n",
        "        \n",
        "        #append the loss and accuracy of every batch\n",
        "        J.append(loss)\n",
        "        A.append(accuracy)\n",
        "        \n",
        "        #print loss and accuracy of the batch dataset.\n",
        "        if(step%100==0):\n",
        "            print('Step : %d'%step)\n",
        "            print('Loss : %f'%loss)\n",
        "            print('Accuracy : %f%%'%(round(accuracy*100,2)))\n",
        "            \n",
        "    return J,A,parameters\n",
        "\n",
        "#TRAINING\n",
        "J,A,parameters = train(train_dataset,neural_train_labels,iters=1000)\n",
        "\n",
        "#for training set\n",
        "_,train_pred = forward_propagation(train_dataset,parameters)\n",
        "_,train_accuracy = cal_loss_accuracy(neural_train_labels,train_pred,parameters)\n",
        "\n",
        "#for valid set\n",
        "_,valid_pred = forward_propagation(valid_dataset,parameters)\n",
        "_,valid_accuracy = cal_loss_accuracy(neural_valid_labels,valid_pred,parameters)\n",
        "\n",
        "#for test set\n",
        "_,test_pred = forward_propagation(test_dataset,parameters)\n",
        "_,test_accuracy = cal_loss_accuracy(neural_test_labels,test_pred,parameters)\n",
        "\n",
        "print('Accuracy of Train Set = {}'.format(round(train_accuracy*100,2)))\n",
        "print('Accuracy of Valid Set = {}'.format(round(valid_accuracy*100,2)))\n",
        "print('Accuracy of Test  Set = {}'.format(round(test_accuracy*100,2)))\n",
        "\n",
        "import sklearn.metrics\n",
        "F1= sklearn.metrics.f1_score(test_labels, np.argmax(test_pred,1))\n",
        "AUC = sklearn.metrics.roc_auc_score(test_labels,np.argmax(test_pred,1))\n",
        "\n",
        "print('F1 score : {}'.format(F1))\n",
        "print('AUC : {}'.format(AUC))\n",
        "\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "cnfn_mat=confusion_matrix(test_labels,np.argmax(test_pred,1))\n",
        "sns.heatmap(cnfn_mat,annot=cnfn_mat,fmt='',xticklabels=['0','1'],yticklabels=['0','1'])\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "avg_loss = list()\n",
        "avg_acc = list()\n",
        "i = 0\n",
        "while(i<len(J)):\n",
        "    avg_loss.append(np.mean(J[i:i+30]))\n",
        "    avg_acc.append(np.mean(A[i:i+30]))\n",
        "    i += 30\n",
        "\n",
        "plt.plot(list(range(len(avg_loss))),avg_loss)\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"Loss (Avg of 30 batches)\")\n",
        "plt.title(\"Loss Graph\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot(list(range(len(avg_acc))),avg_acc)\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"Accuracy (Avg of 30 batches)\")\n",
        "plt.title(\"Accuracy Graph\")\n",
        "plt.show()\n",
        "\n",
        "index = 10\n",
        "test_image = test_dataset[index].reshape(1,28,28,1)\n",
        "plt.imshow(test_image[0,:,:,0],cmap='gray')\n",
        "print(\"Image\")\n",
        "plt.show()\n",
        "print(\"True_Label = {}\".format(np.argmax(neural_test_labels[index])))\n",
        "print(\"Pred_Label = {}\".format(np.argmax(test_pred[index])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gxmt21F4ohZv"
      },
      "outputs": [],
      "source": [
        "#CNN from scratch for bloodmnist\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
        "\n",
        "# Commented out IPython magic to ensure Python compatibility.\n",
        "import matplotlib.pyplot as plt  #for visualization\n",
        "# %matplotlib inline\n",
        "\n",
        "data = np.load(\"../input/bloodmnist/bloodmnist.npz\" )\n",
        "train_dataset = data[\"train_images\"]\n",
        "train_labels = data[\"train_labels\"]\n",
        "\n",
        "valid_dataset = data[\"val_images\"]\n",
        "valid_labels = data[\"val_labels\"]\n",
        "\n",
        "test_dataset = data[\"test_images\"]\n",
        "test_labels = data[\"test_labels\"]\n",
        "\n",
        "print(train_dataset.shape)\n",
        "print(train_labels.shape)\n",
        "print(valid_dataset.shape)\n",
        "print(valid_labels.shape)\n",
        "print(test_dataset.shape)\n",
        "print(test_labels.shape)\n",
        "\n",
        "#Normalize the data\n",
        "train_dataset = (train_dataset-127)/255\n",
        "valid_dataset = (valid_dataset-127)/255\n",
        "test_dataset = (test_dataset-127)/255\n",
        "\n",
        "#One Hot Encoding the label\n",
        "neural_train_labels = np.zeros((train_labels.shape[0],8))\n",
        "neural_valid_labels = np.zeros((valid_labels.shape[0],8))\n",
        "neural_test_labels = np.zeros((test_labels.shape[0],8))\n",
        "\n",
        "for i,value in enumerate(train_labels):\n",
        "    neural_train_labels[i,value] = 1\n",
        "\n",
        "for i,value in enumerate(valid_labels):\n",
        "    neural_valid_labels[i,value] = 1\n",
        "\n",
        "for i,value in enumerate(test_labels):\n",
        "    neural_test_labels[i,value] = 1\n",
        "\n",
        "print(\"Train labels = {}\".format(neural_train_labels.shape))\n",
        "print(\"Valid labels = {}\".format(neural_valid_labels.shape))\n",
        "print(\"Test labels  = {}\".format(neural_test_labels.shape))\n",
        "\n",
        "image_size = 28\n",
        "num_labels = 8\n",
        "num_channels = 3 # color\n",
        "\n",
        "def reformat(dataset):\n",
        "    dataset = dataset.reshape(-1,image_size,image_size,num_channels).astype(np.float32)\n",
        "    return dataset\n",
        "\n",
        "train_dataset = reformat(train_dataset)\n",
        "valid_dataset = reformat(valid_dataset)\n",
        "test_dataset = reformat(test_dataset)\n",
        "\n",
        "print('Training set   = {}'.format(train_dataset.shape, neural_train_labels.shape))\n",
        "print('Validation set = {}'.format(valid_dataset.shape, neural_valid_labels.shape))\n",
        "print('Test set       = {}'.format(test_dataset.shape, neural_test_labels.shape))\n",
        "\n",
        "##HYPER_PARAMETERS\n",
        "#pixels\n",
        "image_size = 28\n",
        "\n",
        "#color\n",
        "num_channels = 3\n",
        "\n",
        "#patch size\n",
        "patch_size = 3\n",
        "\n",
        "#depth\n",
        "depth = 4\n",
        "\n",
        "#hidden layers\n",
        "hidden1 = 512\n",
        "\n",
        "#hyperparameters\n",
        "learning_rate = 0.05\n",
        "\n",
        "#regularization\n",
        "beta = 0.0001\n",
        "\n",
        "#target_labels\n",
        "num_classes = 8\n",
        "\n",
        "#Activation layers used in between and final\n",
        "\n",
        "#sigmoid\n",
        "def sigmoid(X):\n",
        "    return 1/(1+np.exp(-1*X))\n",
        "\n",
        "#sigmoid_derivative\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1-x)\n",
        "\n",
        "#Relu activation function\n",
        "def relu(x):\n",
        "    return x * (x > 0)\n",
        "\n",
        "#relu_derivative\n",
        "def relu_derivative(x):\n",
        "    return 1. * (x > 0)\n",
        "\n",
        "#softmax\n",
        "def softmax(X):\n",
        "    exp_X = np.exp(X)\n",
        "    sum_exp_X = np.sum(exp_X,1).reshape(-1,1)  #col-wise sum\n",
        "    exp_X = exp_X/sum_exp_X\n",
        "    return exp_X\n",
        "\n",
        "def initialize_parameters():\n",
        "    #initialize weights values with 0 mean and 0.5 standard deviation.\n",
        "    mean = 0\n",
        "    std = 0.5\n",
        "    \n",
        "    #conv layer weights\n",
        "    conv_layer1_weights = np.random.normal(mean,std,(patch_size,patch_size,num_channels,depth))\n",
        "    conv_layer1_biases = np.zeros([1,depth])\n",
        "    conv_layer2_weights = np.random.normal(mean,std,(patch_size,patch_size,depth,depth*4))\n",
        "    conv_layer2_biases = np.zeros([1,depth*4])\n",
        "    \n",
        "    #fully-connected weights\n",
        "    full_layer1_weights = np.random.normal(mean,std,(((image_size//4-1) * (image_size//4-1) * depth * 4),hidden1))\n",
        "    full_layer1_biases = np.zeros([hidden1])\n",
        "    full_layer2_weights = np.random.normal(mean,std,(hidden1,num_classes))\n",
        "    full_layer2_biases = np.zeros([num_classes])\n",
        "    \n",
        "    parameters = dict()\n",
        "    parameters['cw1'] = conv_layer1_weights\n",
        "    parameters['cb1'] = conv_layer1_biases\n",
        "    parameters['cw2'] = conv_layer2_weights\n",
        "    parameters['cb2'] = conv_layer2_biases\n",
        "    parameters['fw1'] = full_layer1_weights\n",
        "    parameters['fb1'] = full_layer1_biases\n",
        "    parameters['fw2'] = full_layer2_weights\n",
        "    parameters['fb2'] = full_layer2_biases\n",
        "    \n",
        "    return parameters\n",
        "\n",
        "#Convolution operation i.e multiplying Image with the weights.\n",
        "#stride hardcoded = 2\n",
        "#padding  = 0\n",
        "def conv_multiply(image,weights):\n",
        "    hsize = (image.shape[0]-weights.shape[0])//2 + 1\n",
        "    vsize = (image.shape[1]-weights.shape[1])//2 + 1\n",
        "    logits = np.zeros([hsize,vsize,weights.shape[3]])\n",
        "    for d in range(weights.shape[3]):\n",
        "        row = 0\n",
        "        for rpos in range(0,(image.shape[0]-patch_size+1),2):\n",
        "            col=0\n",
        "            for cpos in range(0,(image.shape[1]-patch_size+1),2):\n",
        "                logits[row,col,d] = np.sum(np.multiply(image[rpos:rpos+patch_size, cpos:cpos+patch_size, :],weights[:,:,:,d]))\n",
        "                col += 1\n",
        "            row+=1\n",
        "    return logits\n",
        "\n",
        "#stride = 1\n",
        "def conv_multiply_stride1(image,weights):\n",
        "    hsize = (image.shape[0]-weights.shape[0])//1 + 1\n",
        "    vsize = (image.shape[1]-weights.shape[1])//1 + 1\n",
        "    logits = np.zeros([hsize,vsize,weights.shape[3]])\n",
        "    for d in range(weights.shape[3]):\n",
        "        row = 0\n",
        "        for rpos in range(0,(image.shape[0]-patch_size+1),1):\n",
        "            col=0\n",
        "            for cpos in range(0,(image.shape[1]-patch_size+1),1):\n",
        "                logits[row,col,d] = np.sum(np.multiply(image[rpos:rpos+patch_size, cpos:cpos+patch_size, :],weights[:,:,:,d]))\n",
        "                col += 1\n",
        "            row+=1\n",
        "    return logits\n",
        "\n",
        "#FORWARD PROPAGATION\n",
        "def forward_propagation(dataset,parameters):\n",
        "    #convolution layers activations\n",
        "    m = dataset.shape[0]\n",
        "    \n",
        "    #get the parameters\n",
        "    cw1 = parameters['cw1']\n",
        "    cb1 = parameters['cb1']\n",
        "    cw2 = parameters['cw2']\n",
        "    cb2 = parameters['cb2']\n",
        "    \n",
        "    fw1 = parameters['fw1']\n",
        "    fb1 = parameters['fb1']\n",
        "    fw2 = parameters['fw2']\n",
        "    fb2 = parameters['fb2']\n",
        "    \n",
        "    #to store the intermediate activations for backward propagation\n",
        "    cache = dict()\n",
        "    \n",
        "    conv_activation1 = list()\n",
        "    conv_activation2 = list()\n",
        "    \n",
        "    #image by image convolutional forward propagation\n",
        "    for i in range(m):\n",
        "        image = dataset[i]\n",
        "        logits = conv_multiply(image,cw1) + cb1\n",
        "        ca1 = sigmoid(logits)\n",
        "        ca2 = sigmoid(conv_multiply(ca1,cw2) + cb2).reshape((image_size // 4 -1) * (image_size // 4 -1) * depth * 4)\n",
        "        \n",
        "        conv_activation1.append(ca1)\n",
        "        conv_activation2.append(ca2)\n",
        "        \n",
        "    #convert into numpy array\n",
        "    conv_activation1 = np.array(conv_activation1).reshape(m,image_size // 2 -1, image_size // 2 -1, depth)\n",
        "    conv_activation2 = np.array(conv_activation2).reshape(m,image_size // 4 -1, image_size // 4 -1, depth * 4)\n",
        "        \n",
        "    #expand the conv_activation2 into (m,num_features) \n",
        "    #num_features = (image_size // 4 * image_size // 4 * depth * 4)\n",
        "    temp_activation = np.array(conv_activation2).reshape(m,(image_size // 4 -1) * (image_size // 4-1) * depth * 4)\n",
        "    \n",
        "    #fully connected layers activations\n",
        "    full_activation1 = np.matmul(temp_activation,fw1) + fb1\n",
        "    full_activation1 = sigmoid(full_activation1)\n",
        "    full_activation2 = np.matmul(full_activation1,fw2) + fb2\n",
        "    output = softmax(full_activation2)\n",
        "    \n",
        "    cache['ca1'] = conv_activation1\n",
        "    cache['ca2'] = conv_activation2\n",
        "    cache['fa1'] = full_activation1\n",
        "    cache['output'] = output\n",
        "    return cache,output\n",
        "\n",
        "#calculate conv deltas or errors only for one example\n",
        "def conv_delta(next_error,weights):\n",
        "    delta = np.zeros([next_error.shape[0]*2+1,next_error.shape[1]*2+1,next_error.shape[2]//4])\n",
        "    for d in range(weights.shape[3]):\n",
        "        row = 0\n",
        "        for rpos in range(0,delta.shape[0]-patch_size+1,2):\n",
        "            col=0\n",
        "            for cpos in range(0,delta.shape[2]-patch_size+1,2):\n",
        "                delta[rpos:rpos+patch_size,cpos:cpos+patch_size,:] += weights[:,:,:,d]*next_error[row,col,d]\n",
        "                col+=1\n",
        "            row +=1\n",
        "    return delta\n",
        "\n",
        "#conv partial derivatives only for single example\n",
        "def conv_derivatives(delta,activation):\n",
        "    partial_derivatives = np.zeros([patch_size,patch_size,activation.shape[2],delta.shape[2]])\n",
        "    for d2 in range(0,partial_derivatives.shape[3]):\n",
        "        row=0\n",
        "        for rpos in range(0,activation.shape[0]-patch_size+1,2):\n",
        "            col = 0\n",
        "            for cpos in range(0,activation.shape[1]-patch_size+1,2):\n",
        "                partial_derivatives[:,:,:,d2] += np.multiply(activation[rpos:rpos+patch_size, cpos:cpos+patch_size, :],delta[row,col,d2])\n",
        "                col += 1\n",
        "            row += 1\n",
        "    return partial_derivatives\n",
        "\n",
        "def backward_propagation(dataset,labels,cache,parameters):\n",
        "    #get activations\n",
        "    output = cache['output']\n",
        "    fa1 = cache['fa1']\n",
        "    ca2 = cache['ca2']\n",
        "    ca1 = cache['ca1']\n",
        "    \n",
        "    temp_act = np.array(ca2).reshape(-1,(image_size // 4-1) * (image_size // 4 -1)* depth * 4)\n",
        "    \n",
        "    #get parameters\n",
        "    cw1 = parameters['cw1']\n",
        "    cw2 = parameters['cw2']\n",
        "    fw1 = parameters['fw1']\n",
        "    fw2 = parameters['fw2']\n",
        "    \n",
        "    \n",
        "    #cal errors fully connected\n",
        "    error_fa2 = output - labels\n",
        "    error_fa1 = np.matmul(error_fa2,fw2.T)\n",
        "    error_fa1 = np.multiply(error_fa1,sigmoid_derivative(fa1))\n",
        "    error_temp = np.matmul(error_fa1,fw1.T)\n",
        "    error_temp = np.multiply(error_temp,sigmoid_derivative(temp_act))\n",
        "    \n",
        "    m = dataset.shape[0]\n",
        "    \n",
        "    #cal errors conv layers\n",
        "    error_ca2 = np.array(error_temp).reshape(-1,image_size//4-1,image_size//4-1,depth*4)\n",
        "    error_ca1 = np.zeros(ca1.shape)\n",
        "    ## Image by Image error\n",
        "    for i in range(m):\n",
        "        error = conv_delta(error_ca2[i],cw2)\n",
        "        error = np.multiply(error,sigmoid_derivative(ca1[i]))\n",
        "        error_ca1 += error\n",
        "    \n",
        "    \n",
        "    #calculate partial derivatives\n",
        "    #fully connected layers\n",
        "    fd2 = (np.matmul(fa1.T,error_fa2) + beta*fw2)/m\n",
        "    fd1 = (np.matmul(temp_act.T,error_fa1) + beta*fw1)/m\n",
        "    \n",
        "    #conv layers\n",
        "    cd2 = np.zeros(cw2.shape)\n",
        "    cd1 = np.zeros(cw1.shape)\n",
        "    \n",
        "    ##Image by Image derivatives\n",
        "    for i in range(m):\n",
        "        cd2 = cd2 + conv_derivatives(error_ca2[i],ca1[i])\n",
        "        cd1 = cd1 + conv_derivatives(error_ca1[i],dataset[i])\n",
        "    cd2 = (cd2 + beta*cw2)/m\n",
        "    cd1 = (cd1 + beta*cw1)/m\n",
        "    \n",
        "    \n",
        "    #store the derivatives in dict\n",
        "    derivatives = dict()\n",
        "    \n",
        "    derivatives['cd1'] = cd1\n",
        "    derivatives['cd2'] = cd2\n",
        "    derivatives['fd1'] = fd1\n",
        "    derivatives['fd2'] = fd2\n",
        "    \n",
        "    return derivatives\n",
        "\n",
        "def update_parameters(derivatives,parameters):\n",
        "    #get parameters\n",
        "    cw1 = parameters['cw1']\n",
        "    cw2 = parameters['cw2']\n",
        "    fw1 = parameters['fw1']\n",
        "    fw2 = parameters['fw2']\n",
        "    \n",
        "    #get derivatives\n",
        "    cd1 = derivatives['cd1']\n",
        "    cd2 = derivatives['cd2']\n",
        "    fd1 = derivatives['fd1']\n",
        "    fd2 = derivatives['fd2']\n",
        "    \n",
        "    #update\n",
        "    cw1 = cw1 - learning_rate*cd1\n",
        "    cw2 = cw2 - learning_rate*cd2\n",
        "    fw1 = fw1 - learning_rate*fd1\n",
        "    fw2 = fw2 - learning_rate*fd2\n",
        "    \n",
        "    #update the dict\n",
        "    parameters['cw1'] = cw1\n",
        "    parameters['cw2'] = cw2\n",
        "    parameters['fw1'] = fw1\n",
        "    parameters['fw2'] = fw2\n",
        "    \n",
        "    return parameters\n",
        "\n",
        "def cal_loss_accuracy(true_labels,predictions,parameters):\n",
        "    #get parameters\n",
        "    cw1 = parameters['cw1']\n",
        "    cw2 = parameters['cw2']\n",
        "    fw1 = parameters['fw1']\n",
        "    fw2 = parameters['fw2']\n",
        "    \n",
        "    m = len(true_labels)\n",
        "    \n",
        "    #cal loss\n",
        "    loss = -1*(np.sum(np.multiply(np.log(predictions),true_labels),1) + np.sum(np.multiply(np.log(1-predictions),1-true_labels),1))\n",
        "    loss = np.sum(loss)\n",
        "    loss = loss + beta*(np.sum(cw1**2) + np.sum(cw2**2) + np.sum(fw1**2) + np.sum(fw2**2))\n",
        "    loss = loss/m\n",
        "    \n",
        "    #cal accuracy\n",
        "    accuracy = np.sum(np.argmax(true_labels,1)==np.argmax(predictions,1))/m\n",
        "    \n",
        "    return loss,accuracy\n",
        "\n",
        "#train function\n",
        "def train(train_dataset,train_labels,batch_size=16,iters=101,stride=2):\n",
        "    \n",
        "    #initialize the parameters\n",
        "    parameters = initialize_parameters()\n",
        "    \n",
        "    cw1 = parameters['cw1']\n",
        "    cb1 = parameters['cb1']\n",
        "    cw2 = parameters['cw2']\n",
        "    cb2 = parameters['cb2']\n",
        "    \n",
        "    fw1 = parameters['fw1']\n",
        "    fb1 = parameters['fb1']\n",
        "    fw2 = parameters['fw2']\n",
        "    fb2 = parameters['fb2']\n",
        "    \n",
        "    J = []  #store the loss o every batch\n",
        "    A = []  #store the accuracy of every batch\n",
        "    \n",
        "    \n",
        "    #training process.\n",
        "    for step in range(iters):\n",
        "        #get the batch data.\n",
        "        start = (step*batch_size)%(train_dataset.shape[0])\n",
        "        end = start + batch_size\n",
        "        \n",
        "        batch_dataset = train_dataset[start:end,:,:,:]\n",
        "        batch_labels = train_labels[start:end,:]\n",
        "        \n",
        "        #forward propagation\n",
        "        cache,output = forward_propagation(batch_dataset,parameters)\n",
        "        \n",
        "        #cal_loss and accuracy\n",
        "        loss,accuracy = cal_loss_accuracy(batch_labels,output,parameters)\n",
        "        \n",
        "        #calculate the derivatives\n",
        "        derivatives = backward_propagation(batch_dataset,batch_labels,cache,parameters)\n",
        "        \n",
        "        #update the parameters\n",
        "        parameters = update_parameters(derivatives,parameters)\n",
        "        \n",
        "        #append the loss and accuracy of every batch\n",
        "        J.append(loss)\n",
        "        A.append(accuracy)\n",
        "        \n",
        "        #print loss and accuracy of the batch dataset.\n",
        "        if(step%100==0):\n",
        "            print('Step : %d'%step)\n",
        "            print('Loss : %f'%loss)\n",
        "            print('Accuracy : %f%%'%(round(accuracy*100,2)))\n",
        "            \n",
        "    return J,A,parameters\n",
        "\n",
        "#TRAINING\n",
        "J,A,parameters = train(train_dataset,neural_train_labels,iters=2000)\n",
        "\n",
        "#for training set\n",
        "_,train_pred = forward_propagation(train_dataset,parameters)\n",
        "_,train_accuracy = cal_loss_accuracy(neural_train_labels,train_pred,parameters)\n",
        "\n",
        "#for valid set\n",
        "_,valid_pred = forward_propagation(valid_dataset,parameters)\n",
        "_,valid_accuracy = cal_loss_accuracy(neural_valid_labels,valid_pred,parameters)\n",
        "\n",
        "#for test set\n",
        "_,test_pred = forward_propagation(test_dataset,parameters)\n",
        "_,test_accuracy = cal_loss_accuracy(neural_test_labels,test_pred,parameters)\n",
        "\n",
        "print('Accuracy of Train Set = {}'.format(round(train_accuracy*100,2)))\n",
        "print('Accuracy of Valid Set = {}'.format(round(valid_accuracy*100,2)))\n",
        "print('Accuracy of Test  Set = {}'.format(round(test_accuracy*100,2)))\n",
        "\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "cnfn_mat=confusion_matrix(test_labels,np.argmax(test_pred,1))\n",
        "sns.heatmap(cnfn_mat,annot=cnfn_mat,fmt='',xticklabels=['0','1'],yticklabels=['0','1'])\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "avg_loss = list()\n",
        "avg_acc = list()\n",
        "i = 0\n",
        "while(i<len(J)):\n",
        "    avg_loss.append(np.mean(J[i:i+30]))\n",
        "    avg_acc.append(np.mean(A[i:i+30]))\n",
        "    i += 30\n",
        "\n",
        "plt.plot(list(range(len(avg_loss))),avg_loss)\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"Loss (Avg of 30 batches)\")\n",
        "plt.title(\"Loss Graph\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot(list(range(len(avg_acc))),avg_acc)\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"Accuracy (Avg of 30 batches)\")\n",
        "plt.title(\"Accuracy Graph\")\n",
        "plt.show()\n",
        "\n",
        "index = 29\n",
        "test_image = test_dataset[index].reshape(1,28,28,3)\n",
        "plt.imshow(test_image[0,:,:,0],cmap='gray')\n",
        "print(\"Image\")\n",
        "plt.show()\n",
        "print(\"True_Label = {}\".format(np.argmax(neural_test_labels[index])))\n",
        "print(\"Pred_Label = {}\".format(np.argmax(test_pred[index])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6O37LsAbopK8"
      },
      "outputs": [],
      "source": [
        "#resnet50 on pneumoniamnist\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
        "\n",
        "from __future__ import print_function\n",
        "#%matplotlib inline\n",
        "import argparse\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "from IPython.display import HTML\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision.utils import make_grid\n",
        "from IPython.display import Image\n",
        "from keras.utils.np_utils import to_categorical   \n",
        "# Set random seed for reproducibility\n",
        "manualSeed = 999\n",
        "#manualSeed = random.randint(1, 10000) # use if you want new results\n",
        "print(\"Random Seed: \", manualSeed)\n",
        "random.seed(manualSeed)\n",
        "torch.manual_seed(manualSeed)\n",
        "\n",
        "DATA_DIR = \"../input/pneumoniamnist/pneumoniamnist.npz\"\n",
        "X_train = np.load(DATA_DIR)\n",
        "#print(f\"Shape of training data: {X_train.shape}\")\n",
        "print(f\"Data type: {type(X_train)}\")\n",
        "data = X_train[\"train_images\"].astype(np.float64)\n",
        "x = []\n",
        "for img in data:\n",
        "    x.append(np.dstack([img,img,img]))\n",
        "x = np.array(x)\n",
        "label_data = X_train[\"train_labels\"]\n",
        "y = []\n",
        "for label in label_data:\n",
        "    y.append(label[0])\n",
        "y = np.array(y)    \n",
        "print(y.shape)\n",
        "#y = to_categorical(np.array(X_train[\"train_labels\"]),num_classes=2)\n",
        "data = x\n",
        "data = 255 * data\n",
        "img = data.astype(np.uint8)\n",
        "X_train = img\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, data, targets, transform=None):\n",
        "        self.data = data\n",
        "        self.targets = torch.LongTensor(targets)\n",
        "        self.transform = transform\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        x = self.data[index]\n",
        "        y = self.targets[index]\n",
        "        \n",
        "        if self.transform:\n",
        "            x = self.transform(x)\n",
        "        \n",
        "        return x, y\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "transform = transforms.Compose([transforms.ToPILImage(),\n",
        "                                transforms.Resize(224),\n",
        "                                transforms.CenterCrop(224),\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),])\n",
        "cropped_dataset = MyDataset(X_train, y, transform=transform)\n",
        "print(len(cropped_dataset))\n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(cropped_dataset, batch_size=128,\n",
        "                                         shuffle=True, num_workers=2, pin_memory=True)\n",
        "\n",
        "def imshow(inp, title=None):\n",
        "    \n",
        "    inp = inp.cpu() if device else inp\n",
        "    inp = inp.numpy().transpose((1, 2, 0))\n",
        "    \n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    inp = std * inp + mean\n",
        "    inp = np.clip(inp, 0, 1)\n",
        "    \n",
        "    plt.imshow(inp)\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.pause(0.001)\n",
        "    \n",
        "images, labels = next(iter(train_dataloader)) \n",
        "print(\"images-size:\", images.shape)\n",
        "\n",
        "out = make_grid(images)\n",
        "print(\"out-size:\", out.shape)\n",
        "\n",
        "#imshow(out, title=[cropped_dataset.classes[x] for x in labels])\n",
        "\n",
        "DATA_DIR = \"../input/pneumoniamnist/pneumoniamnist.npz\"\n",
        "X_test = np.load(DATA_DIR)\n",
        "#print(f\"Shape of training data: {X_train.shape}\")\n",
        "print(f\"Data type: {type(X_test)}\")\n",
        "data = X_test[\"test_images\"].astype(np.float64)\n",
        "val_data = X_test[\"val_images\"].astype(np.float64)\n",
        "x = []\n",
        "for img in data:\n",
        "    x.append(np.dstack([img,img,img]))\n",
        "x = np.array(x)\n",
        "x_val = []\n",
        "for img in val_data:\n",
        "    x_val.append(np.dstack([img,img,img]))\n",
        "x_val = np.array(x_val)\n",
        "label_data = np.array(X_test[\"test_labels\"])\n",
        "val_label = np.array(X_test[\"val_labels\"])\n",
        "y = []\n",
        "for label in label_data:\n",
        "    y.append(label[0])\n",
        "y = np.array(y)\n",
        "y_val = []\n",
        "for label in val_label:\n",
        "    y_val.append(label[0])\n",
        "y_val = np.array(y_val)\n",
        "print(y.shape)\n",
        "#y = to_categorical(np.array(X_test[\"test_labels\"]),num_classes=2)\n",
        "data = x\n",
        "data = 255 * data\n",
        "img = data.astype(np.uint8)\n",
        "X_test = img\n",
        "X_val = (x_val*255).astype(np.uint8)\n",
        "\n",
        "test_dataset = MyDataset(X_test, y, transform=transform)\n",
        "val_dataset =  MyDataset(X_val, y_val, transform=transform)\n",
        "print(len(test_dataset))\n",
        "print(len(val_dataset))\n",
        "\n",
        "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=128,\n",
        "                                         shuffle=True, num_workers=2, pin_memory=True)\n",
        "\n",
        "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=128,\n",
        "                                         shuffle=True, num_workers=2, pin_memory=True)\n",
        "\n",
        "idx2lbl={0:'False',1:'True'}\n",
        "inv_normalize = transforms.Normalize(\n",
        "    mean=[-0.485/0.229, -0.456/0.224, -0.406/0.255],\n",
        "    std=[1/0.229, 1/0.224, 1/0.255]\n",
        ")\n",
        "\n",
        "images,labels=next(iter(train_dataloader))\n",
        "fig=plt.figure(figsize=(25,4))\n",
        "\n",
        "for i in range(1,21):\n",
        "    ax=fig.add_subplot(2,10,i,xticks=[],yticks=[])\n",
        "    ax.imshow(transforms.ToPILImage()(inv_normalize(images[i])))\n",
        "    ax.set_title(str(idx2lbl[labels[i].item()]))\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim,no_grad\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import models,transforms,datasets\n",
        "from torch.nn import Dropout,ReLU,Linear,Sequential,LogSoftmax,Softmax\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "\n",
        "model=models.resnet50(pretrained=True)\n",
        "\n",
        "#set requires_grad to False inorder to freeze parameter update\n",
        "for param in model.parameters():\n",
        "    param.requires_grad=False\n",
        "\n",
        "model.fc\n",
        "\n",
        "#create custom classifcation layer\n",
        "classifier=Sequential(\n",
        "    Dropout(p=0.5),\n",
        "    Linear(in_features=2048, out_features=512, bias=True),\n",
        "    ReLU(),\n",
        "    Linear(in_features=512, out_features=2, bias=True),\n",
        "    LogSoftmax(dim=1)\n",
        ")\n",
        "\n",
        "#replacing resnet classifier layer with custom one\n",
        "model.fc=classifier\n",
        "\n",
        "criterion=nn.NLLLoss()\n",
        "\n",
        "optimizer=optim.Adam(model.fc.parameters(),lr=0.003)\n",
        "\n",
        "model\n",
        "\n",
        "def evaluate(model,dataLoader):\n",
        "    \n",
        "    predicted_labels_list=list()\n",
        "    actual_labels_list=list()\n",
        "    \n",
        "    with no_grad(): #turning off gradient calculation\n",
        "        model.eval() #setting model to evaluation mode,i.e,all dropout will be deactivated\n",
        "        \n",
        "        test_loss_acm=0\n",
        "        accuracy_acm=0\n",
        "\n",
        "        for images,labels in dataLoader:\n",
        "\n",
        "            output=model.forward(images)\n",
        "            test_loss=criterion(output,labels)\n",
        "            result=nn.functional.softmax(output,dim=1)\n",
        "            test_loss_acm+=test_loss.item()\n",
        "            result_labels=torch.argmax(result,dim=1)\n",
        "            bools=(result_labels==labels)\n",
        "            accuracy=bools.sum().type(torch.float)/len(bools)\n",
        "            accuracy_acm+=accuracy\n",
        "            \n",
        "            predicted_labels_list.extend([x.item() for x in result_labels])\n",
        "            actual_labels_list.extend([x.item() for x in labels])\n",
        "            \n",
        "        #setting model back to train mode\n",
        "        model.train()\n",
        "\n",
        "        avg_test_loss=test_loss_acm/len(dataLoader)\n",
        "        avg_test_acc=accuracy_acm/len(dataLoader)\n",
        "        \n",
        "    return (avg_test_loss,avg_test_acc,predicted_labels_list,actual_labels_list)\n",
        "\n",
        "epochs=5\n",
        "\n",
        "train_loss_pe=list()\n",
        "val_loss_pe=list()\n",
        "\n",
        "train_acc_pe=list()\n",
        "val_acc_pe=list()\n",
        "\n",
        "for epoch in range(1,epochs+1):\n",
        "    running_loss=0\n",
        "    running_acc=0\n",
        "    \n",
        "    for images,labels in tqdm(train_dataloader):\n",
        "        \n",
        "        #set optimizer grad to zero\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        #forward pass\n",
        "        output=model.forward(images)\n",
        "    \n",
        "        #loss\n",
        "        loss=criterion(output,labels)\n",
        "        #train accuracy\n",
        "        result=torch.argmax(output,dim=1)\n",
        "        running_loss+=loss.item()\n",
        "        running_acc+=torch.mean((result==labels).type(torch.float))\n",
        "        \n",
        "        #backprop\n",
        "        loss.backward()\n",
        "        \n",
        "        #descent\n",
        "        optimizer.step()\n",
        "    else:\n",
        "        train_loss=running_loss/len(train_dataloader)\n",
        "        train_acc=running_acc/len(train_dataloader)\n",
        "        \n",
        "        val_loss,val_acc,pred,actual=evaluate(model,val_dataloader)\n",
        "        \n",
        "        train_loss_pe.append(train_loss)\n",
        "        val_loss_pe.append(val_loss)\n",
        "        \n",
        "        train_acc_pe.append(train_acc.item())\n",
        "        val_acc_pe.append(val_acc.item())\n",
        "        \n",
        "        print(\"EPOCH: {}\".format(epoch))\n",
        "        print(\"Train Loss: {:.3f}\".format(train_loss,end=\" \"))\n",
        "        print(\"Val Loss: {:.3f}\".format(val_loss,end=\" \"))\n",
        "        print(\"Train Accuracy: {:.2f}%\".format(train_acc*100,end=\" \"))\n",
        "        print(\"Val Accuracy: {:.2f}%\".format(val_acc*100,end=\" \"))\n",
        "        \n",
        "        with open(\"./resnet50_tl_{}.pth\".format(epoch),\"wb\") as f:\n",
        "            model.eval()\n",
        "            pickle.dump(model,f)\n",
        "            model.train()\n",
        "\n",
        "plots=[(train_loss_pe,val_loss_pe),(train_acc_pe,val_acc_pe)]\n",
        "plt_labels=[(\"Training Loss\",\"Validation Loss\"),(\"Training Accuracy\",\"Validation Accuracy\")]\n",
        "plt_titles=[\"Loss\",\"Accuracy\"]\n",
        "plt.figure(figsize=(20,7))\n",
        "for i in range(0,2):\n",
        "    ax=plt.subplot(1,2,i+1)\n",
        "    ax.plot(plots[i][0],label=plt_labels[i][0])\n",
        "    ax.plot(plots[i][1],label=plt_labels[i][1])\n",
        "    ax.set_title(plt_titles[i])\n",
        "    ax.legend()\n",
        "\n",
        "#selecting the best model\n",
        "with open(\"./resnet50_tl_5.pth\",\"rb\") as f:\n",
        "    loaded_model=pickle.load(f)\n",
        "\n",
        "avg_test_loss,avg_test_acc,predicted_label,actual_label=evaluate(loaded_model,test_dataloader)\n",
        "print(\"Test Loss:{:.3f}\".format(avg_test_loss,end=\"  \"))\n",
        "print(\"Test Accuracy:{:.2f}%\".format(avg_test_acc*100,end=\" \"))\n",
        "\n",
        "cnfn_mat=confusion_matrix(actual_label,predicted_label)\n",
        "sns.heatmap(cnfn_mat,annot=cnfn_mat,fmt='',xticklabels=['0','1'],yticklabels=['0','1'])\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()\n",
        "\n",
        "report=pd.DataFrame.from_dict(classification_report(actual_label,predicted_label,output_dict=True)).T\n",
        "report=report[['f1-score','precision','recall','support']]\n",
        "report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GtGWr6wcoxcR"
      },
      "outputs": [],
      "source": [
        "#resnet50 for bloodmnist\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
        "\n",
        "from __future__ import print_function\n",
        "#%matplotlib inline\n",
        "import argparse\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "from IPython.display import HTML\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision.utils import make_grid\n",
        "from IPython.display import Image\n",
        "from keras.utils.np_utils import to_categorical   \n",
        "# Set random seed for reproducibility\n",
        "manualSeed = 999\n",
        "#manualSeed = random.randint(1, 10000) # use if you want new results\n",
        "print(\"Random Seed: \", manualSeed)\n",
        "random.seed(manualSeed)\n",
        "torch.manual_seed(manualSeed)\n",
        "\n",
        "DATA_DIR = \"../input/bloodmnist/bloodmnist.npz\" #change to pneumoniamnist/pneumoniamnist.npz\n",
        "X_train = np.load(DATA_DIR)\n",
        "#print(f\"Shape of training data: {X_train.shape}\")\n",
        "print(f\"Data type: {type(X_train)}\")\n",
        "data = X_train[\"train_images\"].astype(np.float64)\n",
        "x = []\n",
        "for img in data:\n",
        "    x.append(img) #use x.append(np.dstack[img,img,img]) for pneumoniamnist\n",
        "x = np.array(x)\n",
        "label_data = X_train[\"train_labels\"]\n",
        "y = []\n",
        "for label in label_data:\n",
        "    y.append(label[0])\n",
        "y = np.array(y)    \n",
        "print(y.shape)\n",
        "#y = to_categorical(np.array(X_train[\"train_labels\"]),num_classes=2)\n",
        "data = x\n",
        "data = 255 * data\n",
        "img = data.astype(np.uint8)\n",
        "X_train = img\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, data, targets, transform=None):\n",
        "        self.data = data\n",
        "        self.targets = torch.LongTensor(targets)\n",
        "        self.transform = transform\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        x = self.data[index]\n",
        "        y = self.targets[index]\n",
        "        \n",
        "        if self.transform:\n",
        "            x = self.transform(x)\n",
        "        \n",
        "        return x, y\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "transform = transforms.Compose([transforms.ToPILImage(),\n",
        "                                transforms.Resize(224),\n",
        "                                transforms.CenterCrop(224),\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),])\n",
        "cropped_dataset = MyDataset(X_train, y, transform=transform)\n",
        "print(len(cropped_dataset))\n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(cropped_dataset, batch_size=128,\n",
        "                                         shuffle=True, num_workers=2, pin_memory=True)\n",
        "\n",
        "def imshow(inp, title=None):\n",
        "    \n",
        "    inp = inp.cpu() if device else inp\n",
        "    inp = inp.numpy().transpose((1, 2, 0))\n",
        "    \n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    inp = std * inp + mean\n",
        "    inp = np.clip(inp, 0, 1)\n",
        "    \n",
        "    plt.imshow(inp)\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.pause(0.001)\n",
        "    \n",
        "images, labels = next(iter(train_dataloader)) \n",
        "print(\"images-size:\", images.shape)\n",
        "\n",
        "out = make_grid(images)\n",
        "print(\"out-size:\", out.shape)\n",
        "\n",
        "#imshow(out, title=[cropped_dataset.classes[x] for x in labels])\n",
        "\n",
        "DATA_DIR = \"../input/bloodmnist/bloodmnist.npz\"\n",
        "X_test = np.load(DATA_DIR)\n",
        "#print(f\"Shape of training data: {X_train.shape}\")\n",
        "print(f\"Data type: {type(X_test)}\")\n",
        "data = X_test[\"test_images\"].astype(np.float64)\n",
        "val_data = X_test[\"val_images\"].astype(np.float64)\n",
        "x = []\n",
        "for img in data:\n",
        "    x.append(img)\n",
        "x = np.array(x)\n",
        "x_val = []\n",
        "for img in val_data:\n",
        "    x_val.append(img)\n",
        "x_val = np.array(x_val)\n",
        "label_data = np.array(X_test[\"test_labels\"])\n",
        "val_label = np.array(X_test[\"val_labels\"])\n",
        "y = []\n",
        "for label in label_data:\n",
        "    y.append(label[0])\n",
        "y = np.array(y)\n",
        "y_val = []\n",
        "for label in val_label:\n",
        "    y_val.append(label[0])\n",
        "y_val = np.array(y_val)\n",
        "print(y.shape)\n",
        "#y = to_categorical(np.array(X_test[\"test_labels\"]),num_classes=2)\n",
        "data = x\n",
        "data = 255 * data\n",
        "img = data.astype(np.uint8)\n",
        "X_test = img\n",
        "X_val = (x_val*255).astype(np.uint8)\n",
        "\n",
        "test_dataset = MyDataset(X_test, y, transform=transform)\n",
        "val_dataset =  MyDataset(X_val, y_val, transform=transform)\n",
        "print(len(test_dataset))\n",
        "print(len(val_dataset))\n",
        "\n",
        "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=128,\n",
        "                                         shuffle=True, num_workers=2, pin_memory=True)\n",
        "\n",
        "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=128,\n",
        "                                         shuffle=True, num_workers=2, pin_memory=True)\n",
        "\n",
        "idx2lbl={0:'0',1:'1',2:'2',3:'3',4:'4',5:'5',6:'6',7:'7'}\n",
        "inv_normalize = transforms.Normalize(\n",
        "    mean=[-0.485/0.229, -0.456/0.224, -0.406/0.255],\n",
        "    std=[1/0.229, 1/0.224, 1/0.255]\n",
        ")\n",
        "\n",
        "images,labels=next(iter(train_dataloader))\n",
        "fig=plt.figure(figsize=(25,4))\n",
        "\n",
        "for i in range(1,21):\n",
        "    ax=fig.add_subplot(2,10,i,xticks=[],yticks=[])\n",
        "    ax.imshow(transforms.ToPILImage()(inv_normalize(images[i])))\n",
        "    ax.set_title(str(idx2lbl[labels[i].item()]))\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim,no_grad\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import models,transforms,datasets\n",
        "from torch.nn import Dropout,ReLU,Linear,Sequential,LogSoftmax,Softmax\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "\n",
        "model=models.resnet50(pretrained=True)\n",
        "\n",
        "#set requires_grad to False inorder to freeze parameter update\n",
        "for param in model.parameters():\n",
        "    param.requires_grad=False\n",
        "\n",
        "model.fc\n",
        "\n",
        "#create custom classifcation layer\n",
        "classifier=Sequential(\n",
        "    Dropout(p=0.5),\n",
        "    Linear(in_features=2048, out_features=512, bias=True),\n",
        "    ReLU(),\n",
        "    Linear(in_features=512, out_features=8, bias=True),\n",
        "    LogSoftmax(dim=1)\n",
        ")\n",
        "\n",
        "#replacing resnet classifier layer with custom one\n",
        "model.fc=classifier\n",
        "\n",
        "criterion=nn.NLLLoss()\n",
        "\n",
        "optimizer=optim.Adam(model.fc.parameters(),lr=0.003)\n",
        "\n",
        "model\n",
        "\n",
        "def evaluate(model,dataLoader):\n",
        "    \n",
        "    predicted_labels_list=list()\n",
        "    actual_labels_list=list()\n",
        "    \n",
        "    with no_grad(): #turning off gradient calculation\n",
        "        model.eval() #setting model to evaluation mode,i.e,all dropout will be deactivated\n",
        "        \n",
        "        test_loss_acm=0\n",
        "        accuracy_acm=0\n",
        "\n",
        "        for images,labels in dataLoader:\n",
        "\n",
        "            output=model.forward(images)\n",
        "            test_loss=criterion(output,labels)\n",
        "            result=nn.functional.softmax(output,dim=1)\n",
        "            test_loss_acm+=test_loss.item()\n",
        "            result_labels=torch.argmax(result,dim=1)\n",
        "            bools=(result_labels==labels)\n",
        "            accuracy=bools.sum().type(torch.float)/len(bools)\n",
        "            accuracy_acm+=accuracy\n",
        "            \n",
        "            predicted_labels_list.extend([x.item() for x in result_labels])\n",
        "            actual_labels_list.extend([x.item() for x in labels])\n",
        "            \n",
        "        #setting model back to train mode\n",
        "        model.train()\n",
        "\n",
        "        avg_test_loss=test_loss_acm/len(dataLoader)\n",
        "        avg_test_acc=accuracy_acm/len(dataLoader)\n",
        "        \n",
        "    return (avg_test_loss,avg_test_acc,predicted_labels_list,actual_labels_list)\n",
        "\n",
        "epochs=5\n",
        "\n",
        "train_loss_pe=list()\n",
        "val_loss_pe=list()\n",
        "\n",
        "train_acc_pe=list()\n",
        "val_acc_pe=list()\n",
        "\n",
        "for epoch in range(1,epochs+1):\n",
        "    running_loss=0\n",
        "    running_acc=0\n",
        "    \n",
        "    for images,labels in tqdm(train_dataloader):\n",
        "        \n",
        "        #set optimizer grad to zero\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        #forward pass\n",
        "        output=model.forward(images)\n",
        "    \n",
        "        #loss\n",
        "        loss=criterion(output,labels)\n",
        "        #train accuracy\n",
        "        result=torch.argmax(output,dim=1)\n",
        "        running_loss+=loss.item()\n",
        "        running_acc+=torch.mean((result==labels).type(torch.float))\n",
        "        \n",
        "        #backprop\n",
        "        loss.backward()\n",
        "        \n",
        "        #descent\n",
        "        optimizer.step()\n",
        "    else:\n",
        "        train_loss=running_loss/len(train_dataloader)\n",
        "        train_acc=running_acc/len(train_dataloader)\n",
        "        \n",
        "        val_loss,val_acc,pred,actual=evaluate(model,val_dataloader)\n",
        "        \n",
        "        train_loss_pe.append(train_loss)\n",
        "        val_loss_pe.append(val_loss)\n",
        "        \n",
        "        train_acc_pe.append(train_acc.item())\n",
        "        val_acc_pe.append(val_acc.item())\n",
        "        \n",
        "        print(\"EPOCH: {}\".format(epoch))\n",
        "        print(\"Train Loss: {:.3f}\".format(train_loss,end=\" \"))\n",
        "        print(\"Val Loss: {:.3f}\".format(val_loss,end=\" \"))\n",
        "        print(\"Train Accuracy: {:.2f}%\".format(train_acc*100,end=\" \"))\n",
        "        print(\"Val Accuracy: {:.2f}%\".format(val_acc*100,end=\" \"))\n",
        "        \n",
        "        with open(\"./resnet50_tl_{}.pth\".format(epoch),\"wb\") as f:\n",
        "            model.eval()\n",
        "            pickle.dump(model,f)\n",
        "            model.train()\n",
        "\n",
        "plots=[(train_loss_pe,val_loss_pe),(train_acc_pe,val_acc_pe)]\n",
        "plt_labels=[(\"Training Loss\",\"Validation Loss\"),(\"Training Accuracy\",\"Validation Accuracy\")]\n",
        "plt_titles=[\"Loss\",\"Accuracy\"]\n",
        "plt.figure(figsize=(20,7))\n",
        "for i in range(0,2):\n",
        "    ax=plt.subplot(1,2,i+1)\n",
        "    ax.plot(plots[i][0],label=plt_labels[i][0])\n",
        "    ax.plot(plots[i][1],label=plt_labels[i][1])\n",
        "    ax.set_title(plt_titles[i])\n",
        "    ax.legend()\n",
        "\n",
        "#selecting the best model\n",
        "with open(\"./resnet50_tl_3.pth\",\"rb\") as f:\n",
        "    loaded_model=pickle.load(f)\n",
        "\n",
        "avg_test_loss,avg_test_acc,predicted_label,actual_label=evaluate(loaded_model,test_dataloader)\n",
        "print(\"Test Loss:{:.3f}\".format(avg_test_loss,end=\"  \"))\n",
        "print(\"Test Accuracy:{:.2f}%\".format(avg_test_acc*100,end=\" \"))\n",
        "\n",
        "cnfn_mat=confusion_matrix(actual_label,predicted_label)\n",
        "sns.heatmap(cnfn_mat,annot=cnfn_mat,fmt='',xticklabels=['0','1'],yticklabels=['0','1'])\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()\n",
        "\n",
        "report=pd.DataFrame.from_dict(classification_report(actual_label,predicted_label,output_dict=True)).T\n",
        "report=report[['f1-score','precision','recall','support']]\n",
        "report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ApOYrGTuo7rK"
      },
      "outputs": [],
      "source": [
        "#vgg16 for pneumoniamnist\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
        "\n",
        "from __future__ import print_function\n",
        "#%matplotlib inline\n",
        "import argparse\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "from IPython.display import HTML\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision.utils import make_grid\n",
        "from IPython.display import Image\n",
        "from keras.utils.np_utils import to_categorical   \n",
        "# Set random seed for reproducibility\n",
        "manualSeed = 999\n",
        "#manualSeed = random.randint(1, 10000) # use if you want new results\n",
        "print(\"Random Seed: \", manualSeed)\n",
        "random.seed(manualSeed)\n",
        "torch.manual_seed(manualSeed)\n",
        "\n",
        "# check GPU availability\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "import torch.optim as optim\n",
        "import time\n",
        "\n",
        "from torchvision import models\n",
        "\n",
        "DATA_DIR = \"../input/pneumoniamnist/pneumoniamnist.npz\" #change to pneumoniamnist/pneumoniamnist.npz\n",
        "X_train = np.load(DATA_DIR)\n",
        "#print(f\"Shape of training data: {X_train.shape}\")\n",
        "data = X_train['train_images'].astype(np.float64)\n",
        "print('Hi')\n",
        "x = []\n",
        "for img in data:\n",
        "    x.append(np.dstack((img,img,img))) # for pneumoniamnist\n",
        "x = np.array(x)\n",
        "label_data = X_train['train_labels']\n",
        "y = []\n",
        "for label in label_data:\n",
        "    y.append(label[0])\n",
        "y = np.array(y)    \n",
        "print(y.shape)\n",
        "#y = to_categorical(np.array(X_train[\"train_labels\"]),num_classes=2)\n",
        "data = x\n",
        "data = 255 * data\n",
        "img = data.astype(np.uint8)\n",
        "X_train = img\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, data, targets, transform=None):\n",
        "        self.data = data\n",
        "        self.targets = torch.LongTensor(targets)\n",
        "        self.transform = transform\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        x = self.data[index]\n",
        "        y = self.targets[index]\n",
        "        \n",
        "        if self.transform:\n",
        "            x = self.transform(x)\n",
        "        \n",
        "        return x, y\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "transform = transforms.Compose([transforms.ToPILImage(),\n",
        "                                transforms.Resize(224),\n",
        "                                transforms.CenterCrop(224),\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5, 0.5, 0.5), (0.225, 0.225, 0.225)),])\n",
        "cropped_dataset = MyDataset(X_train, y, transform=transform)\n",
        "print(len(cropped_dataset))\n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(cropped_dataset, batch_size=128,\n",
        "                                         shuffle=True, num_workers=2, pin_memory=True)\n",
        "\n",
        "def imshow(inp, title=None):\n",
        "    \n",
        "    inp = inp.cpu() if device else inp\n",
        "    inp = inp.numpy().transpose((1, 2, 0))\n",
        "    \n",
        "    mean = np.array([0.5, 0.5, 0.5])\n",
        "    std = np.array([0.225, 0.225, 0.225])\n",
        "    inp = std * inp + mean\n",
        "    inp = np.clip(inp, 0, 1)\n",
        "    \n",
        "    plt.imshow(inp)\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.pause(0.001)\n",
        "    \n",
        "images, labels = next(iter(train_dataloader)) \n",
        "print(\"images-size:\", images.shape)\n",
        "\n",
        "out = make_grid(images)\n",
        "print(\"out-size:\", out.shape)\n",
        "\n",
        "#imshow(out, title=[cropped_dataset.classes[x] for x in labels])\n",
        "\n",
        "DATA_DIR = \"../input/pneumoniamnist/pneumoniamnist.npz\"\n",
        "X_test = np.load(DATA_DIR)\n",
        "#print(f\"Shape of training data: {X_train.shape}\")\n",
        "print(f\"Data type: {type(X_test)}\")\n",
        "data = X_test[\"test_images\"].astype(np.float64)\n",
        "val_data = X_test[\"val_images\"].astype(np.float64)\n",
        "x = []\n",
        "for img in data:\n",
        "    x.append(np.dstack((img,img,img)))\n",
        "x = np.array(x)\n",
        "x_val = []\n",
        "for img in val_data:\n",
        "    x_val.append(np.dstack((img,img,img)))\n",
        "x_val = np.array(x_val)\n",
        "label_data = np.array(X_test[\"test_labels\"])\n",
        "val_label = np.array(X_test[\"val_labels\"])\n",
        "y = []\n",
        "for label in label_data:\n",
        "    y.append(label[0])\n",
        "y = np.array(y)\n",
        "y_val = []\n",
        "for label in val_label:\n",
        "    y_val.append(label[0])\n",
        "y_val = np.array(y_val)\n",
        "print(y.shape)\n",
        "#y = to_categorical(np.array(X_test[\"test_labels\"]),num_classes=2)\n",
        "data = x\n",
        "data = 255 * data\n",
        "img = data.astype(np.uint8)\n",
        "X_test = img\n",
        "X_val = (x_val*255).astype(np.uint8)\n",
        "\n",
        "test_dataset = MyDataset(X_test, y, transform=transform)\n",
        "val_dataset =  MyDataset(X_val, y_val, transform=transform)\n",
        "print(len(test_dataset))\n",
        "print(len(val_dataset))\n",
        "\n",
        "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=128,\n",
        "                                         shuffle=True, num_workers=2, pin_memory=True)\n",
        "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=128,\n",
        "                                         shuffle=True, num_workers=2, pin_memory=True)\n",
        "\n",
        "model=models.vgg16(pretrained=True)\n",
        "model.to(device)\n",
        "model\n",
        "\n",
        "# change the number of classes \n",
        "model.classifier[6].out_features = 2\n",
        "\n",
        "# freeze convolution weights\n",
        "for param in model.features.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# optimizer\n",
        "optimizer = optim.Adam(model.classifier.parameters(), lr=0.001,weight_decay=1e-4)\n",
        "\n",
        "# loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\"\"\"In the validate() method, we are calculating the loss and accuracy. But we are not backpropagating the gradients. Backpropagation is only required during training.\"\"\"\n",
        "# Training and Validation Functions\n",
        "# validation function\n",
        "def validate(model, test_dataloader):\n",
        "    model.eval()\n",
        "    val_running_loss = 0.0\n",
        "    val_running_correct = 0\n",
        "    \n",
        "    for int, data in enumerate(test_dataloader):\n",
        "        data, target = data[0].to(device), data[1].to(device)\n",
        "        output = model.forward(data)\n",
        "        loss = criterion(output, target)\n",
        "        \n",
        "        val_running_loss += loss.item()\n",
        "        _, preds = torch.max(output.data, 1)\n",
        "        val_running_correct += (preds == target).sum().item()\n",
        "    \n",
        "    val_loss = val_running_loss/len(test_dataloader.dataset)\n",
        "    val_accuracy = 100. * val_running_correct/len(test_dataloader.dataset)\n",
        "\n",
        "    print(f'Validation Loss: {val_loss:.4f}, Validation Acc: {val_accuracy:.2f}')\n",
        "    \n",
        "    return val_loss, val_accuracy\n",
        "\n",
        "# we will define the fit() method for training.\n",
        "# training function\n",
        "def fit(model, train_dataloader):\n",
        "    model.train()\n",
        "    train_running_loss = 0.0\n",
        "    train_running_correct = 0\n",
        "    \n",
        "    for i, data in enumerate(train_dataloader):\n",
        "        data, target = data[0].to(device), data[1].to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model.forward(data)\n",
        "        loss = criterion(output, target)\n",
        "\n",
        "        train_running_loss += loss.item()\n",
        "        _, preds = torch.max(output.data, 1)\n",
        "        train_running_correct += (preds == target).sum().item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    train_loss = train_running_loss/len(train_dataloader.dataset)\n",
        "    train_accuracy = 100. * train_running_correct/len(train_dataloader.dataset)\n",
        "\n",
        "    print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.2f}')\n",
        "    \n",
        "    return train_loss, train_accuracy\n",
        "\n",
        "import torch.optim as optim\n",
        "import time\n",
        "\"\"\"Let’s train the model for 10 epochs. For each epoch, we will call the fit() and validate() method.\"\"\"\n",
        "train_loss , train_accuracy = [], []\n",
        "val_loss , val_accuracy = [], []\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "for epoch in range(10):\n",
        "    train_epoch_loss, train_epoch_accuracy = fit(model, train_dataloader)\n",
        "    val_epoch_loss, val_epoch_accuracy = validate(model, val_dataloader)\n",
        "    train_loss.append(train_epoch_loss)\n",
        "    train_accuracy.append(train_epoch_accuracy)\n",
        "    val_loss.append(val_epoch_loss)\n",
        "    val_accuracy.append(val_epoch_accuracy)\n",
        "    \n",
        "end = time.time()\n",
        "print((end-start)/60, 'minutes')\n",
        "\n",
        "# Visualizing the Plots\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.plot(train_accuracy, color='green', label='train accuracy')\n",
        "plt.plot(val_accuracy, color='blue', label='validataion accuracy')\n",
        "plt.legend()\n",
        "plt.savefig('accuracy.png')\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.plot(train_loss, color='orange', label='train loss')\n",
        "plt.plot(val_loss, color='red', label='validataion loss')\n",
        "plt.legend()\n",
        "plt.savefig('loss.png')\n",
        "plt.show()\n",
        "\n",
        "def evaluate(model,dataLoader):\n",
        "    \n",
        "    predicted_labels_list=list()\n",
        "    actual_labels_list=list()\n",
        "    \n",
        "    with no_grad(): #turning off gradient calculation\n",
        "        model.eval() #setting model to evaluation mode,i.e,all dropout will be deactivated\n",
        "        \n",
        "        test_loss_acm=0\n",
        "        accuracy_acm=0\n",
        "\n",
        "        for i,data in enumerate(dataLoader):\n",
        "            images, labels = data[0].to(device), data[1].to(device)\n",
        "            output=model.forward(images)\n",
        "            test_loss=criterion(output,labels)\n",
        "            result=nn.functional.softmax(output,dim=1)\n",
        "            test_loss_acm+=test_loss.item()\n",
        "            result_labels=torch.argmax(result,dim=1)\n",
        "            bools=(result_labels==labels)\n",
        "            accuracy=bools.sum().type(torch.float)/len(bools)\n",
        "            accuracy_acm+=accuracy\n",
        "            \n",
        "            predicted_labels_list.extend([x.item() for x in result_labels])\n",
        "            actual_labels_list.extend([x.item() for x in labels])\n",
        "            \n",
        "        #setting model back to train mode\n",
        "        model.train()\n",
        "\n",
        "        avg_test_loss=test_loss_acm/len(dataLoader)\n",
        "        avg_test_acc=accuracy_acm/len(dataLoader)\n",
        "        \n",
        "    return (avg_test_loss,avg_test_acc,predicted_labels_list,actual_labels_list)\n",
        "\n",
        "from torch import optim,no_grad\n",
        "avg_test_loss,avg_test_acc,predicted_label,actual_label=evaluate(model,test_dataloader)\n",
        "print(\"Test Loss:{:.3f}\".format(avg_test_loss,end=\"  \"))\n",
        "print(\"Test Accuracy:{:.2f}%\".format(avg_test_acc*100,end=\" \"))\n",
        "\n",
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "import seaborn as sns\n",
        "cnfn_mat=confusion_matrix(actual_label,predicted_label)\n",
        "sns.heatmap(cnfn_mat,annot=cnfn_mat,fmt='',xticklabels=['0','1'],yticklabels=['0','1'])\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()\n",
        "\n",
        "report=pd.DataFrame.from_dict(classification_report(actual_label,predicted_label,output_dict=True)).T\n",
        "report=report[['f1-score','precision','recall','support']]\n",
        "report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2xmSFcJ7pEJK"
      },
      "outputs": [],
      "source": [
        "#vgg16 for bloodmnist\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
        "\n",
        "from __future__ import print_function\n",
        "#%matplotlib inline\n",
        "import argparse\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "from IPython.display import HTML\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision.utils import make_grid\n",
        "from IPython.display import Image\n",
        "from keras.utils.np_utils import to_categorical \n",
        "from torch import optim,no_grad\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import models\n",
        "from torch.nn import Dropout,ReLU,Linear,Sequential,LogSoftmax,Softmax\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "# Set random seed for reproducibility\n",
        "manualSeed = 999\n",
        "#manualSeed = random.randint(1, 10000) # use if you want new results\n",
        "print(\"Random Seed: \", manualSeed)\n",
        "random.seed(manualSeed)\n",
        "torch.manual_seed(manualSeed)\n",
        "\n",
        "# check GPU availability\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "import torch.optim as optim\n",
        "import time\n",
        "\n",
        "DATA_DIR = \"../input/bloodmnist/bloodmnist.npz\"\n",
        "X_train = np.load(DATA_DIR)\n",
        "#print(f\"Shape of training data: {X_train.shape}\")\n",
        "print(f\"Data type: {type(X_train)}\")\n",
        "data = X_train[\"train_images\"].astype(np.float64)\n",
        "test_data = X_train[\"test_images\"].astype(np.float64)\n",
        "val_data = X_train[\"val_images\"].astype(np.float64)\n",
        "x = np.array(data)\n",
        "x_test = np.array(test_data)\n",
        "x_val = np.array(val_data)\n",
        "label_data = X_train[\"train_labels\"]\n",
        "test_label = X_train[\"test_labels\"]\n",
        "val_label = X_train[\"val_labels\"]\n",
        "y = []\n",
        "for label in label_data:\n",
        "    y.append(label[0])\n",
        "y = np.array(y)\n",
        "y_test = []\n",
        "for label in test_label:\n",
        "    y_test.append(label[0])\n",
        "y_test = np.array(y_test) \n",
        "y_val = []\n",
        "for label in val_label:\n",
        "    y_val.append(label[0])\n",
        "y_val = np.array(y_val) \n",
        "print(y.shape)\n",
        "#y = to_categorical(np.array(X_train[\"train_labels\"]),num_classes=2)\n",
        "data = x\n",
        "data = 255 * data\n",
        "img = data.astype(np.uint8)\n",
        "X_train = img\n",
        "X_val = (x_val*255).astype(np.uint8)\n",
        "X_test  = (x_test*255).astype(np.uint8)\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, data, targets, transform=None):\n",
        "        self.data = data\n",
        "        self.targets = torch.LongTensor(targets)\n",
        "        self.transform = transform\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        x = self.data[index]\n",
        "        y = self.targets[index]\n",
        "        \n",
        "        if self.transform:\n",
        "            x = self.transform(x)\n",
        "        \n",
        "        return x, y\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "transform = transforms.Compose([transforms.ToPILImage(),\n",
        "                                transforms.Resize(224),\n",
        "                                transforms.CenterCrop(224),\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),])\n",
        "y1_train = y\n",
        "y1_test = y_test\n",
        "y1_val = y_val\n",
        "\n",
        "cropped_dataset = MyDataset(X_train, y, transform=transform)\n",
        "test_dataset = MyDataset(X_test, y_test, transform=transform)\n",
        "val_dataset =  MyDataset(X_val, y_val, transform=transform)\n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(cropped_dataset, batch_size=128,\n",
        "                                         shuffle=True, num_workers=2, pin_memory=True)\n",
        "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=128,\n",
        "                                         shuffle=True, num_workers=2, pin_memory=True)\n",
        "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=128,\n",
        "                                         shuffle=True, num_workers=2, pin_memory=True)\n",
        "\n",
        "model=models.vgg16(pretrained=True)\n",
        "\n",
        "model.to(device)\n",
        "model\n",
        "\n",
        "# change the number of classes \n",
        "model.classifier[6].out_features = 8\n",
        "\n",
        "# freeze convolution weights\n",
        "for param in model.features.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "model\n",
        "\n",
        "# optimizer\n",
        "optimizer = optim.Adam(model.classifier.parameters(), lr=0.001)\n",
        "\n",
        "# loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\"\"\"In the validate() method, we are calculating the loss and accuracy. But we are not backpropagating the gradients. Backpropagation is only required during training.\"\"\"\n",
        "# Training and Validation Functions\n",
        "# validation function\n",
        "def validate(model, test_dataloader):\n",
        "    model.eval()\n",
        "    val_running_loss = 0.0\n",
        "    val_running_correct = 0\n",
        "    \n",
        "    for int, data in enumerate(test_dataloader):\n",
        "        data, target = data[0].to(device), data[1].to(device)\n",
        "        output = model.forward(data)\n",
        "        loss = criterion(output, target)\n",
        "        \n",
        "        val_running_loss += loss.item()\n",
        "        _, preds = torch.max(output.data, 1)\n",
        "        val_running_correct += (preds == target).sum().item()\n",
        "    \n",
        "    val_loss = val_running_loss/len(test_dataloader.dataset)\n",
        "    val_accuracy = 100. * val_running_correct/len(test_dataloader.dataset)\n",
        "\n",
        "    print(f'Validation Loss: {val_loss:.4f}, Validation Acc: {val_accuracy:.2f}')\n",
        "    \n",
        "    return val_loss, val_accuracy\n",
        "\n",
        "# we will define the fit() method for training.\n",
        "# training function\n",
        "def fit(model, train_dataloader):\n",
        "    model.train()\n",
        "    train_running_loss = 0.0\n",
        "    train_running_correct = 0\n",
        "    \n",
        "    for i, data in enumerate(train_dataloader):\n",
        "        data, target = data[0].to(device), data[1].to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model.forward(data)\n",
        "        loss = criterion(output, target)\n",
        "\n",
        "        train_running_loss += loss.item()\n",
        "        _, preds = torch.max(output.data, 1)\n",
        "        train_running_correct += (preds == target).sum().item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    train_loss = train_running_loss/len(train_dataloader.dataset)\n",
        "    train_accuracy = 100. * train_running_correct/len(train_dataloader.dataset)\n",
        "\n",
        "    print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.2f}')\n",
        "    \n",
        "    return train_loss, train_accuracy\n",
        "\n",
        "import torch.optim as optim\n",
        "import time\n",
        "\"\"\"Let’s train the model for 10 epochs. For each epoch, we will call the fit() and validate() method.\"\"\"\n",
        "train_loss , train_accuracy = [], []\n",
        "val_loss , val_accuracy = [], []\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "for epoch in range(10):\n",
        "    train_epoch_loss, train_epoch_accuracy = fit(model, train_dataloader)\n",
        "    val_epoch_loss, val_epoch_accuracy = validate(model, val_dataloader)\n",
        "    train_loss.append(train_epoch_loss)\n",
        "    train_accuracy.append(train_epoch_accuracy)\n",
        "    val_loss.append(val_epoch_loss)\n",
        "    val_accuracy.append(val_epoch_accuracy)\n",
        "    \n",
        "end = time.time()\n",
        "print((end-start)/60, 'minutes')\n",
        "\n",
        "# Visualizing the Plots\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.plot(train_accuracy, color='green', label='train accuracy')\n",
        "plt.plot(val_accuracy, color='blue', label='validataion accuracy')\n",
        "plt.legend()\n",
        "plt.savefig('accuracy.png')\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.plot(train_loss, color='orange', label='train loss')\n",
        "plt.plot(val_loss, color='red', label='validataion loss')\n",
        "plt.legend()\n",
        "plt.savefig('loss.png')\n",
        "plt.show()\n",
        "\n",
        "def evaluate(model,dataLoader):\n",
        "    \n",
        "    predicted_labels_list=list()\n",
        "    actual_labels_list=list()\n",
        "    \n",
        "    with no_grad(): #turning off gradient calculation\n",
        "        model.eval() #setting model to evaluation mode,i.e,all dropout will be deactivated\n",
        "        \n",
        "        test_loss_acm=0\n",
        "        accuracy_acm=0\n",
        "\n",
        "        for i,data in enumerate(dataLoader):\n",
        "            images, labels = data[0].to(device), data[1].to(device)\n",
        "            output=model.forward(images)\n",
        "            test_loss=criterion(output,labels)\n",
        "            result=nn.functional.softmax(output,dim=1)\n",
        "            test_loss_acm+=test_loss.item()\n",
        "            result_labels=torch.argmax(result,dim=1)\n",
        "            bools=(result_labels==labels)\n",
        "            accuracy=bools.sum().type(torch.float)/len(bools)\n",
        "            accuracy_acm+=accuracy\n",
        "            \n",
        "            predicted_labels_list.extend([x.item() for x in result_labels])\n",
        "            actual_labels_list.extend([x.item() for x in labels])\n",
        "            \n",
        "        #setting model back to train mode\n",
        "        model.train()\n",
        "\n",
        "        avg_test_loss=test_loss_acm/len(dataLoader)\n",
        "        avg_test_acc=accuracy_acm/len(dataLoader)\n",
        "        \n",
        "    return (avg_test_loss,avg_test_acc,predicted_labels_list,actual_labels_list)\n",
        "\n",
        "avg_test_loss,avg_test_acc,predicted_label,actual_label=evaluate(model,test_dataloader)\n",
        "print(\"Test Loss:{:.3f}\".format(avg_test_loss,end=\"  \"))\n",
        "print(\"Test Accuracy:{:.2f}%\".format(avg_test_acc*100,end=\" \"))\n",
        "\n",
        "cnfn_mat=confusion_matrix(actual_label,predicted_label)\n",
        "sns.heatmap(cnfn_mat,annot=cnfn_mat,fmt='',xticklabels=['0','1','2','3','4','5','6','7'],yticklabels=['0','1','2','3','4','5','6','7'])\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()\n",
        "\n",
        "report=pd.DataFrame.from_dict(classification_report(actual_label,predicted_label,output_dict=True)).T\n",
        "report=report[['f1-score','precision','recall','support']]\n",
        "report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gvnBjmTBpP0g"
      },
      "outputs": [],
      "source": [
        "#FLDA\n",
        "\"\"\"\n",
        "Fisher Linear Discriminant \n",
        "\n",
        "Returns the labels for the test dataset from the Fischer Linear Discriminant using the training data\n",
        "\"\"\"\n",
        "def FisherLD(trainX, trainY, nClasses, ndim, dim) :\n",
        "    classMean = []\n",
        "    for i in range(nClasses):\n",
        "        classMean.append(np.mean(trainX[trainY.T[0] == i], axis = 0))\n",
        "    mean = np.mean(trainX, axis=0)\n",
        "    \n",
        "    SW = np.zeros((ndim, ndim))\n",
        "    for cl,mv in zip(range(nClasses), classMean):\n",
        "        class_sc_mat = np.zeros((ndim,ndim))                  # scatter matrix for every class\n",
        "        for row in trainX[trainY.T[0] == i]:\n",
        "            row, mv = row.reshape(ndim,1), mv.reshape(ndim,1) # make column vectors\n",
        "            class_sc_mat += (row-mv).dot((row-mv).T)\n",
        "        SW += class_sc_mat\n",
        "            \n",
        "    SB = np.zeros((ndim, ndim))\n",
        "    for i,mean_vec in enumerate(classMean):  \n",
        "        n = trainX[trainY.T[0] == i, :].shape[0]\n",
        "        mean_vec = mean_vec.reshape(ndim,1) # make column vector\n",
        "        mean = mean.reshape(ndim,1) # make column vector\n",
        "        SB += n * (mean_vec - mean).dot((mean_vec - mean).T)\n",
        "    \n",
        "    return np.linalg.pinv(SW).dot(SB)\n",
        "\n",
        "def Pred_FisherLD(testX, W, nClasses) :\n",
        "    probDensR = np.zeros((testX.shape[0], nClasses))\n",
        "    for i in range(testX.shape[0]):\n",
        "        point = np.array([testX[i][j] for j in range(testX.shape[1])])\n",
        "        probDensR[i] = W.dot(point)\n",
        "    return probDensR\n",
        "\n",
        "W = FisherLD(pnuTrainX, pnuTrainY, len(pnuClasses), len(pnuTrainX[0]), 2)\n",
        "\n",
        "FLDProbDensR = Pred_FisherLD(pnuTestX, W, 2)\n",
        "\n",
        "pnuFLDTestR = FLDProbDensR.argmax(axis = 1)\n",
        "\n",
        "from __future__ import print_function, division\n",
        "\n",
        "class LDA():\n",
        "    def __init__(self):\n",
        "        self.w = None\n",
        "\n",
        "    def transform(self, X, y):\n",
        "        self.fit(X[y == 0], X[y == 1], y)\n",
        "        # Project data onto vector\n",
        "        X_transform = X.dot(self.w)\n",
        "        return X_transform\n",
        "    \n",
        "    def fit(self, X1, X2, y):\n",
        "        # Separate data by class\n",
        "        #X1 = X[y == 0]\n",
        "        #X2 = X[y == 1]\n",
        "\n",
        "        # Calculate the covariance matrices of the two datasets\n",
        "        mean1 = X1.mean(0)\n",
        "        mean2 = X2.mean(0)\n",
        "        \n",
        "        cov1 = np.zeros((X1.shape[1], X1.shape[1]))\n",
        "        for i in X1:\n",
        "            cov1 += (i - mean1).T.dot((i - mean2))\n",
        "        \n",
        "        cov2 = np.zeros((X2.shape[1], X2.shape[1]))\n",
        "        for i in X2:\n",
        "            cov1 += (i - mean2).T.dot((i - mean2))\n",
        "            \n",
        "        cov_tot = cov1 + cov2\n",
        "        # Calculate the mean of the two datasets\n",
        "        mean1 = X1.mean(0)\n",
        "        mean2 = X2.mean(0)\n",
        "        mean_diff = np.atleast_1d(mean1 - mean2)\n",
        "        # Determine the vector which when X is projected onto it best separates the\n",
        "        # data by class. w = (mean1 - mean2) / (cov1 + cov2)\n",
        "        self.w = np.linalg.pinv(cov_tot).dot(mean_diff)\n",
        "\n",
        "    def predict(self, X):\n",
        "        y_pred = []\n",
        "        for sample in X:\n",
        "            h = sample.dot(self.w)\n",
        "            y = 1 * (h < 0)\n",
        "            y_pred.append(y)\n",
        "        return y_pred\n",
        "\n",
        "y = pnuTrainY.T[0]\n",
        "X1 = pnuTrainX[y == 0]\n",
        "X2 = pnuTrainX[y == 1]\n",
        "\n",
        "lda = LDA()\n",
        "\n",
        "lda.fit(X1, X2, y)\n",
        "\n",
        "pnuFLDTestR = lda.predict(pnuTestX)\n",
        "\n",
        "FLDCM = confMatrix(pnuTestY, pnuFLDTestR, pnuClasses)\n",
        "print(\"pnu FLD Accuracy:\", accuracy(FLDCM))\n",
        "print(\"pnu FLD F1 Score:\", f1Score(FLDCM))\n",
        "print(\"pnu FLD AUC Score:\", roc_auc_score(pnuFLDTestR, pnuTestY))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7E7Vc_lpyYy"
      },
      "outputs": [],
      "source": [
        "#LSTM\n",
        "# Commented out IPython magic to ensure Python compatibility.\n",
        "import IPython.display as ipd  # To play sound in the notebook\n",
        "from scipy.io import wavfile # for reading wave files as numpy arrays\n",
        "import wave # opening .wav files\n",
        "import struct # for padding\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import matplotlib.pyplot as plt # visualizations\n",
        "# %matplotlib inline\n",
        "import os # operating system\n",
        "from os.path import join\n",
        "import time\n",
        "\n",
        "import os\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "RATE = 16000\n",
        "data_dir = \"../input/darpa-timit-acousticphonetic-continuous-speech/data\"\n",
        "train_csv_file = \"../input/darpa-timit-acousticphonetic-continuous-speech/train_data.csv\"\n",
        "test_csv_file = \"../input/darpa-timit-acousticphonetic-continuous-speech/test_data.csv\"\n",
        "\n",
        "def get_good_audio_files(filename):\n",
        "    df = pd.read_csv(filename)\n",
        "    return df[df['is_converted_audio'] == True]\n",
        "\n",
        "def parse_phn_timestamps(wrd_path, verbose=False):\n",
        "    print('phn_path', wrd_path) if verbose else None\n",
        "    speaker_id = wrd_path.split('/')[-2]\n",
        "    sentence_id = wrd_path.split('/')[-1].replace('.PHN', '')\n",
        "    wrd_file = open(wrd_path)\n",
        "    content = wrd_file.read()\n",
        "    content = content.split('\\n')\n",
        "    content = [tuple(foo.split(' ') + [speaker_id, sentence_id]) for foo in content if foo != '']\n",
        "    wrd_file.close()\n",
        "    return content\n",
        "\n",
        "def read_audio(wave_path, verbose=False):\n",
        "    rate, data = wavfile.read(wave_path)\n",
        "    # make sure the rate of the file is the RATE that we want\n",
        "    assert rate == RATE\n",
        "    print(\"Sampling (frame) rate = \", rate) if verbose else None\n",
        "    print(\"Total samples (frames) = \", data.shape) if verbose else None\n",
        "    return data\n",
        "\n",
        "def join_dirs(row):\n",
        "    return os.path.join(data_dir,\n",
        "                       row['test_or_train'],\n",
        "                       row['dialect_region'],\n",
        "                       row['speaker_id'],\n",
        "                       row['filename'])\n",
        "\n",
        "def parse_word_waves(time_aligned_words, audio_data, verbose=False):\n",
        "    return [align_data(data, words, verbose) for data, words in zip(audio_data, time_aligned_words)]\n",
        "    \n",
        "def align_data(data, words, verbose=False):\n",
        "    aligned = []\n",
        "    print('len(data)', len(data)) if verbose else None\n",
        "    print('len(words)', len(words)) if verbose else None\n",
        "    print('data', data) if verbose else None\n",
        "    print('words', words) if verbose else None\n",
        "    for tup in words[1:-1]:\n",
        "        print('tup',tup) if verbose else None\n",
        "        start = int(tup[0])\n",
        "        end = int(tup[1])\n",
        "        word = tup[2]\n",
        "        assert start >= 0\n",
        "        assert end <= len(data)\n",
        "        aligned.append( (data[start:end], word))\n",
        "    return aligned\n",
        "\n",
        "def make_data_set(train_csv_file):\n",
        "    train_csv = get_good_audio_files(train_csv_file)\n",
        "    train_csv['filepath'] = train_csv.apply(lambda row: join_dirs(row), axis=1)\n",
        "    waves = train_csv['filepath']\n",
        "    audio_data = [read_audio(wave) for wave in waves]\n",
        "    wrds = [wave.replace('.WAV.wav', '') + '.PHN' for wave in waves]\n",
        "    word_data = [parse_phn_timestamps(wrd) for wrd in wrds]\n",
        "    train_data = [align_data(audio, wrd) for audio, wrd in zip(audio_data, word_data)]\n",
        "    train_data = [item for sublist in train_data for item in sublist]\n",
        "    return train_data\n",
        "\n",
        "def add_padding(data, length) :\n",
        "    padded_data = []\n",
        "    for row in data :\n",
        "        x1 = np.zeros(length)\n",
        "        if row[0].shape[0] > length:\n",
        "            x1 = row[0][:length]\n",
        "        else:\n",
        "            x1[:row[0].shape[0]] = row[0]\n",
        "        padded_data.append((x1, row[1]))\n",
        "    return padded_data\n",
        "\n",
        "train_data = make_data_set(train_csv_file)\n",
        "test_data = make_data_set(test_csv_file)\n",
        "\n",
        "vowels = [\"iy\", \"ih\", \"eh\", \"ey\", \"ae\", \"aa\", \"aw\", \"ay\", \"ah\", \"ao\", \"oy\", \"ow\", \"uh\", \"uw\", \"ux\", \"er\", \"ax\", \"ix\", \"axr\", \"ax-h\"]\n",
        "\n",
        "train_data = add_padding(train_data, 100)\n",
        "test_data = add_padding(test_data, 100)\n",
        "\n",
        "trainX = np.array([i[0] for i in train_data])\n",
        "trainY = np.array([int(i[1] in vowels) for i in train_data])\n",
        "\n",
        "testX = np.array([i[0] for i in test_data])\n",
        "testY = np.array([int(i[1] in vowels) for i in test_data])\n",
        "\n",
        "trainX = torch.from_numpy(trainX).type(torch.Tensor)\n",
        "trainY = torch.from_numpy(trainY).type(torch.LongTensor)\n",
        "\n",
        "testX = torch.from_numpy(testX).type(torch.Tensor)\n",
        "testY = torch.from_numpy(testY).type(torch.LongTensor)\n",
        "\n",
        "trainX = torch.reshape(trainX,   (trainX.shape[0], trainX.shape[1], 1))\n",
        "trainX = torch.reshape(testX,  (testX.shape[0], testX.shape[1], 1))\n",
        "\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim=1, num_layers=2):\n",
        "        super(LSTM, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # setup LSTM layer\n",
        "        self.lstm = nn.LSTM(self.input_dim, self.hidden_dim, self.num_layers)\n",
        "\n",
        "        # setup output layer\n",
        "        self.linear = nn.Linear(self.hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, input, hidden=None):\n",
        "        # lstm step => then ONLY take the sequence's final timetep to pass into the linear/dense layer\n",
        "        # Note: lstm_out contains outputs for every step of the sequence we are looping over (for BPTT)\n",
        "        # but we just need the output of the last step of the sequence, aka lstm_out[-1]\n",
        "        lstm_out, hidden = self.lstm(input, hidden)\n",
        "        logits = self.linear(lstm_out[-1])              # equivalent to return_sequences=False from Keras\n",
        "        genre_scores = F.log_softmax(logits, dim=1)\n",
        "        return genre_scores, hidden\n",
        "\n",
        "    def get_accuracy(self, logits, target):\n",
        "        \"\"\" compute accuracy for training round \"\"\"\n",
        "        corrects = (\n",
        "                torch.max(logits, 1)[1].view(target.size()).data == target.data\n",
        "        ).sum()\n",
        "        accuracy = 100.0 * corrects / self.batch_size\n",
        "        return accuracy.item()\n",
        "\n",
        "num_epochs = 10\n",
        "\n",
        "# Define model\n",
        "print(\"Build LSTM RNN model ...\")\n",
        "model = LSTM(input_dim=1, hidden_dim=10, output_dim=1, num_layers=2)\n",
        "loss_function = nn.NLLLoss()  # expects ouputs from LogSoftmax\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# To keep LSTM stateful between batches, you can set stateful = True, which is not suggested for training\n",
        "stateful = False\n",
        "\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "if train_on_gpu:\n",
        "    print(\"\\nTraining on GPU\")\n",
        "else:\n",
        "    print(\"\\nNo GPU, training on CPU\")\n",
        "\n",
        "print(\"Training ...\")\n",
        "for epoch in range(1):\n",
        "\n",
        "    train_running_loss, train_acc = 0.0, 0.0\n",
        "\n",
        "    # Init hidden state - if you don't want a stateful LSTM (between epochs)\n",
        "    hidden_state = None\n",
        "    # zero out gradient, so they don't accumulate btw batches\n",
        "    model.zero_grad()\n",
        "\n",
        "    y_pred, hidden_state = model(trainX, hidden_state)  # forward pass\n",
        "\n",
        "    # Stateful = False for training. Do we go Stateful = True during inference/prediction time?\n",
        "    if not stateful:\n",
        "        hidden_state = None\n",
        "    else:\n",
        "        h_0, c_0 = hidden_state\n",
        "        h_0.detach_(), c_0.detach_()\n",
        "        hidden_state = (h_0, c_0)\n",
        "    \n",
        "    print(y_pred.shape)\n",
        "#     loss = loss_function(y_pred, trainY)  # compute loss\n",
        "#     loss.backward()  # backward pass\n",
        "#     optimizer.step()  # parameter update\n",
        "\n",
        "#     train_running_loss += loss.detach().item()  # unpacks the tensor into a scalar value\n",
        "#     train_acc += model.get_accuracy(y_pred, trainY)\n",
        "\n",
        "#     print(\n",
        "#         \"Epoch:  %d | NLLoss: %.4f | Train Accuracy: %.2f\"\n",
        "#         % (epoch, train_running_loss / num_batches, train_acc / num_batches)\n",
        "#     )\n",
        "\n",
        "# # visualization loss\n",
        "# plt.plot(epoch_list, val_loss_list)\n",
        "# plt.xlabel(\"# of epochs\")\n",
        "# plt.ylabel(\"Loss\")\n",
        "# plt.title(\"LSTM: Loss vs # epochs\")\n",
        "# plt.show()\n",
        "\n",
        "# # visualization accuracy\n",
        "# plt.plot(epoch_list, val_accuracy_list, color=\"red\")\n",
        "# plt.xlabel(\"# of epochs\")\n",
        "# plt.ylabel(\"Accuracy\")\n",
        "# plt.title(\"LSTM: Accuracy vs # epochs\")\n",
        "# # plt.savefig('graph.png')\n",
        "# plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "PRNNA2_Rahul_Nishanth_Bhartendu_Jeevithiesh.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
