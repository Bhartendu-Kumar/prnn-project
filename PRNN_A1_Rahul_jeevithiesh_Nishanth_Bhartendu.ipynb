{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ynXJ5EFMgVBM"
      },
      "outputs": [],
      "source": [
        "#Q1 MLP with Gaussian likelihood for pneumoniamnist\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "#data1 = np.load('drive/MyDrive/Colab Notebooks/PRNN_A1_DATA/pneumoniamnist.npz')\n",
        "data1 = np.load('/content/drive/MyDrive/PRNN/pneumoniamnist.npz')\n",
        "#data2 = np.load('drive/MyDrive/Colab Notebooks/PRNN_A1_DATA/bloodmnist.npz')\n",
        "print(data1.files)\n",
        "print(data2.files)\n",
        "\n",
        "for key, value in data1.items():\n",
        "  print(key)\n",
        "  print(data1[key].shape)\n",
        "for key, value in data2.items():\n",
        "  print(key)\n",
        "  print(data2[key].shape)\n",
        "\n",
        "# Reshape images and save in numpy arrays\n",
        "train1_data = np.reshape(data1['train_images'],(4708,784))\n",
        "train1_labels = np.reshape(data1['train_labels'],(4708))\n",
        "\n",
        "print(train1_data.shape)\n",
        "print(train1_labels.shape)\n",
        "test1_data = np.reshape(data1['test_images'],(624,784))\n",
        "test1_labels = np.reshape(data1['test_labels'],(624))\n",
        "print(test1_data.shape)\n",
        "print(test1_labels.shape)\n",
        "\n",
        "# Gaussian Random Vector with Maximum Likelihood Estimate\n",
        "# Sample Mean Calculation\n",
        "mu0 = np.zeros(784)\n",
        "mu1 = np.zeros(784)\n",
        "N0 = 0\n",
        "N1 = 0\n",
        "for i in range(len(train1_labels)):\n",
        "  if train1_labels[i] == 0:\n",
        "    mu0 += train1_data[i]\n",
        "    N0+=1\n",
        "  else:\n",
        "    mu1 += train1_data[i]\n",
        "    N1+=1 \n",
        "mu0 = mu0/N0\n",
        "mu1 = mu1/N1\n",
        "print(N0)\n",
        "print(N1)\n",
        "\n",
        "# Sample Variance Calculation \n",
        "sig0 = np.zeros((784,784))\n",
        "sig1 = np.zeros((784,784))\n",
        "\n",
        "for i in range(len(train1_labels)):\n",
        "  if train1_labels[i] == 0:\n",
        "    sig0 += (1/N0)* np.outer(train1_data[i]-mu0,train1_data[i].T - mu0.T )\n",
        "  else:  \n",
        "    sig1 += (1/N1)*np.outer(train1_data[i]-mu1,train1_data[i].T - mu1.T )\n",
        "#print(np.log(np.linalg.det(sig0/9.5)))\n",
        "#print(np.log(np.linalg.det(sig1/9.5)))\n",
        "#print(np.linalg.inv(sig0))\n",
        "#print(np.outer(train1_data[0]-mu0,train1_data[0].T - mu0.T ).shape)\n",
        "\n",
        "test1_label_predict_grv = np.zeros(len(test1_labels))\n",
        "print(test1_label_predict_grv.shape)\n",
        "inv0 = np.linalg.inv(sig0)\n",
        "inv1 = np.linalg.inv(sig1)\n",
        "print(inv0.shape)\n",
        "print(inv1.shape)\n",
        "d = np.log(np.linalg.det(sig0/9.5)) - np.log(np.linalg.det(sig1/9.5))\n",
        "print(d)\n",
        "print(np.dot(inv1,(test1_data[0]-mu1)).shape)\n",
        "c = 0\n",
        "for i in range(len(test1_labels)):\n",
        "  if np.dot((test1_data[i]-mu0),np.dot(inv0,(test1_data[i]-mu0))) - np.dot((test1_data[i]-mu1),np.dot(inv1,(test1_data[i]-mu1))) + d > 0  :\n",
        "    test1_label_predict_grv[i] = 1\n",
        "  else:\n",
        "    test1_label_predict_grv[i] = 0\n",
        "\n",
        "test1_label_predict_grv = test1_label_predict_grv.astype(int)\n",
        "test1_labels = test1_labels.astype(int)\n",
        "\n",
        "for i,x in enumerate(test1_labels):\n",
        "  if x == test1_label_predict_grv[i]:\n",
        "    c+=1\n",
        "print(c/len(test1_labels)*100)    \n",
        "\n",
        "def compute_confusion_matrix(true, pred):\n",
        "\n",
        "  K = len(np.unique(true)) # Number of classes \n",
        "  result = np.zeros((K, K))\n",
        "\n",
        "  for i in range(len(true)):\n",
        "    result[true[i]][pred[i]] += 1\n",
        "  tn = result[0][0] \n",
        "  fn = result[0][1] \n",
        "  fp = result[1][0] \n",
        "  tp = result[1][1]\n",
        "  acc = (tp+tn)/(tp+tn+fp+fn)\n",
        "  f1 = tp/(tp+0.5*(fp+fn)) \n",
        "  print(\"Accuracy = \", acc)\n",
        "  print(\"F-1 Score = \", f1)\n",
        "  return result\n",
        "\n",
        "print(compute_confusion_matrix(test1_labels,test1_label_predict_grv))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RUFKIaEggsaL"
      },
      "outputs": [],
      "source": [
        "#MLE with Naive Bayes Gaussian likehood for pneumoniamnist\n",
        "sig0nb = np.zeros(784)\n",
        "sig1nb = np.zeros(784)\n",
        "for i in range(len(train1_labels)):\n",
        "  if train1_labels[i] == 0:\n",
        "    sig0nb+= (1/N0)*(mu0 - train1_data[i])**2\n",
        "  else:\n",
        "    sig1nb+= (1/N1)*(mu1 - train1_data[i])**2 \n",
        "\n",
        "test1_labels_predict_nb = np.zeros(784)\n",
        "sig0_inv = 1/sig0nb\n",
        "sig1_inv = 1/sig1nb\n",
        "for i in range(len(test1_labels)):\n",
        "  if np.dot((test1_data[i] - mu0),(sig0_inv *(test1_data[i] - mu0))) > np.dot((test1_data[i] - mu1),(sig1_inv *(test1_data[i] - mu1))):\n",
        "    test1_labels_predict_nb[i] = 1\n",
        "c= 0 \n",
        "\n",
        "test1_labels_predict_nb = test1_labels_predict_nb.astype(int)\n",
        "test1_labels = test1_labels.astype(int)\n",
        "\n",
        "for i,x in enumerate(test1_labels):\n",
        "  if x == test1_labels_predict_nb[i]:\n",
        "    c+=1\n",
        "print(c/len(test1_labels)*100)\n",
        "\n",
        "\n",
        "def compute_confusion_matrix(true, pred):\n",
        "\n",
        "  K = len(np.unique(true)) # Number of classes \n",
        "  result = np.zeros((K, K))\n",
        "\n",
        "  for i in range(len(true)):\n",
        "    result[true[i]][pred[i]] += 1\n",
        "  tn = result[0][0] \n",
        "  fn = result[0][1] \n",
        "  fp = result[1][0] \n",
        "  tp = result[1][1]\n",
        "  acc = (tp+tn)/(tp+tn+fp+fn)\n",
        "  f1 = tp/(tp+0.5*(fp+fn)) \n",
        "  print(\"Accuracy = \", acc)\n",
        "  print(\"F-1 Score = \", f1)\n",
        "  return result\n",
        "\n",
        "print(compute_confusion_matrix(test1_labels,test1_labels_predict_nb))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MyBwi41phH7B"
      },
      "outputs": [],
      "source": [
        "#MLE with Gaussian Random Vector likelihood for bloodmnist\n",
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
        "\n",
        "data2 = np.load('../input/bloodmnist/bloodmnist.npz')\n",
        "print(data2.files)\n",
        "\n",
        "for key, value in data2.items():\n",
        "  print(key)\n",
        "  print(data2[key].shape)\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage.color import rgb2gray\n",
        "\n",
        "\n",
        "def process_data(path):\n",
        "    dataset = np.load(path)\n",
        "    train_x, train_y = np.reshape(\n",
        "        rgb2gray(dataset['train_images']), (11959, 784, 1)), dataset['train_labels']\n",
        "    val_x, val_y = np.reshape(rgb2gray(dataset['val_images']),\n",
        "                              (1712, 784, 1)), dataset['val_labels']\n",
        "    test_x, test_y = np.reshape(\n",
        "        rgb2gray(dataset['test_images']), (3421, 784, 1)), dataset['test_labels']\n",
        "\n",
        "    return train_x, train_y, test_x, test_y\n",
        "\n",
        "\n",
        "def estimate_class_conditionals(train_x, train_y):\n",
        "    n = train_x.shape[1]\n",
        "    mu0 = np.zeros((n, 1))\n",
        "    mu1 = np.zeros((n, 1))\n",
        "    mu2 = np.zeros((n, 1))\n",
        "    mu3 = np.zeros((n, 1))\n",
        "    mu4 = np.zeros((n, 1))\n",
        "    mu5 = np.zeros((n, 1))\n",
        "    mu6 = np.zeros((n, 1))\n",
        "    mu7 = np.zeros((n, 1))\n",
        "    sig0 = np.zeros((n, n), dtype=np.double)\n",
        "    sig1 = np.zeros((n, n), dtype=np.double)\n",
        "    sig2 = np.zeros((n, n), dtype=np.double)\n",
        "    sig3 = np.zeros((n, n), dtype=np.double)\n",
        "    sig4 = np.zeros((n, n), dtype=np.double)\n",
        "    sig5 = np.zeros((n, n), dtype=np.double)\n",
        "    sig6 = np.zeros((n, n), dtype=np.double)\n",
        "    sig7 = np.zeros((n, n), dtype=np.double)\n",
        "    n0, n1, n2, n3, n4, n5, n6, n7 = 0, 0, 0, 0, 0, 0, 0, 0\n",
        "    n = train_x.shape[0]\n",
        "\n",
        "    print(\"[INFO] Estimating class conditional densities\")\n",
        "\n",
        "    for i in range(train_y.shape[0]):\n",
        "        if(train_y[i] == 0):\n",
        "            mu0 += train_x[i]\n",
        "            n0 += 1\n",
        "        elif(train_y[i] == 1):\n",
        "            mu1 += train_x[i]\n",
        "            n1 += 1\n",
        "        elif(train_y[i] == 2):\n",
        "            mu2 += train_x[i]\n",
        "            n2 += 1\n",
        "        if(train_y[i] == 3):\n",
        "            mu3 += train_x[i]\n",
        "            n3 += 1\n",
        "        elif(train_y[i] == 4):\n",
        "            mu4 += train_x[i]\n",
        "            n4 += 1\n",
        "        elif(train_y[i] == 5):\n",
        "            mu5 += train_x[i]\n",
        "            n5 += 1\n",
        "        if(train_y[i] == 6):\n",
        "            mu6 += train_x[i]\n",
        "            n6 += 1\n",
        "        else:\n",
        "            mu7 += train_x[i]\n",
        "            n7 += 1\n",
        "\n",
        "    mu0 *= 1.0/n0\n",
        "    mu1 *= 1.0/n1\n",
        "    mu2 *= 1.0/n2\n",
        "    mu3 *= 1.0/n3\n",
        "    mu4 *= 1.0/n4\n",
        "    mu5 *= 1.0/n5\n",
        "    mu6 *= 1.0/n6\n",
        "    mu7 *= 1.0/n7\n",
        "    means = [mu0, mu1, mu2, mu3, mu4, mu5, mu6, mu7]\n",
        "    print(n0,n1,n2,n3,n4,n5,n6,n7)\n",
        "    for i in range(train_y.shape[0]):\n",
        "        if(train_y[i] == 0):\n",
        "            sig0 += np.outer(train_x[i]-mu0, (train_x[i]-mu0).T)\n",
        "        elif(train_y[i] == 1):\n",
        "            sig1 += np.outer(train_x[i]-mu1, (train_x[i]-mu1).T)\n",
        "        elif(train_y[i] == 2):\n",
        "            sig2 += np.outer(train_x[i]-mu2, (train_x[i]-mu2).T)\n",
        "        elif(train_y[i] == 3):\n",
        "            sig3 += np.outer(train_x[i]-mu3, (train_x[i]-mu3).T)\n",
        "        elif(train_y[i] == 4):\n",
        "            sig4 += np.outer(train_x[i]-mu4, (train_x[i]-mu4).T)\n",
        "        elif(train_y[i] == 5):\n",
        "            sig5 += np.outer(train_x[i]-mu5, (train_x[i]-mu5).T)\n",
        "        elif(train_y[i] == 6):\n",
        "            sig6 += np.outer(train_x[i]-mu6, (train_x[i]-mu6).T)\n",
        "        else:\n",
        "            sig7 += np.outer(train_x[i]-mu7, (train_x[i]-mu7).T)\n",
        "\n",
        "    sig0 *= 1.0/n0\n",
        "    sig1 *= 1.0/n1\n",
        "    sig2 *= 1.0/n2\n",
        "    sig3 *= 1.0/n3\n",
        "    sig4 *= 1.0/n4\n",
        "    sig5 *= 1.0/n5\n",
        "    sig6 *= 1.0/n6\n",
        "    sig7 *= 1.0/n7\n",
        "    covariances = [sig0, sig1, sig2, sig3, sig4, sig5, sig6, sig7]\n",
        "\n",
        "    p0 = n0/n\n",
        "    p1 = n1/n\n",
        "    p2 = n2/n\n",
        "    p3 = n3/n\n",
        "    p4 = n4/n\n",
        "    p5 = n5/n\n",
        "    p6 = n6/n\n",
        "    p7 = n7/n\n",
        "    priors = [p0, p1, p2, p3, p4, p5, p6, p7]\n",
        "\n",
        "    (sign0, logdet0) = np.linalg.slogdet(sig0)\n",
        "    (sign1, logdet1) = np.linalg.slogdet(sig1)\n",
        "    (sign2, logdet2) = np.linalg.slogdet(sig2)\n",
        "    (sign3, logdet3) = np.linalg.slogdet(sig3)\n",
        "    (sign4, logdet4) = np.linalg.slogdet(sig4)\n",
        "    (sign5, logdet5) = np.linalg.slogdet(sig5)\n",
        "    (sign6, logdet6) = np.linalg.slogdet(sig6)\n",
        "    (sign7, logdet7) = np.linalg.slogdet(sig7)\n",
        "    \n",
        "    sigma0_inv = np.linalg.pinv(sig0)\n",
        "    sigma1_inv = np.linalg.pinv(sig1)\n",
        "    sigma2_inv = np.linalg.pinv(sig2)\n",
        "    sigma3_inv = np.linalg.pinv(sig3)\n",
        "    sigma4_inv = np.linalg.pinv(sig4)\n",
        "    sigma5_inv = np.linalg.pinv(sig5)\n",
        "    sigma6_inv = np.linalg.pinv(sig6)\n",
        "    sigma7_inv = np.linalg.pinv(sig7)\n",
        "    sigma_inv = [sigma0_inv,sigma1_inv,sigma2_inv,sigma3_inv,sigma4_inv,sigma5_inv,sigma6_inv,sigma7_inv]\n",
        "    logdets = [logdet0, logdet1, logdet2, logdet3, logdet4, logdet5, logdet6, logdet7]\n",
        "\n",
        "    return means, covariances, logdets, priors, sigma_inv\n",
        "\n",
        "\n",
        "def calc_log_posterior(X, mu, sigma, prior, logdet,sigma_inv):\n",
        "    return np.log(prior) - 0.5*logdet - 0.5*np.dot((X-mu).T,\n",
        "                                                      np.dot(sigma_inv, X-mu))\n",
        "\n",
        "\n",
        "def confusion_matrix(actual, predicted):\n",
        "\n",
        "    # extract the different classes\n",
        "    classes = np.unique(actual)\n",
        "    # initialize the confusion matrix\n",
        "    confmat = np.zeros((len(classes), len(classes)))\n",
        "\n",
        "    # loop across the different combinations of actual / predicted classes\n",
        "    for i in range(len(classes)):\n",
        "        for j in range(len(classes)):\n",
        "            # count the number of instances in each combination of actual / predicted classes\n",
        "            confmat[i, j] = np.sum((actual == classes[i]) & (predicted == classes[j]))\n",
        "\n",
        "    return confmat\n",
        "\n",
        "\n",
        "def testing_bayes(test_x, test_y, means, covariances, logdets, priors,sigma_inv):\n",
        "    tp = 0\n",
        "    tn = 0\n",
        "    fp = 0\n",
        "    fn = 0\n",
        "    K = len(priors)  # Number of classes\n",
        "    print(K)\n",
        "    print(\"[INFO] Calculating accuracy and F1 score on test data\")\n",
        "    #print(means,logdets)\n",
        "    predicted_labels = np.zeros((test_y.shape[0], 1))\n",
        "    print(test_y.shape[0])\n",
        "    for i in range(test_y.shape[0]):\n",
        "        vals = []\n",
        "        for k in range(K):\n",
        "            vals.append(calc_log_posterior(test_x[i, :, :], means[k], covariances[k], priors[k], logdets[k],sigma_inv[k]))\n",
        "        predicted_labels[i] = np.argmax(vals)\n",
        "    conf_mat = confusion_matrix(test_y, predicted_labels)\n",
        "    print(conf_mat)\n",
        "\n",
        "    acc = np.sum(np.diag(conf_mat))/np.sum(conf_mat)\n",
        "    #f1 = tp/(tp+0.5*(fp+fn))\n",
        "    print(\"Accuracy = \", acc)\n",
        "    #print(\"F-1 Score = \", f1)\n",
        "\n",
        "    return\n",
        "\n",
        "\n",
        "def main():\n",
        "    #path = '/content/drive/MyDrive/PRNN/bloodmnist.npz'\n",
        "    path = \"../input/bloodmnist/bloodmnist.npz\" #path for rahul\n",
        "    train_x, train_y, test_x, test_y = process_data(path)\n",
        "    means, covariances, logdets, priors,sigma_inv = estimate_class_conditionals(train_x, train_y)\n",
        "    print(\"Ok\")\n",
        "    testing_bayes(test_x, test_y, means, covariances, logdets, priors,sigma_inv)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SLGEefWXheFy"
      },
      "outputs": [],
      "source": [
        "#Naive Bayes Gaussian for bloodmnist\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
        "\n",
        "data2 = np.load('../input/bloodmnist/bloodmnist.npz')\n",
        "print(data2.files)\n",
        "\n",
        "for key, value in data2.items():\n",
        "  print(key)\n",
        "  print(data2[key].shape)\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage.color import rgb2gray\n",
        "\n",
        "\n",
        "def process_data(path):\n",
        "    dataset = np.load(path)\n",
        "    train_x, train_y = np.reshape(\n",
        "        rgb2gray(dataset['train_images']), (11959, 784, 1)), dataset['train_labels']\n",
        "    val_x, val_y = np.reshape(rgb2gray(dataset['val_images']),\n",
        "                              (1712, 784, 1)), dataset['val_labels']\n",
        "    test_x, test_y = np.reshape(\n",
        "        rgb2gray(dataset['test_images']), (3421, 784, 1)), dataset['test_labels']\n",
        "\n",
        "    return train_x, train_y, test_x, test_y\n",
        "\n",
        "\n",
        "def estimate_class_conditionals(train_x, train_y):\n",
        "    n = train_x.shape[1]\n",
        "    mu0 = np.zeros((n, 1))\n",
        "    mu1 = np.zeros((n, 1))\n",
        "    mu2 = np.zeros((n, 1))\n",
        "    mu3 = np.zeros((n, 1))\n",
        "    mu4 = np.zeros((n, 1))\n",
        "    mu5 = np.zeros((n, 1))\n",
        "    mu6 = np.zeros((n, 1))\n",
        "    mu7 = np.zeros((n, 1))\n",
        "    sig0 = np.zeros((n, n), dtype=np.double)\n",
        "    sig1 = np.zeros((n, n), dtype=np.double)\n",
        "    sig2 = np.zeros((n, n), dtype=np.double)\n",
        "    sig3 = np.zeros((n, n), dtype=np.double)\n",
        "    sig4 = np.zeros((n, n), dtype=np.double)\n",
        "    sig5 = np.zeros((n, n), dtype=np.double)\n",
        "    sig6 = np.zeros((n, n), dtype=np.double)\n",
        "    sig7 = np.zeros((n, n), dtype=np.double)\n",
        "    n0, n1, n2, n3, n4, n5, n6, n7 = 0, 0, 0, 0, 0, 0, 0, 0\n",
        "    n = train_x.shape[0]\n",
        "\n",
        "    print(\"[INFO] Estimating class conditional densities\")\n",
        "\n",
        "    for i in range(train_y.shape[0]):\n",
        "        if(train_y[i] == 0):\n",
        "            mu0 += train_x[i]\n",
        "            n0 += 1\n",
        "        elif(train_y[i] == 1):\n",
        "            mu1 += train_x[i]\n",
        "            n1 += 1\n",
        "        elif(train_y[i] == 2):\n",
        "            mu2 += train_x[i]\n",
        "            n2 += 1\n",
        "        if(train_y[i] == 3):\n",
        "            mu3 += train_x[i]\n",
        "            n3 += 1\n",
        "        elif(train_y[i] == 4):\n",
        "            mu4 += train_x[i]\n",
        "            n4 += 1\n",
        "        elif(train_y[i] == 5):\n",
        "            mu5 += train_x[i]\n",
        "            n5 += 1\n",
        "        if(train_y[i] == 6):\n",
        "            mu6 += train_x[i]\n",
        "            n6 += 1\n",
        "        else:\n",
        "            mu7 += train_x[i]\n",
        "            n7 += 1\n",
        "\n",
        "    mu0 *= 1.0/n0\n",
        "    mu1 *= 1.0/n1\n",
        "    mu2 *= 1.0/n2\n",
        "    mu3 *= 1.0/n3\n",
        "    mu4 *= 1.0/n4\n",
        "    mu5 *= 1.0/n5\n",
        "    mu6 *= 1.0/n6\n",
        "    mu7 *= 1.0/n7\n",
        "    means = [mu0, mu1, mu2, mu3, mu4, mu5, mu6, mu7]\n",
        "    print(n0,n1,n2,n3,n4,n5,n6,n7)\n",
        "    for i in range(train_y.shape[0]):\n",
        "        if(train_y[i] == 0):\n",
        "            sig0 += np.outer(train_x[i]-mu0, (train_x[i]-mu0).T)\n",
        "        elif(train_y[i] == 1):\n",
        "            sig1 += np.outer(train_x[i]-mu1, (train_x[i]-mu1).T)\n",
        "        elif(train_y[i] == 2):\n",
        "            sig2 += np.outer(train_x[i]-mu2, (train_x[i]-mu2).T)\n",
        "        elif(train_y[i] == 3):\n",
        "            sig3 += np.outer(train_x[i]-mu3, (train_x[i]-mu3).T)\n",
        "        elif(train_y[i] == 4):\n",
        "            sig4 += np.outer(train_x[i]-mu4, (train_x[i]-mu4).T)\n",
        "        elif(train_y[i] == 5):\n",
        "            sig5 += np.outer(train_x[i]-mu5, (train_x[i]-mu5).T)\n",
        "        elif(train_y[i] == 6):\n",
        "            sig6 += np.outer(train_x[i]-mu6, (train_x[i]-mu6).T)\n",
        "        else:\n",
        "            sig7 += np.outer(train_x[i]-mu7, (train_x[i]-mu7).T)\n",
        "\n",
        "    sig0 *= 1.0/n0\n",
        "    sig1 *= 1.0/n1\n",
        "    sig2 *= 1.0/n2\n",
        "    sig3 *= 1.0/n3\n",
        "    sig4 *= 1.0/n4\n",
        "    sig5 *= 1.0/n5\n",
        "    sig6 *= 1.0/n6\n",
        "    sig7 *= 1.0/n7\n",
        "    covariances = [sig0, sig1, sig2, sig3, sig4, sig5, sig6, sig7]\n",
        "\n",
        "    p0 = n0/n\n",
        "    p1 = n1/n\n",
        "    p2 = n2/n\n",
        "    p3 = n3/n\n",
        "    p4 = n4/n\n",
        "    p5 = n5/n\n",
        "    p6 = n6/n\n",
        "    p7 = n7/n\n",
        "    priors = [p0, p1, p2, p3, p4, p5, p6, p7]\n",
        "\n",
        "    (sign0, logdet0) = np.linalg.slogdet(sig0)\n",
        "    (sign1, logdet1) = np.linalg.slogdet(sig1)\n",
        "    (sign2, logdet2) = np.linalg.slogdet(sig2)\n",
        "    (sign3, logdet3) = np.linalg.slogdet(sig3)\n",
        "    (sign4, logdet4) = np.linalg.slogdet(sig4)\n",
        "    (sign5, logdet5) = np.linalg.slogdet(sig5)\n",
        "    (sign6, logdet6) = np.linalg.slogdet(sig6)\n",
        "    (sign7, logdet7) = np.linalg.slogdet(sig7)\n",
        "    \n",
        "    sigma0_inv = np.linalg.pinv(sig0)\n",
        "    sigma1_inv = np.linalg.pinv(sig1)\n",
        "    sigma2_inv = np.linalg.pinv(sig2)\n",
        "    sigma3_inv = np.linalg.pinv(sig3)\n",
        "    sigma4_inv = np.linalg.pinv(sig4)\n",
        "    sigma5_inv = np.linalg.pinv(sig5)\n",
        "    sigma6_inv = np.linalg.pinv(sig6)\n",
        "    sigma7_inv = np.linalg.pinv(sig7)\n",
        "    sigma_inv = [sigma0_inv,sigma1_inv,sigma2_inv,sigma3_inv,sigma4_inv,sigma5_inv,sigma6_inv,sigma7_inv]\n",
        "    logdets = [logdet0, logdet1, logdet2, logdet3, logdet4, logdet5, logdet6, logdet7]\n",
        "\n",
        "    return means, covariances, logdets, priors, sigma_inv\n",
        "\n",
        "\n",
        "def calc_log_posterior(X, mu, sigma, prior, logdet,sigma_inv):\n",
        "    return np.log(prior) - 0.5*logdet - 0.5*np.dot((X-mu).T,\n",
        "                                                      np.dot(sigma_inv, X-mu))\n",
        "\n",
        "\n",
        "def confusion_matrix(actual, predicted):\n",
        "\n",
        "    # extract the different classes\n",
        "    classes = np.unique(actual)\n",
        "    # initialize the confusion matrix\n",
        "    confmat = np.zeros((len(classes), len(classes)))\n",
        "\n",
        "    # loop across the different combinations of actual / predicted classes\n",
        "    for i in range(len(classes)):\n",
        "        for j in range(len(classes)):\n",
        "            # count the number of instances in each combination of actual / predicted classes\n",
        "            confmat[i, j] = np.sum((actual == classes[i]) & (predicted == classes[j]))\n",
        "\n",
        "    return confmat\n",
        "\n",
        "\n",
        "def testing_bayes(test_x, test_y, means, covariances, logdets, priors,sigma_inv):\n",
        "    tp = 0\n",
        "    tn = 0\n",
        "    fp = 0\n",
        "    fn = 0\n",
        "    K = len(priors)  # Number of classes\n",
        "    print(K)\n",
        "    print(\"[INFO] Calculating accuracy and F1 score on test data\")\n",
        "    #print(means,logdets)\n",
        "    predicted_labels = np.zeros((test_y.shape[0], 1))\n",
        "    print(test_y.shape[0])\n",
        "    for i in range(test_y.shape[0]):\n",
        "        vals = []\n",
        "        for k in range(K):\n",
        "            vals.append(calc_log_posterior(test_x[i, :, :], means[k], covariances[k], priors[k], logdets[k],sigma_inv[k]))\n",
        "        predicted_labels[i] = np.argmax(vals)\n",
        "    conf_mat = confusion_matrix(test_y, predicted_labels)\n",
        "    print(conf_mat)\n",
        "\n",
        "    acc = np.sum(np.diag(conf_mat))/np.sum(conf_mat)\n",
        "    #f1 = tp/(tp+0.5*(fp+fn))\n",
        "    print(\"Accuracy = \", acc)\n",
        "    #print(\"F-1 Score = \", f1)\n",
        "\n",
        "    return\n",
        "\n",
        "\n",
        "def main():\n",
        "    #path = '/content/drive/MyDrive/PRNN/bloodmnist.npz'\n",
        "    path = \"../input/bloodmnist/bloodmnist.npz\" #path for rahul\n",
        "    train_x, train_y, test_x, test_y = process_data(path)\n",
        "    means, covariances, logdets, priors,sigma_inv = estimate_class_conditionals(train_x, train_y)\n",
        "    print(\"Ok\")\n",
        "    testing_bayes(test_x, test_y, means, covariances, logdets, priors,sigma_inv)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VO470TJBh0ts"
      },
      "outputs": [],
      "source": [
        "#Linear Classification Models on pneumoniamnist with L1,L2, Elastic Regularizers\n",
        "#Linear Classification With L2 Regularizer\n",
        "\n",
        "import numpy as np\n",
        "from numpy import load\n",
        "import pandas as pd\n",
        "from skimage.color import rgb2gray\n",
        "from matplotlib import pyplot\n",
        "import sklearn.metrics\n",
        "\n",
        "\n",
        "def initialize_with_zeros(dim):\n",
        "    w = np.zeros(shape=(dim, 1))\n",
        "    b = 0\n",
        "    return w, b\n",
        "\n",
        "def normalize(X):\n",
        "\n",
        "    X[:, 1:] = (X[:, 1:] - np.mean(X[:, 1:], axis=0)) / np.std(X[:, 1:], axis=0)\n",
        "\n",
        "    return X\n",
        "\n",
        "def propagate(w, b, X, Y):\n",
        "    # Find the number of training data\n",
        "    m = X.shape[1]\n",
        "\n",
        "\n",
        "    # Calculate the predicted output\n",
        "    A_reg = (np.dot(w.T, X) + b)\n",
        "    A=np.copy(A_reg)\n",
        "    A[A_reg>=0]=1\n",
        "    A[A_reg<0]=-1\n",
        "\n",
        "\n",
        "    # Calculate the cost function\n",
        "    cost = (np.sum(((A-Y).T@(A-Y)))+ 0.1 * np.sum(np.abs(w)))  / (2* m)\n",
        "\n",
        "    # Calculate the gradients\n",
        "    dw = 1 / m * (np.dot(X, (A - Y).T)+0.1*np.sign(w))\n",
        "    db = 1 / m * (np.sum(A - Y)+0.1*np.sign(b))\n",
        "\n",
        "    grads = {\"dw\": dw,\n",
        "             \"db\": db}\n",
        "    return grads, cost\n",
        "\n",
        "\n",
        "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost=False):\n",
        "    costs = []\n",
        "\n",
        "    # propagate function will run for a number of iterations\n",
        "    for i in range(num_iterations):\n",
        "        grads, cost = propagate(w, b, X, Y)\n",
        "        dw = grads[\"dw\"]\n",
        "        db = grads[\"db\"]\n",
        "\n",
        "        # Updating w and b by deducting the dw\n",
        "        # and db times learning rate from the previous\n",
        "        # w and b\n",
        "\n",
        "        w = w - learning_rate * dw\n",
        "        b = b - learning_rate * db\n",
        "\n",
        "        # Record the cost function value for each 100 iterations\n",
        "        if i % 100 == 0:\n",
        "            costs.append(cost)\n",
        "\n",
        "    # The final updated parameters\n",
        "    params = {\"w\": w,\n",
        "              \"b\": b}\n",
        "\n",
        "    # The final updated gradients\n",
        "    grads = {\"dw\": dw,\n",
        "             \"db\": db}\n",
        "\n",
        "    return params, grads, costs\n",
        "\n",
        "\n",
        "def predict(w, b, X):\n",
        "    m = X.shape[1]\n",
        "\n",
        "    # Initializing an aray of zeros which has a size of the input\n",
        "    # These zeros will be replaced by the predicted output\n",
        "    Y_prediction = np.zeros((1, m))\n",
        "\n",
        "    # w = w.reshape(X.shape[0], 1)\n",
        "\n",
        "    # Calculating the predicted output using the Formula 1\n",
        "    # This will return the values from 0 to 1\n",
        "    A = (np.dot(w.T, X) + b)\n",
        "\n",
        "    # Iterating through A and predict an 1 if the value of A\n",
        "    # is greater than 0.5 and zero otherwise\n",
        "\n",
        "    Y_prediction[A<0]=-1\n",
        "    Y_prediction[A>0] =1\n",
        "    return Y_prediction\n",
        "\n",
        "\n",
        "def model(X_train, Y_train, X_test, Y_test, num_iterations, learning_rate):\n",
        "\n",
        "    X_train=normalize(X_train)\n",
        "    X_test=normalize(X_test)\n",
        "\n",
        "    # Initializing the w and b as zeros\n",
        "    w, b = initialize_with_zeros(X_train.shape[0])\n",
        "    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost=True)\n",
        "    w = parameters[\"w\"]\n",
        "    b = parameters[\"b\"]\n",
        "\n",
        "    # Predicting the output for both test and training set\n",
        "    Y_prediction_test = predict(w, b, X_test)\n",
        "    Y_prediction_train = predict(w, b, X_train)\n",
        "\n",
        "    print(\"Linear Classification With L2 Regularizer\")\n",
        "    # Calculating the training and test set accuracy by comparing\n",
        "    # the predicted output and the original output\n",
        "    num_correct = np.sum(Y_prediction_train == Y_train)\n",
        "    train_accuracy = (num_correct / X_train.shape[1]) * 100\n",
        "    print(\"Train accuracy :  {} %\".format(train_accuracy))\n",
        "    num_correct = np.sum(Y_prediction_test == Y_test)\n",
        "    test_accuracy = (num_correct / X_test.shape[1]) * 100\n",
        "    print(\"Test accuracy :  {} %\".format(test_accuracy))\n",
        "    \n",
        "    F1= sklearn.metrics.f1_score(Y_test[0, :], Y_prediction_test[0, :])\n",
        "    AUC = sklearn.metrics.roc_auc_score(Y_test[0, :], Y_prediction_test[0, :])\n",
        "\n",
        "    print('F1 score : {}'.format(F1))\n",
        "    print('AUC : {}'.format(AUC))\n",
        "    \n",
        "    d = {\"costs\": costs,\n",
        "         \"Y_prediction_test\": Y_prediction_test,\n",
        "         \"Y_prediction_train\": Y_prediction_train,\n",
        "         \"w\": w,\n",
        "         \"b\": b,\n",
        "         \"learning_rate\": learning_rate,\n",
        "         \"num_iterations\": num_iterations}\n",
        "\n",
        "    return d\n",
        "\n",
        "\n",
        "def main():\n",
        "    data = load('../input/d/datasets/pogurahulraju/pneumoniamnist/pneumoniamnist.npz')\n",
        "\n",
        "    \n",
        "    flatten_data = {}\n",
        "    n1, l, b = data['train_images'].shape\n",
        "    n2, l, b = data['test_images'].shape\n",
        "    flatten_data['train_images'] = data['train_images'].reshape(n1, l * b)\n",
        "    flatten_data['test_images'] = data['test_images'].reshape(n2, l * b)\n",
        "    #print(flatten_data['train_images'].shape)\n",
        "    #print(flatten_data['test_images'].shape)\n",
        "\n",
        "    l_train = data['train_labels']\n",
        "    l_test = data['test_labels']\n",
        "\n",
        "\n",
        "    X_train = pd.DataFrame(flatten_data['train_images'])\n",
        "    X_test = pd.DataFrame(flatten_data['test_images'])\n",
        "    y_train = pd.DataFrame(l_train)\n",
        "    y_test = pd.DataFrame(l_test)\n",
        "\n",
        "\n",
        "    X_train = np.array(X_train.T)/255\n",
        "    y_train = np.array(y_train.T)\n",
        "    X_test = np.array(X_test.T)/255\n",
        "    y_test = np.array(y_test.T)\n",
        "\n",
        "    y_train = y_train.astype(\"int\")\n",
        "    y_test = y_test.astype(\"int\")\n",
        "\n",
        "    y_train[y_train==0]=(0-1)\n",
        "    y_test[y_test==0]=(0-1)\n",
        "\n",
        "\n",
        "    d = model(X_train, y_train, X_test, y_test, num_iterations=50, learning_rate=0.2)\n",
        "\n",
        "    return\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "\n",
        "#Linear Classification With L1 Regularizer\n",
        "import numpy as np\n",
        "from numpy import load\n",
        "import pandas as pd\n",
        "from skimage.color import rgb2gray\n",
        "from matplotlib import pyplot\n",
        "\n",
        "\n",
        "def initialize_with_zeros(dim):\n",
        "    w = np.zeros(shape=(dim, 1))\n",
        "    b = 0\n",
        "    return w, b\n",
        "\n",
        "def normalize(X):\n",
        "\n",
        "    X[:, 1:] = (X[:, 1:] - np.mean(X[:, 1:], axis=0)) / np.std(X[:, 1:], axis=0)\n",
        "\n",
        "    return X\n",
        "\n",
        "def propagate(w, b, X, Y):\n",
        "    # Find the number of training data\n",
        "    m = X.shape[1]\n",
        "\n",
        "\n",
        "    # Calculate the predicted output\n",
        "    A_reg = (np.dot(w.T, X) + b)\n",
        "    A=np.copy(A_reg)\n",
        "    A[A_reg>=0]=1\n",
        "    A[A_reg<0]=-1\n",
        "\n",
        "\n",
        "    # Calculate the cost function\n",
        "    cost = (np.sum(((A-Y).T@(A-Y))) +  0.1 * (np.sum(np.dot(w.T,w))+b**2))/ (2* m)\n",
        "\n",
        "    # Calculate the gradients\n",
        "    dw = 1 / m * (np.dot(X, (A - Y).T)+0.1*w)\n",
        "    db = 1 / m * (np.sum(A - Y)+0.1*b)\n",
        "\n",
        "    grads = {\"dw\": dw,\n",
        "             \"db\": db}\n",
        "    return grads, cost\n",
        "\n",
        "\n",
        "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost=False):\n",
        "    costs = []\n",
        "\n",
        "    # propagate function will run for a number of iterations\n",
        "    for i in range(num_iterations):\n",
        "        grads, cost = propagate(w, b, X, Y)\n",
        "        dw = grads[\"dw\"]\n",
        "        db = grads[\"db\"]\n",
        "\n",
        "        # Updating w and b by deducting the dw\n",
        "        # and db times learning rate from the previous\n",
        "        # w and b\n",
        "\n",
        "        w = w - learning_rate * dw\n",
        "        b = b - learning_rate * db\n",
        "\n",
        "        # Record the cost function value for each 100 iterations\n",
        "        if i % 100 == 0:\n",
        "            costs.append(cost)\n",
        "\n",
        "    # The final updated parameters\n",
        "    params = {\"w\": w,\n",
        "              \"b\": b}\n",
        "\n",
        "    # The final updated gradients\n",
        "    grads = {\"dw\": dw,\n",
        "             \"db\": db}\n",
        "\n",
        "    return params, grads, costs\n",
        "\n",
        "\n",
        "def predict(w, b, X):\n",
        "    m = X.shape[1]\n",
        "\n",
        "    # Initializing an aray of zeros which has a size of the input\n",
        "    # These zeros will be replaced by the predicted output\n",
        "    Y_prediction = np.zeros((1, m))\n",
        "\n",
        "    # w = w.reshape(X.shape[0], 1)\n",
        "\n",
        "    # Calculating the predicted output using the Formula 1\n",
        "    # This will return the values from 0 to 1\n",
        "    A = (np.dot(w.T, X) + b)\n",
        "\n",
        "    # Iterating through A and predict an 1 if the value of A\n",
        "    # is greater than 0.5 and zero otherwise\n",
        "\n",
        "    Y_prediction[A<0]=-1\n",
        "    Y_prediction[A>0] =1\n",
        "    return Y_prediction\n",
        "\n",
        "\n",
        "def model(X_train, Y_train, X_test, Y_test, num_iterations, learning_rate):\n",
        "\n",
        "    X_train=normalize(X_train)\n",
        "    X_test=normalize(X_test)\n",
        "\n",
        "    # Initializing the w and b as zeros\n",
        "    w, b = initialize_with_zeros(X_train.shape[0])\n",
        "    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost=True)\n",
        "    w = parameters[\"w\"]\n",
        "    b = parameters[\"b\"]\n",
        "\n",
        "    # Predicting the output for both test and training set\n",
        "    Y_prediction_test = predict(w, b, X_test)\n",
        "    Y_prediction_train = predict(w, b, X_train)\n",
        "\n",
        "    print(\"Linear Classification With L1 Regularizer\")    \n",
        "    # Calculating the training and test set accuracy by comparing\n",
        "    # the predicted output and the original output\n",
        "    num_correct = np.sum(Y_prediction_train == Y_train)\n",
        "    train_accuracy = (num_correct / X_train.shape[1]) * 100\n",
        "    print(\"Train accuracy :  {} %\".format(train_accuracy))\n",
        "    num_correct = np.sum(Y_prediction_test == Y_test)\n",
        "    test_accuracy = (num_correct / X_test.shape[1]) * 100\n",
        "    print(\"Test accuracy :  {} %\".format(test_accuracy))\n",
        "    \n",
        "    F1= sklearn.metrics.f1_score(Y_test[0, :], Y_prediction_test[0, :])\n",
        "    AUC = sklearn.metrics.roc_auc_score(Y_test[0, :], Y_prediction_test[0, :])\n",
        "    \n",
        "    print('F1 score : {}'.format(F1))\n",
        "    print('AUC : {}'.format(AUC))\n",
        "\n",
        "    d = {\"costs\": costs,\n",
        "         \"Y_prediction_test\": Y_prediction_test,\n",
        "         \"Y_prediction_train\": Y_prediction_train,\n",
        "         \"w\": w,\n",
        "         \"b\": b,\n",
        "         \"learning_rate\": learning_rate,\n",
        "         \"num_iterations\": num_iterations}\n",
        "\n",
        "    return d\n",
        "\n",
        "\n",
        "def main():\n",
        "    data = load('../input/d/datasets/pogurahulraju/pneumoniamnist/pneumoniamnist.npz')\n",
        "\n",
        "    flatten_data = {}\n",
        "    n1, l, b = data['train_images'].shape\n",
        "    n2, l, b = data['test_images'].shape\n",
        "    flatten_data['train_images'] = data['train_images'].reshape(n1, l * b)\n",
        "    flatten_data['test_images'] = data['test_images'].reshape(n2, l * b)\n",
        "   \n",
        "\n",
        "    l_train = data['train_labels']\n",
        "    l_test = data['test_labels']\n",
        "\n",
        "\n",
        "    X_train = pd.DataFrame(flatten_data['train_images'])\n",
        "    X_test = pd.DataFrame(flatten_data['test_images'])\n",
        "    y_train = pd.DataFrame(l_train)\n",
        "    y_test = pd.DataFrame(l_test)\n",
        "\n",
        "\n",
        "    X_train = np.array(X_train.T)/255\n",
        "    y_train = np.array(y_train.T)\n",
        "    X_test = np.array(X_test.T)/255\n",
        "    y_test = np.array(y_test.T)\n",
        "\n",
        "    y_train = y_train.astype(\"int\")\n",
        "    y_test = y_test.astype(\"int\")\n",
        "\n",
        "    y_train[y_train==0]=(0-1)\n",
        "    y_test[y_test==0]=(0-1)\n",
        "\n",
        "\n",
        "    d = model(X_train, y_train, X_test, y_test, num_iterations=50, learning_rate=0.2)\n",
        "\n",
        "    return\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "\n",
        "#Linear Regression With Elastic Regularizer\n",
        "\n",
        "import numpy as np\n",
        "from numpy import load\n",
        "import pandas as pd\n",
        "from skimage.color import rgb2gray\n",
        "from matplotlib import pyplot\n",
        "\n",
        "\n",
        "def initialize_with_zeros(dim):\n",
        "    w = np.zeros(shape=(dim, 1))\n",
        "    b = 0\n",
        "    return w, b\n",
        "\n",
        "def normalize(X):\n",
        "\n",
        "    X[:, 1:] = (X[:, 1:] - np.mean(X[:, 1:], axis=0)) / np.std(X[:, 1:], axis=0)\n",
        "\n",
        "    return X\n",
        "\n",
        "def propagate(w, b, X, Y):\n",
        "    # Find the number of training data\n",
        "    m = X.shape[1]\n",
        "\n",
        "\n",
        "    # Calculate the predicted output\n",
        "    A_reg = (np.dot(w.T, X) + b)\n",
        "    A=np.copy(A_reg)\n",
        "    A[A_reg>=0]=1\n",
        "    A[A_reg<0]=-1\n",
        "\n",
        "\n",
        "    # Calculate the cost function\n",
        "    cost = (np.sum(((A-Y).T@(A-Y))) + 0.1 * (np.sum(np.dot(w.T,w))+b**2) + 0.1 * np.sum(np.abs(w))) / (2* m)\n",
        "\n",
        "    # Calculate the gradients\n",
        "    dw = 1 / m * (np.dot(X, (A - Y).T)+0.1*w+0.1*np.sign(w))\n",
        "    db = 1 / m * (np.sum(A - Y)+0.1*b+0.1*np.sign(b))\n",
        "\n",
        "    grads = {\"dw\": dw,\n",
        "             \"db\": db}\n",
        "    return grads, cost\n",
        "\n",
        "\n",
        "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost=False):\n",
        "    costs = []\n",
        "\n",
        "    # propagate function will run for a number of iterations\n",
        "    for i in range(num_iterations):\n",
        "        grads, cost = propagate(w, b, X, Y)\n",
        "        dw = grads[\"dw\"]\n",
        "        db = grads[\"db\"]\n",
        "\n",
        "        # Updating w and b by deducting the dw\n",
        "        # and db times learning rate from the previous\n",
        "        # w and b\n",
        "\n",
        "        w = w - learning_rate * dw\n",
        "        b = b - learning_rate * db\n",
        "\n",
        "        # Record the cost function value for each 100 iterations\n",
        "        if i % 100 == 0:\n",
        "            costs.append(cost)\n",
        "\n",
        "    # The final updated parameters\n",
        "    params = {\"w\": w,\n",
        "              \"b\": b}\n",
        "\n",
        "    # The final updated gradients\n",
        "    grads = {\"dw\": dw,\n",
        "             \"db\": db}\n",
        "\n",
        "    return params, grads, costs\n",
        "\n",
        "\n",
        "def predict(w, b, X):\n",
        "    m = X.shape[1]\n",
        "\n",
        "    # Initializing an aray of zeros which has a size of the input\n",
        "    # These zeros will be replaced by the predicted output\n",
        "    Y_prediction = np.zeros((1, m))\n",
        "\n",
        "    # w = w.reshape(X.shape[0], 1)\n",
        "\n",
        "    # Calculating the predicted output using the Formula 1\n",
        "    # This will return the values from 0 to 1\n",
        "    A = (np.dot(w.T, X) + b)\n",
        "\n",
        "    # Iterating through A and predict an 1 if the value of A\n",
        "    # is greater than 0.5 and zero otherwise\n",
        "\n",
        "    Y_prediction[A<0]=-1\n",
        "    Y_prediction[A>0] =1\n",
        "    return Y_prediction\n",
        "\n",
        "\n",
        "def model(X_train, Y_train, X_test, Y_test, num_iterations, learning_rate):\n",
        "\n",
        "    X_train=normalize(X_train)\n",
        "    X_test=normalize(X_test)\n",
        "\n",
        "    # Initializing the w and b as zeros\n",
        "    w, b = initialize_with_zeros(X_train.shape[0])\n",
        "    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost=True)\n",
        "    w = parameters[\"w\"]\n",
        "    b = parameters[\"b\"]\n",
        "\n",
        "    # Predicting the output for both test and training set\n",
        "    Y_prediction_test = predict(w, b, X_test)\n",
        "    Y_prediction_train = predict(w, b, X_train)\n",
        "    \n",
        "    print(\"Linear Classification With Elastic Regularizer\")\n",
        "    \n",
        "    # Calculating the training and test set accuracy by comparing\n",
        "    # the predicted output and the original output\n",
        "    num_correct = np.sum(Y_prediction_train == Y_train)\n",
        "    train_accuracy = (num_correct / X_train.shape[1]) * 100\n",
        "    print(\"Train accuracy :  {} %\".format(train_accuracy))\n",
        "    num_correct = np.sum(Y_prediction_test == Y_test)\n",
        "    test_accuracy = (num_correct / X_test.shape[1]) * 100\n",
        "    print(\"Test accuracy :  {} %\".format(test_accuracy))\n",
        "    \n",
        "    F1= sklearn.metrics.f1_score(Y_test[0, :], Y_prediction_test[0, :])\n",
        "    AUC = sklearn.metrics.roc_auc_score(Y_test[0, :], Y_prediction_test[0, :])\n",
        "\n",
        "    print('F1 score : {}'.format(F1))\n",
        "    print('AUC : {}'.format(AUC))\n",
        "\n",
        "    d = {\"costs\": costs,\n",
        "         \"Y_prediction_test\": Y_prediction_test,\n",
        "         \"Y_prediction_train\": Y_prediction_train,\n",
        "         \"w\": w,\n",
        "         \"b\": b,\n",
        "         \"learning_rate\": learning_rate,\n",
        "         \"num_iterations\": num_iterations}\n",
        "\n",
        "    return d\n",
        "\n",
        "\n",
        "def main():\n",
        "    data = load('../input/d/datasets/pogurahulraju/pneumoniamnist/pneumoniamnist.npz')\n",
        "    \n",
        "    flatten_data = {}\n",
        "    n1, l, b = data['train_images'].shape\n",
        "    n2, l, b = data['test_images'].shape\n",
        "    flatten_data['train_images'] = data['train_images'].reshape(n1, l * b)\n",
        "    flatten_data['test_images'] = data['test_images'].reshape(n2, l * b)\n",
        "\n",
        "    l_train = data['train_labels']\n",
        "    l_test = data['test_labels']\n",
        "\n",
        "\n",
        "    X_train = pd.DataFrame(flatten_data['train_images'])\n",
        "    X_test = pd.DataFrame(flatten_data['test_images'])\n",
        "    y_train = pd.DataFrame(l_train)\n",
        "    y_test = pd.DataFrame(l_test)\n",
        "\n",
        "\n",
        "    X_train = np.array(X_train.T)/255\n",
        "    y_train = np.array(y_train.T)\n",
        "    X_test = np.array(X_test.T)/255\n",
        "    y_test = np.array(y_test.T)\n",
        "\n",
        "    y_train = y_train.astype(\"int\")\n",
        "    y_test = y_test.astype(\"int\")\n",
        "\n",
        "    y_train[y_train==0]=(0-1)\n",
        "    y_test[y_test==0]=(0-1)\n",
        "\n",
        "\n",
        "    d = model(X_train, y_train, X_test, y_test, num_iterations=50, learning_rate=0.2)\n",
        "\n",
        "    return\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tyxc1C5FhVW0"
      },
      "outputs": [],
      "source": [
        "#K Nearest Neighbours, Parzen window estimation on pneumoniamnist\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "\"\"\"\n",
        "Evaluation Metrics for Classification\n",
        "\n",
        "Confusion Matrix, Accuracy, F1 Score and AUC\n",
        "\"\"\"\n",
        "\n",
        "# Confusion Matrix\n",
        "def confMatrix(test_y, test_r, classes) :\n",
        "    conf_mat = pd.DataFrame(np.zeros((classes.shape[0], classes.shape[0])), classes, classes)\n",
        "    for i in range(test_y.shape[0]):\n",
        "        conf_mat[test_r[i]][test_y[i]] += 1\n",
        "    return conf_mat\n",
        "\n",
        "# Accuracy\n",
        "def accuracy(conf_mat) :\n",
        "    return np.trace(conf_mat)/conf_mat.values.sum()\n",
        "\n",
        "# F1 Score\n",
        "def f1Score(conf_mat) :\n",
        "    return pd.DataFrame(2*np.diag(conf_mat), conf_mat.index, [\"f1-score\"]).div(pd.DataFrame(conf_mat.sum(axis = 0) + conf_mat.sum(axis = 1), columns=[\"f1-score\"]))\n",
        "\n",
        "\"\"\"\n",
        "Distance Functions\n",
        "\n",
        "Returns the distance between the two objects\n",
        "\"\"\"\n",
        "# Euclidean Distance Function\n",
        "def eucDist(a, b):\n",
        "    return np.linalg.norm(a - b)\n",
        "    \n",
        "# Manhattan Distance Function\n",
        "def manDist(a, b):\n",
        "    return np.sum(np.abs(a - b))\n",
        "\n",
        "\"\"\"\n",
        "Nearest Neighbours Classifiers\n",
        "\n",
        "Returns the probabilities of the labels of the test dataset from the nearest neighbour classfication\n",
        "using the training data\n",
        "\"\"\"\n",
        "\n",
        "# Nearest Neighbour Classification\n",
        "def NNClassifier(trainX, trainY, testX, distFunc = eucDist):\n",
        "    minDist = np.inf\n",
        "    argMin = -1\n",
        "    testR = np.zeros(testX.shape[0])\n",
        "    for i in range(testX.shape[0]):\n",
        "        for j in range(trainX.shape[0]):\n",
        "            Dist = distFunc(trainX[j], testX[i])\n",
        "            if minDist > Dist:\n",
        "                minDist = Dist\n",
        "                argMin = j\n",
        "        testR[i] = trainY[argMin]\n",
        "    return testR\n",
        "\n",
        "# K Nearest Neighbour Classification\n",
        "def kNNClassifier(trainX, trainY, testX, nClasses, k = 3, distFunc = eucDist):\n",
        "    dist = np.zeros((trainX.shape[0],2))\n",
        "    probR = np.zeros((testX.shape[0], nClasses))\n",
        "    for i in range(testX.shape[0]):\n",
        "        for j in range(trainX.shape[0]):\n",
        "            dist[j][0] = distFunc(testX[i], trainX[j])\n",
        "            dist[j][1] = j\n",
        "        dist = dist[np.argsort(dist[:, 0])]\n",
        "        for j in range(k):\n",
        "            probR[i][trainY[int(dist[j][1])]] += 1/k\n",
        "    return probR\n",
        "\n",
        "\"\"\"\n",
        "Window Functions\n",
        "\n",
        "Returns the weight of the given object within the window function\n",
        "\"\"\"\n",
        "# HyperCube Window Function\n",
        "def HCWindow(x, a, h):\n",
        "    for i in range(a.shape[0]):\n",
        "        if abs(a[i] - x[i]) > h/2:\n",
        "            return 0\n",
        "    return 1\n",
        "\n",
        "# Gaussian Window Function\n",
        "def GWindow(x, a, h):\n",
        "    return np.exp(-eucDist(x,a)/(2*h*h))\n",
        "\n",
        "\"\"\"\n",
        "Parzen Window Classifiers\n",
        "\n",
        "Returns the probabilities of the labels for the test dataset from the Parzen Window Classifier\n",
        "using the training Data\n",
        "\"\"\"\n",
        "def PWClassifier(trainX, trainY, testX, nClasses, h = 0.3, windFunc = GWindow):\n",
        "    cClass = np.zeros(nClasses)\n",
        "    for j in trainY:\n",
        "        cClass[j] += 1\n",
        "    \n",
        "    probDensR = np.zeros((testX.shape[0], nClasses))\n",
        "    for i in range(testX.shape[0]):\n",
        "        for j in range(trainX.shape[0]):\n",
        "            probDensR[i][trainY[j]] += windFunc(testX[i], trainX[j], h)/cClass[trainY[j]]\n",
        "    \n",
        "    return probDensR\n",
        "\n",
        "pnuData = np.load(\"../input/classification-dataset/pneumoniamnist.npz\")\n",
        "bldData = np.load(\"../input/classification-dataset/bloodmnist.npz\")\n",
        "\n",
        "pnuTrShp = pnuData['train_images'].shape\n",
        "pnuTrainX = pnuData['train_images'].reshape(pnuTrShp[0], pnuTrShp[1]*pnuTrShp[2])\n",
        "pnuTrainY = pnuData['train_labels']\n",
        "\n",
        "pnuTeShp = pnuData['test_images'].shape\n",
        "pnuTestX = pnuData['test_images'].reshape(pnuTeShp[0], pnuTeShp[1]*pnuTeShp[2])\n",
        "pnuTestY = pnuData['test_labels']\n",
        "\n",
        "pnuTrainX = pnuTrainX/255\n",
        "pnuTestX = pnuTestX/255\n",
        "\n",
        "pnuClasses = np.union1d(pnuTrainY, pnuTestY)\n",
        "\n",
        "pnukNNProbR = kNNClassifier(pnuTrainX, pnuTrainY, pnuTestX, len(pnuClasses))\n",
        "pnukNNTestR = pnukNNProbR.argmax(axis = 1)\n",
        "\n",
        "pnukNNCM = confMatrix(pnuTestY, pnukNNTestR, pnuClasses)\n",
        "print(\"pnu kNN Accuracy:\", accuracy(pnukNNCM))\n",
        "print(\"pnu kNN F1 Score:\", f1Score(pnukNNCM))\n",
        "print(\"pnu kNN AUC Score:\", roc_auc_score(pnukNNTestR, pnuTestY))\n",
        "\n",
        "\"\"\"Accuracy is around 83 percent\"\"\"\n",
        "\n",
        "pnuPWProbR = PWClassifier(pnuTrainX, pnuTrainY, pnuTestX, len(pnuClasses))\n",
        "pnuPWTestR = pnuPWProbR.argmax(axis = 1)\n",
        "\n",
        "pnuPWCM = confMatrix(pnuTestY, pnuPWTestR, pnuClasses)\n",
        "print(\"pnu PW Accuracy:\", accuracy(pnuPWCM))\n",
        "print(\"pnu PW F1 Score:\", f1Score(pnuPWCM))\n",
        "print(\"pnu PW AUC Score:\", roc_auc_score(pnuPWTestR, pnuTestY))\n",
        "\n",
        "\"\"\"Accuracy is around 47 percent\"\"\"\n",
        "\n",
        "rgb2gray = [0.299, 0.587, 0.114]\n",
        "\n",
        "bldTrShp = bldData['train_images'].shape\n",
        "bldTrXC = bldData['train_images'].reshape(bldTrShp[0], bldTrShp[1]*bldTrShp[2]*bldTrShp[3])\n",
        "bldTrXG = np.average(bldData['train_images'], axis = 3, weights = rgb2gray).reshape(bldTrShp[0], bldTrShp[1]*bldTrShp[2])\n",
        "bldTrY = bldData['train_labels']\n",
        "\n",
        "bldTeShp = bldData['test_images'].shape\n",
        "bldTeXC = bldData['test_images'].reshape(bldTeShp[0], bldTeShp[1]*bldTeShp[2]*bldTeShp[3])\n",
        "bldTeXG = np.average(bldData['test_images'], axis = 3, weights = rgb2gray).reshape(bldTeShp[0], bldTeShp[1]*bldTeShp[2])\n",
        "bldTeY = bldData['test_labels']\n",
        "\n",
        "bldTrXC = bldTrXC/255\n",
        "bldTrXG = bldTrXG/255\n",
        "\n",
        "bldTeXC = bldTeXC/255\n",
        "bldTeXG = bldTeXG/255\n",
        "\n",
        "bldClasses = np.union1d(bldTrY, bldTeY)\n",
        "\n",
        "bldkNNProbR = kNNClassifier(bldTrXG, bldTrY, bldTeXG, len(bldClasses))\n",
        "bldkNNTestR = bldkNNProbR.argmax(axis = 1)\n",
        "\n",
        "bldkNNCM = confMatrix(bldTeY, bldkNNTestR, bldClasses)\n",
        "print(\"bld kNN Accuracy:\", accuracy(bldkNNCM))\n",
        "print(\"bld kNN F1 Score:\", f1Score(bldkNNCM))\n",
        "print(\"bld kNN AUC Score:\", roc_auc_score(bldkNNProbR, bldTestY))\n",
        "\n",
        "\"\"\"Accuracy with rgb = around 71 percent\n",
        "Accuracy with grayscale = around 67 percent\n",
        "\"\"\"\n",
        "\n",
        "bldPWProbR = PWClassifier(bldTrXG, bldTrY, bldTeXG, len(bldClasses), 1) \n",
        "bldPWTestR = bldPWProbR.argmax(axis = 1)\n",
        "\n",
        "bldPWCM = confMatrix(bldTeY, bldPWTestR, bldClasses)\n",
        "print(\"bld PW Accuracy:\", accuracy(bldPWCM))\n",
        "print(\"bld PW F1 Score:\", f1Score(bldPWCM))\n",
        "print(\"bld PW AUC Score:\", roc_auc_score(bldPWProbR, bldTestY))\n",
        "\n",
        "\"\"\"Accuracy with grayscale = around 53.4 percent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VHXixXlTiO-p"
      },
      "outputs": [],
      "source": [
        "#Linear Regression with L1,L2,elastic regularization on vegetable dataset, \n",
        "#polynomial regression on vegetable dataset, logistic regression on pneumonimnist and bloodmnist\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
        "\n",
        "data = np.load('../input/vegetables/object_localization.npy',encoding = 'latin1',allow_pickle=True)\n",
        "print(data.shape)\n",
        "X = data[:,0]\n",
        "Y = data[:,1:3]\n",
        "print(X.shape)\n",
        "print(Y.shape)\n",
        "\n",
        "# Commented out IPython magic to ensure Python compatibility.\n",
        "# %matplotlib inline\n",
        "x=[]\n",
        "x1 = []\n",
        "for i in X:\n",
        "    x.append(i)\n",
        "    x1.append(i.flatten()/255)\n",
        "X_input = np.array(x)\n",
        "X_list = np.array(x1)\n",
        "print(X_input.shape)\n",
        "print(X_list.shape)\n",
        "\n",
        "x=[]\n",
        "for i in Y:\n",
        "    x.append(i[1])\n",
        "Y_coord = np.array(x)\n",
        "print(Y_coord.shape)\n",
        "\n",
        "import cv2\n",
        "from matplotlib import pyplot as plt\n",
        "def draw_rect(im, cords, color = None):\n",
        "    im = im.copy()\n",
        "    \n",
        "    \n",
        "    if not color:\n",
        "        color = [255,0,0]\n",
        "    pt1 = int(cords[0]),int(cords[1])\n",
        "    pt2 = int(cords[2]),int(cords[3])\n",
        "    \n",
        "    im = cv2.rectangle(im, pt1, pt2, color, int(max(im.shape[:2])/200),)\n",
        "    plt.imshow(im)\n",
        "    plt.show()\n",
        "    return \n",
        "draw_rect(X_input[1],Y_coord[1],)\n",
        "\n",
        "#Code to split the data randomly in 9:1 ratio\n",
        "test_indices = np.random.choice(X_list.shape[0], size = X_list.shape[0]//10, replace = False)\n",
        "train_indices = list(filter(lambda x : x not in test_indices, range(X_list.shape[0])))\n",
        "X_test = X_list[test_indices]\n",
        "X_train = X_list[train_indices]\n",
        "Y_test = Y_coord[test_indices]\n",
        "Y_train = Y_coord[train_indices]\n",
        "\n",
        "print(X_test.shape, X_train.shape)\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import SGDRegressor\n",
        "\n",
        "reg = LinearRegression().fit(X_train, Y_train)\n",
        "\n",
        "Z_test = reg.predict(X_test)\n",
        "Z_train = reg.predict(X_train)\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "print(mean_squared_error(Z_test,Y_test))\n",
        "\n",
        "draw_rect(X_input[test_indices[5]], Z_test[5],)\n",
        "\n",
        "def linearRegression(X, Y, learning_rate, iterations) :\n",
        "    X = np.c_[X, np.ones(X.shape[0])]\n",
        "    n = X.shape[0]\n",
        "    W = np.zeros((X.shape[1], Y.shape[1]))\n",
        "    C = []\n",
        "    \n",
        "    for _ in range(iterations):\n",
        "        Y_pred = np.dot(X, W)\n",
        "        c = np.mean(np.square(Y_pred - Y))/2\n",
        "        dW = np.dot(X.T, Y_pred - Y)/n\n",
        "        #print(dW.shape)\n",
        "        W = W - learning_rate*dW\n",
        "        C.append(c)\n",
        "    \n",
        "    return W, C\n",
        "\n",
        "\n",
        "def linearRegressionSGD(X, Y, learning_rate, iterations, batch_size) :\n",
        "    X = np.c_[X, np.ones(X.shape[0])]\n",
        "    n = X.shape[0]\n",
        "    W = np.zeros((X.shape[1], Y.shape[1]))\n",
        "    C = []\n",
        "    X1 = []\n",
        "    Y1 = []\n",
        "    n1 = n - n%batch_size\n",
        "    for i in range(0,n1,batch_size):\n",
        "        X1.append(X[i:i+batch_size])\n",
        "        Y1.append(Y[i:i+batch_size])\n",
        "    k = len(X1)\n",
        "    for _ in range(iterations):\n",
        "        c = 0\n",
        "        for i in range(k):\n",
        "            Y_pred = np.dot(X1[i], W)\n",
        "            c += np.mean(np.square(Y_pred - Y1[i]))/2\n",
        "            dW = np.dot(X1[i].T, Y_pred - Y1[i])/batch_size\n",
        "            #print(dW.shape)\n",
        "            W = W - learning_rate*dW\n",
        "        print(c/n*batch_size)\n",
        "        C.append(c/n*batch_size)\n",
        "        \n",
        "    return W, C\n",
        "\n",
        "def linearRegressionL2(X, Y, learning_rate, iterations,alpha,batch_size) :\n",
        "    X = np.c_[X, np.ones(X.shape[0])]\n",
        "    n = X.shape[0]\n",
        "    W = np.zeros((X.shape[1], Y.shape[1]))\n",
        "    C = []\n",
        "    \n",
        "    for _ in range(iterations):\n",
        "        c=0\n",
        "        for i in range(0,n,batch_size):\n",
        "            if i+batch_size < n:\n",
        "                Y_pred = np.dot(X[i:i+batch_size], W)\n",
        "                c += np.mean(np.square(Y_pred - Y[i:i+batch_size]))/2 + alpha*np.mean(np.square(W))/2\n",
        "                dW = np.dot(X[i:i+batch_size].T, Y_pred - Y[i:i+batch_size])/batch_size\n",
        "            else:\n",
        "                Y_pred = np.dot(X[i:n], W)\n",
        "                c += np.mean(np.square(Y_pred - Y[i:n]))/2 + alpha*np.mean(np.square(W))/2\n",
        "                dW = np.dot(X[i:n].T, Y_pred - Y[i:n])/(n-i)\n",
        "        W = W - learning_rate*(dW + alpha*W)\n",
        "        print(c/n*batch_size)\n",
        "        C.append(c/n*batch_size)\n",
        "    \n",
        "    return W, C\n",
        "\n",
        "def linearRegressionL1(X, Y, learning_rate, iterations,alpha=None) :\n",
        "    X = np.c_[X, np.ones(X.shape[0])]\n",
        "    n = X.shape[0]\n",
        "    W = np.zeros((X.shape[1], Y.shape[1]))\n",
        "    C = []\n",
        "    if alpha == None:\n",
        "        alpha = 0.001\n",
        "    \n",
        "    for _ in range(iterations):\n",
        "        c=0\n",
        "        for i in range(0,n,batch_size):\n",
        "            if i+batch_size < n:\n",
        "                Y_pred = np.dot(X[i:i+batch_size], W)\n",
        "                c += np.mean(np.square(Y_pred - Y[i:i+batch_size]))/2 + alpha*np.mean(np.abs(W))/2\n",
        "                dW = np.dot(X[i:i+batch_size].T, Y_pred - Y[i:i+batch_size])/batch_size\n",
        "            else:\n",
        "                Y_pred = np.dot(X[i:n], W)\n",
        "                c += np.mean(np.square(Y_pred - Y[i:n]))/2 + alpha*np.mean(np.abs(W))/2\n",
        "                dW = np.dot(X[i:n].T, Y_pred - Y[i:n])/(n-i)\n",
        "        dL1 = W.copy()\n",
        "        dL1[W>=0]=1\n",
        "        dL1[W<0]=-1\n",
        "        \n",
        "        #print(dW.shape)\n",
        "        W = W - learning_rate*(dW + alpha*dL1)\n",
        "        print(c/n*batch_size)\n",
        "        C.append(c/n*batch_size)\n",
        "    \n",
        "    return W, C\n",
        "\n",
        "def bb_intersection_over_union(boxA, boxB):\n",
        "    xA = max(boxA[0], boxB[0])\n",
        "    yA = max(boxA[1], boxB[1])\n",
        "    xB = min(boxA[2], boxB[2])\n",
        "    yB = min(boxA[3], boxB[3])\n",
        "\n",
        "    interArea = abs(max((xB - xA, 0)) * max((yB - yA), 0))\n",
        "    if interArea == 0:\n",
        "        return 0\n",
        "\n",
        "    boxAArea = abs((boxA[2] - boxA[0]) * (boxA[3] - boxA[1]))\n",
        "    boxBArea = abs((boxB[2] - boxB[0]) * (boxB[3] - boxB[1]))\n",
        "\n",
        "    iou = interArea / float(boxAArea + boxBArea - interArea)\n",
        "\n",
        "    return iou\n",
        "\n",
        "def mean_absolute_error(y1,y2):\n",
        "    return np.mean(np.abs(y1-y2))\n",
        "\n",
        "def mean_square_error(y1,y2):\n",
        "    return np.mean(np.square(y1-y2))\n",
        "\n",
        "def mean_IOU(y1,y2):\n",
        "    l = len(y1)\n",
        "    c=0\n",
        "    for i in range(l):\n",
        "        c+= bb_intersection_over_union(y1[i],y2[i])\n",
        "    return c/l\n",
        "\n",
        "W, C = linearRegression(X_train, Y_train, 0.0000002, 500)\n",
        "print(C)\n",
        "print(X_train.shape)\n",
        "\n",
        "W, C = linearRegressionSGD(X_train, Y_train, 0.00001, 1000,16)\n",
        "#print(C)\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "X = np.c_[X_test, np.ones(X_test.shape[0])]\n",
        "Z_test_LR = np.dot(X, W)\n",
        "print(mean_square_error(Z_test_LR,Y_test))\n",
        "\n",
        "W1,C1= linearRegressionL2(X_train, Y_train, 0.00001, 1000,0.001,16)\n",
        "W2,C2= linearRegressionL1(X_train, Y_train, 0.00001, 1000,0.001,16)\n",
        "\n",
        "X_test1 = np.c_[X_test, np.ones(X_test.shape[0])]\n",
        "W_test1 = np.dot(X_test1, W)\n",
        "t = np.linspace(0,1,100)\n",
        "plt.plot(C1,t,C2,t,C,t)\n",
        "plt.grid()\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('MSE')\n",
        "plt.legend(['L2_Regularization','L1_Regularization','without Regularization'])\n",
        "plt.show()\n",
        "\n",
        "draw_rect(X_input[test_indices[150]],Y_test[150],)\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "data1 = np.load('../input/pneumoniamnist/pneumoniamnist.npz')\n",
        "data2 = np.load('../input/bloodmnist/bloodmnist.npz')\n",
        "print(data1.files)\n",
        "print(data2.files)\n",
        "\n",
        "# Reshape images and save in numpy arrays\n",
        "train1_data = np.reshape(data1['train_images'],(4708,784))\n",
        "train1_labels = np.reshape(data1['train_labels'],(4708))\n",
        "\n",
        "print(train1_data.shape)\n",
        "print(train1_labels.shape)\n",
        "test1_data = np.reshape(data1['test_images'],(624,784))\n",
        "test1_labels = np.reshape(data1['test_labels'],(624))\n",
        "print(test1_data.shape)\n",
        "print(test1_labels.shape)\n",
        "\n",
        "def sigmoid(scores):\n",
        "    return 1 / (1 + np.exp(-scores))\n",
        "\n",
        "def log_likelihood(features, target, weights):\n",
        "    scores = np.dot(features, weights)\n",
        "    ll = np.sum( target*scores - np.log(1 + np.exp(scores)) )\n",
        "    return ll\n",
        "\n",
        "def logistic_regression(features, target, num_steps, learning_rate, add_intercept = False):\n",
        "    if add_intercept:\n",
        "        intercept = np.ones((features.shape[0], 1))\n",
        "        features = np.hstack((intercept, features))\n",
        "        \n",
        "    weights = np.zeros(features.shape[1])\n",
        "    \n",
        "    for step in xrange(num_steps):\n",
        "        scores = np.dot(features, weights)\n",
        "        predictions = sigmoid(scores)\n",
        "\n",
        "        # Update weights with log likelihood gradient\n",
        "        output_error_signal = target - predictions\n",
        "        \n",
        "        gradient = np.dot(features.T, output_error_signal)\n",
        "        weights += learning_rate * gradient\n",
        "\n",
        "        # Print log-likelihood every so often\n",
        "        if step % 10000 == 0:\n",
        "            print log_likelihood(features, target, weights)\n",
        "        \n",
        "    return weights\n",
        "\n",
        "weights = logistic_regression(simulated_separableish_features, simulated_labels,\n",
        "                     num_steps = 1000, learning_rate = 1e-3, add_intercept=True)\n",
        "\n",
        "\n",
        "\n",
        "def compute_confusion_matrix(true, pred):\n",
        "\n",
        "  K = len(np.unique(true)) # Number of classes \n",
        "  result = np.zeros((K, K))\n",
        "\n",
        "  for i in range(len(true)):\n",
        "    result[true[i]][pred[i]] += 1\n",
        "  tn = result[0][0] \n",
        "  fn = result[0][1] \n",
        "  fp = result[1][0] \n",
        "  tp = result[1][1]\n",
        "  acc = (tp+tn)/(tp+tn+fp+fn)\n",
        "  f1 = tp/(tp+0.5*(fp+fn)) \n",
        "  print(\"Accuracy = \", acc)\n",
        "  print(\"F-1 Score = \", f1)\n",
        "  return result\n",
        "\n",
        "print(compute_confusion_matrix(test1_labels, predictions))\n",
        "\n",
        "#print(data2['train_images'].shape)\n",
        "train2_data = np.reshape(data2['train_images'],(11959,2352))\n",
        "train2_labels = np.reshape(data2['train_labels'],(11959))\n",
        "\n",
        "#print(data2['test_images'].shape)\n",
        "test2_data = np.reshape(data2['test_images'],(3421,2352))\n",
        "test2_labels = np.reshape(data2['test_labels'],(3421))\n",
        "\n",
        "def feature_scaling(X):\n",
        "    mean = np.mean(X_data, axis=0) \n",
        "    sd = np.std(X_data, axis=0) \n",
        "    X_scaled= (X -  mean) / sd\n",
        "    \n",
        "    return X_scaled\n",
        "\n",
        "def cost_function(theta, X, Y,lambda_reg = 0.0):\n",
        "   \n",
        "    m = len(Y)\n",
        "    A = sigmoid(np.dot(X,theta)) \n",
        "    regularization = (lambda_reg/(2 * m)) * np.sum(theta**2)\n",
        "    cost=(- 1 / m) * np.sum(Y * np.log(A) + (1 - Y) * (np.log(1 - A))) + regularization\n",
        "\n",
        "def gradient(theta, X, Y,lambda_reg = 0.00):\n",
        "   \n",
        "    cost,A = cost_function(theta, X, Y)\n",
        "    m, n = X.shape\n",
        "    theta = theta.reshape((n, 1))\n",
        "    gradient = (1 / m) * np.dot(X.T,(A - Y))+ (lambda_reg /m)*theta\n",
        "    return cost, gradient\n",
        "\n",
        "def optimize(X, Y, theta, num_iterations , learning_rate,print_cost = False):\n",
        "    \n",
        "    costs = []\n",
        "    \n",
        "    \n",
        "    for i in range(num_iterations):\n",
        "        \n",
        "        cost, _gradient = gradient(theta, X, Y)\n",
        "        theta -= learning_rate * _gradient \n",
        "        \n",
        "        costs.append(cost)\n",
        "        \n",
        "        # Print the cost every 100 training iterations if required\n",
        "        if print_cost and i % 100 == 0:\n",
        "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
        "        \n",
        "    return costs,theta.reshape(2352)\n",
        "\n",
        "def predict(theta, X):\n",
        "    \n",
        "    A = sigmoid(np.dot(X,theta.T)) \n",
        "    #Predicting the class as the one with highest probability value\n",
        "    predictions = [classes[np.argmax(A[i, :])] for i in range(X.shape[0])]\n",
        "        \n",
        "    return predictions\n",
        "\n",
        "def accuracy(Y, predictions):\n",
        "   \n",
        "    accuracy = sum(predictions == Y)/len(Y)\n",
        "    \n",
        "    return accuracy\n",
        "\n",
        "def model( num_iterations = 2000, learning_rate = 0.0005,print_cost = False, print_accuracy = True):\n",
        "   \n",
        "    \n",
        "    ##Running the implemented code\n",
        "    classification = []\n",
        "    logistic_reg_accuracy = np.zeros((10))\n",
        "\n",
        "    X_data_scaled = feature_scaling(X_data)\n",
        "    \n",
        "    ##Iterating over the implemented Logistic Regression for 10 times\n",
        "    for t in range(10):\n",
        "        # Splitting the data into traning and testing where training data is 2/3 of dataset randomly with the help of \"Shuffle\"\n",
        "        X_train, X_test, Y_train, Y_test = train2_data, test2_data, train2_labels, test2_labels\n",
        "        classes = np.unique(Y_data)\n",
        "        theta = np.zeros((8, 2352)) \n",
        "        i = 0\n",
        "        for j in classes:\n",
        "            #converting the categorical variables to 0 and 1\n",
        "            Y_temp = np.array(Y_train == j, dtype = int)\n",
        "            #print(Y_temp)\n",
        "            Y_temp = Y_temp.reshape((Y_temp.shape[0], 1))\n",
        "            costs, optimal_theta = optimize(X_train, Y_temp, np.zeros((2352,1)),num_iterations , learning_rate , print_cost)\n",
        "            theta[i] = optimal_theta\n",
        "            i += 1\n",
        "          \n",
        "        #Predicting for X_test for every iteration\n",
        "        predictions = predict(theta, X_test)\n",
        "    \n",
        "        for j in range(len(predictions)):\n",
        "            classification.append(\"%s, %s\" %(predictions[j], Y_test[j]))\n",
        "    \n",
        "        #Storing the accuracies of each iteration\n",
        "        logistic_reg_accuracy[t] = accuracy(Y_test, predictions)*100\n",
        "        if print_accuracy:\n",
        "            ##Printing the accuracies of each iteration\n",
        "            print(\"Accuracy for \", t+1, \" iteration: \", logistic_reg_accuracy[t])\n",
        "\n",
        "    ##printing the average of accuracies across 10 iterations\n",
        "    print(\"\\nAverage Test Accuracy for Implemented Logistic Regression: \", logistic_reg_accuracy.mean(), '%')\n",
        "\n",
        "    ##Writing the results to file\n",
        "    f = open('C:\\\\Users\\\\javva\\\\Downloads\\\\predictions.csv', 'w')\n",
        "    for line in classification:\n",
        "        f.write(line + \"\\n\")\n",
        "    f.close()\n",
        "    \n",
        "    d = {\"costs\": costs,\n",
        "         \"Y_test\": Y_test, \n",
        "         \"Y_test_predictions\" : predictions, \n",
        "         \"learning_rate\" : learning_rate,\n",
        "         \"num_iterations\": num_iterations,\n",
        "         \"logistic_reg_accuracy\": logistic_reg_accuracy}\n",
        "    \n",
        "    return d\n",
        "\n",
        "d = model(num_iterations = 10000, learning_rate = 0.001)\n",
        "plt.plot(range(len(d[\"costs\"])),d[\"costs\"],'r')\n",
        "plt.title(\"Learning Curve \")\n",
        "plt.xlabel(\"Number of Iterations\")\n",
        "plt.ylabel(\"Cost\")\n",
        "plt.show()\n",
        "\n",
        "#Confusion matrix for the last iteration of Logistic Regression model\n",
        "conf_matrix = confusion_matrix(d[\"Y_test\"],d[\"Y_test_predictions\"])\n",
        "matrix = sns.heatmap(conf_matrix, annot = True, xticklabels = classes, yticklabels = classes)\n",
        "matrix.set(xlabel = \"True Values\", ylabel = \"False Values\")\n",
        "\n",
        "learning_rates = [0.5,0.1,0.01, 0.001]\n",
        "models = {}\n",
        "for i in learning_rates:\n",
        "    print (\"learning rate is: \" + str(i))\n",
        "    models[str(i)] = model( num_iterations = 2000, learning_rate = i, print_accuracy = False)\n",
        "    print ('\\n' + \"-------------------------------------------------------\" + '\\n')\n",
        "\n",
        "for i in learning_rates:\n",
        "    plt.plot(np.squeeze(models[str(i)][\"costs\"]), label= str(models[str(i)][\"learning_rate\"]))\n",
        "\n",
        "plt.ylabel('cost')\n",
        "plt.xlabel('iterations (hundreds)')\n",
        "\n",
        "legend = plt.legend(loc='upper right', shadow=True)\n",
        "frame = legend.get_frame()\n",
        "frame.set_facecolor('0.90')\n",
        "plt.show()\n",
        "\n",
        "class PolynomialRegression(object):\n",
        "\n",
        "    def __init__(self, x, y):     \n",
        "        \n",
        "        self.x = x\n",
        "        self.y = y      \n",
        "    \n",
        "    def standardize(self,data):\n",
        "\n",
        "        return (data - np.mean(data))/(np.max(data) - np.min(data))\n",
        "        \n",
        "    def hypothesis(self, theta, x):\n",
        "         \n",
        "        h = theta[0]\n",
        "        for i in np.arange(1, len(theta)):\n",
        "            h += theta[i]*x ** i        \n",
        "        return h        \n",
        "        \n",
        "    def computeCost(self, x, y, theta):\n",
        "   \n",
        "        m = len(y)  \n",
        "        h = self.hypothesis(theta, x)\n",
        "        errors = h-y\n",
        "        \n",
        "        return (1/(2*m))*np.sum(errors**2) \n",
        "        \n",
        "    def fit(self, method = 'normal_equation', order = 1, tol = 10**-3, numIters = 20, learningRate = 0.01):\n",
        "\n",
        "\n",
        "        if method == 'normal_equation': \n",
        "            d = {}\n",
        "            d['x' + str(0)] = np.ones([1,len(x_pts)])[0]    \n",
        "            for i in np.arange(1, order+1):                \n",
        "                d['x' + str(i)] = self.x ** (i)        \n",
        "                \n",
        "            d = OrderedDict(sorted(d.items(), key=lambda t: t[0]))\n",
        "            X = np.column_stack(d.values())  \n",
        "\n",
        "            theta = np.matmul(np.matmul(linalg.pinv(np.matmul(np.transpose(X),X)), np.transpose(X)), self.y)\n",
        "\n",
        "        elif method == 'gradient_descent':\n",
        "                \n",
        "            d = {}\n",
        "            d['x' + str(0)] = np.ones([1,len(x_pts)])[0]    \n",
        "            for i in np.arange(1, order+1):                \n",
        "                d['x' + str(i)] = self.standardize(self.x ** (i))      \n",
        "                \n",
        "            d = OrderedDict(sorted(d.items(), key=lambda t: t[0]))\n",
        "            X = np.column_stack(d.values())  \n",
        "                \n",
        "            m = len(self.x)\n",
        "            theta = np.zeros(order + 1)           \n",
        "            costs = []\n",
        "            for i in range(numIters):\n",
        "             \n",
        "                h = self.hypothesis(theta, self.x)       \n",
        "                errors = h-self.y\n",
        "                theta += -learningRate * (1/m)*np.dot(errors, X)\n",
        "                cost = self.computeCost(self.x, self.y, theta)\n",
        "                costs.append(cost)         \n",
        "                #tolerance check\n",
        "                if cost < tol:\n",
        "                    break\n",
        "                \n",
        "            self.costs = costs\n",
        "            self.numIters = numIters\n",
        "            \n",
        "        self.method = method    \n",
        "        self.theta = theta        \n",
        "\n",
        "        return self\n",
        "        \n",
        "    def plot_predictedPolyLine(self):\n",
        "               \n",
        "        plt.figure()\n",
        "        plt.scatter(self.x, self.y, s = 30, c = 'b') \n",
        "        line = self.theta[0] #y-intercept \n",
        "        label_holder = []\n",
        "        label_holder.append('%.*f' % (2, self.theta[0]))\n",
        "        for i in np.arange(1, len(self.theta)):            \n",
        "            line += self.theta[i] * self.x ** i \n",
        "            label_holder.append(' + ' +'%.*f' % (2, self.theta[i]) + r'$x^' + str(i) + '$') \n",
        "\n",
        "        plt.plot(self.x, line, label = ''.join(label_holder))        \n",
        "        plt.title('Polynomial Fit: Order ' + str(len(self.theta)-1))\n",
        "        plt.xlabel('x')\n",
        "        plt.ylabel('y') \n",
        "        plt.legend(loc = 'best')      \n",
        "\n",
        "    def plotCost(self):\n",
        "           \n",
        "        if self.method == 'gradient_descent':\n",
        "            plt.figure()\n",
        "            plt.plot(np.arange(1, self.numIters+1), self.costs, label = r'$J(\\theta)$')\n",
        "            plt.xlabel('Iterations')\n",
        "            plt.ylabel(r'$J(\\theta)$')\n",
        "            plt.title('Cost vs Iterations of Gradient Descent')\n",
        "            plt.legend(loc = 'best')\n",
        "        else:\n",
        "            print('plotCost method can only be called when using gradient descent method')\n",
        "  \n",
        "            "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4I8OVzWSioID"
      },
      "outputs": [],
      "source": [
        "#GMM for tiny imagenet\n",
        "\"\"\"Q5 GMM as a Generative Model\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from scipy.stats import multivariate_normal\n",
        "from skimage.io import imread\n",
        "from skimage.transform import downscale_local_mean, rescale\n",
        "from tqdm import tqdm\n",
        "from numba import jit\n",
        "from skimage.filters import unsharp_mask\n",
        "import os\n",
        "\n",
        "def process_data(path):\n",
        "    i = 0\n",
        "    dataset = np.zeros((500, 8*2, 8*2, 3))\n",
        "    dataset_ = np.zeros((500, 4*8*8*3))\n",
        "    for file_ in os.listdir(path):\n",
        "        if imread(path+'/'+file_).shape == (64, 64, 3):\n",
        "            dataset[i, :, :, :] = downscale_local_mean(imread(path+'/'+file_)/255.0, (4, 4, 1))\n",
        "        i += 1\n",
        "\n",
        "    dataset_ = np.reshape(dataset, (500, 4*8*8*3))\n",
        "    return dataset_\n",
        "\n",
        "\n",
        "def density_estimate(x, N):\n",
        "    gm = GaussianMixture(n_components=N, random_state=0).fit(x)\n",
        "    return gm\n",
        "\n",
        "\n",
        "def calc_pdf(x, prior, lambdas, means, covariances, N):\n",
        "    s = 0.0\n",
        "    for i in range(N):\n",
        "        s += lambdas[i]*multivariate_normal.pdf(x, mean=means[i, :], cov=covariances[i, :, :])\n",
        "    posterior = prior * s\n",
        "    return posterior\n",
        "\n",
        "def main():\n",
        "    path = '/content/drive/MyDrive/PRNN/jellyfish/images'\n",
        "    dataset = process_data(path)\n",
        "    f = density_estimate(dataset, 5)\n",
        "    c = f.sample(16)\n",
        "\n",
        "    rows = cols = 4\n",
        "    fig2, axes = plt.subplots(nrows=rows, ncols=cols, figsize=(5,5))\n",
        "    for idx in range(rows*cols):\n",
        "        d = c[0][idx]\n",
        "        d = d-np.min(d)\n",
        "        d = d/np.max(d)\n",
        "        image = np.reshape(d, (8*2, 8*2, 3))\n",
        "        #image = rescale(np.reshape(d, (8*2, 8*2, 3)), (4, 4, 1))\n",
        "        row = idx // cols\n",
        "        col = idx % cols\n",
        "        axes[row, col].axis(\"off\")\n",
        "        axes[row, col].imshow(image, aspect=\"auto\")\n",
        "    plt.subplots_adjust(wspace=.01, hspace=.01)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n",
        "from torch.nn.functional import adaptive_avg_pool2d\n",
        "import torchvision.models as models\n",
        "import torch.nn.functional as F\n",
        "from scipy import linalg\n",
        "\n",
        "class InceptionV3(nn.Module):\n",
        "    \"\"\"Pretrained InceptionV3 network returning feature maps\"\"\"\n",
        "\n",
        "    # Index of default block of inception to return,\n",
        "    # corresponds to output of final average pooling\n",
        "    DEFAULT_BLOCK_INDEX = 3\n",
        "\n",
        "    # Maps feature dimensionality to their output blocks indices\n",
        "    BLOCK_INDEX_BY_DIM = {\n",
        "        64: 0,   # First max pooling features\n",
        "        192: 1,  # Second max pooling featurs\n",
        "        768: 2,  # Pre-aux classifier features\n",
        "        2048: 3  # Final average pooling features\n",
        "    }\n",
        "\n",
        "    def __init__(self,\n",
        "                 output_blocks=[DEFAULT_BLOCK_INDEX],\n",
        "                 resize_input=True,\n",
        "                 normalize_input=True,\n",
        "                 requires_grad=False):\n",
        "        \n",
        "        super(InceptionV3, self).__init__()\n",
        "\n",
        "        self.resize_input = resize_input\n",
        "        self.normalize_input = normalize_input\n",
        "        self.output_blocks = sorted(output_blocks)\n",
        "        self.last_needed_block = max(output_blocks)\n",
        "\n",
        "        assert self.last_needed_block <= 3, \\\n",
        "            'Last possible output block index is 3'\n",
        "\n",
        "        self.blocks = nn.ModuleList()\n",
        "\n",
        "        \n",
        "        inception = models.inception_v3(pretrained=True)\n",
        "\n",
        "        # Block 0: input to maxpool1\n",
        "        block0 = [\n",
        "            inception.Conv2d_1a_3x3,\n",
        "            inception.Conv2d_2a_3x3,\n",
        "            inception.Conv2d_2b_3x3,\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2)\n",
        "        ]\n",
        "        self.blocks.append(nn.Sequential(*block0))\n",
        "\n",
        "        # Block 1: maxpool1 to maxpool2\n",
        "        if self.last_needed_block >= 1:\n",
        "            block1 = [\n",
        "                inception.Conv2d_3b_1x1,\n",
        "                inception.Conv2d_4a_3x3,\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2)\n",
        "            ]\n",
        "            self.blocks.append(nn.Sequential(*block1))\n",
        "\n",
        "        # Block 2: maxpool2 to aux classifier\n",
        "        if self.last_needed_block >= 2:\n",
        "            block2 = [\n",
        "                inception.Mixed_5b,\n",
        "                inception.Mixed_5c,\n",
        "                inception.Mixed_5d,\n",
        "                inception.Mixed_6a,\n",
        "                inception.Mixed_6b,\n",
        "                inception.Mixed_6c,\n",
        "                inception.Mixed_6d,\n",
        "                inception.Mixed_6e,\n",
        "            ]\n",
        "            self.blocks.append(nn.Sequential(*block2))\n",
        "\n",
        "        # Block 3: aux classifier to final avgpool\n",
        "        if self.last_needed_block >= 3:\n",
        "            block3 = [\n",
        "                inception.Mixed_7a,\n",
        "                inception.Mixed_7b,\n",
        "                inception.Mixed_7c,\n",
        "                nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
        "            ]\n",
        "            self.blocks.append(nn.Sequential(*block3))\n",
        "\n",
        "        for param in self.parameters():\n",
        "            param.requires_grad = requires_grad\n",
        "\n",
        "    def forward(self, inp):\n",
        "        \"\"\"Get Inception feature maps\n",
        "        Parameters\n",
        "        ----------\n",
        "        inp : torch.autograd.Variable\n",
        "            Input tensor of shape Bx3xHxW. Values are expected to be in\n",
        "            range (0, 1)\n",
        "        Returns\n",
        "        -------\n",
        "        List of torch.autograd.Variable, corresponding to the selected output\n",
        "        block, sorted ascending by index\n",
        "        \"\"\"\n",
        "        outp = []\n",
        "        x = inp\n",
        "\n",
        "        if self.resize_input:\n",
        "            x = F.interpolate(x,\n",
        "                              size=(299, 299),\n",
        "                              mode='bilinear',\n",
        "                              align_corners=False)\n",
        "\n",
        "        if self.normalize_input:\n",
        "            x = 2 * x - 1  # Scale from range (0, 1) to range (-1, 1)\n",
        "\n",
        "        for idx, block in enumerate(self.blocks):\n",
        "            x = block(x)\n",
        "            if idx in self.output_blocks:\n",
        "                outp.append(x)\n",
        "\n",
        "            if idx == self.last_needed_block:\n",
        "                break\n",
        "\n",
        "        return outp\n",
        "    \n",
        "block_idx = InceptionV3.BLOCK_INDEX_BY_DIM[2048]\n",
        "fid_model = InceptionV3([block_idx])\n",
        "fid_model = fid_model.cuda()\n",
        "\n",
        "def calculate_activation_statistics(images,model,batch_size=128, dims=2048,\n",
        "                    cuda=False):\n",
        "    model.eval()\n",
        "    act=np.empty((len(images), dims))\n",
        "    \n",
        "    if cuda:\n",
        "        batch=images.cuda()\n",
        "    else:\n",
        "        batch=images\n",
        "    pred = model(batch)[0]\n",
        "\n",
        "        # If model output is not scalar, apply global spatial average pooling.\n",
        "        # This happens if you choose a dimensionality not equal 2048.\n",
        "    if pred.size(2) != 1 or pred.size(3) != 1:\n",
        "        pred = adaptive_avg_pool2d(pred, output_size=(1, 1))\n",
        "\n",
        "    act= pred.cpu().data.numpy().reshape(pred.size(0), -1)\n",
        "    \n",
        "    mu = np.mean(act, axis=0)\n",
        "    sigma = np.cov(act, rowvar=False)\n",
        "    return mu, sigma\n",
        "\n",
        "def calculate_frechet_distance(mu1, sigma1, mu2, sigma2, eps=1e-6):\n",
        "  \n",
        "    mu1 = np.atleast_1d(mu1)\n",
        "    mu2 = np.atleast_1d(mu2)\n",
        "\n",
        "    sigma1 = np.atleast_2d(sigma1)\n",
        "    sigma2 = np.atleast_2d(sigma2)\n",
        "\n",
        "    assert mu1.shape == mu2.shape, \\\n",
        "        'Training and test mean vectors have different lengths'\n",
        "    assert sigma1.shape == sigma2.shape, \\\n",
        "        'Training and test covariances have different dimensions'\n",
        "\n",
        "    diff = mu1 - mu2\n",
        "\n",
        "    \n",
        "    covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n",
        "    if not np.isfinite(covmean).all():\n",
        "        msg = ('fid calculation produces singular product; '\n",
        "               'adding %s to diagonal of cov estimates') % eps\n",
        "        print(msg)\n",
        "        offset = np.eye(sigma1.shape[0]) * eps\n",
        "        covmean = linalg.sqrtm((sigma1 + offset).dot(sigma2 + offset))\n",
        "\n",
        "    \n",
        "    if np.iscomplexobj(covmean):\n",
        "        if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):\n",
        "            m = np.max(np.abs(covmean.imag))\n",
        "            raise ValueError('Imaginary component {}'.format(m))\n",
        "        covmean = covmean.real\n",
        "\n",
        "    tr_covmean = np.trace(covmean)\n",
        "\n",
        "    return (diff.dot(diff) + np.trace(sigma1) +\n",
        "            np.trace(sigma2) - 2 * tr_covmean)\n",
        "\n",
        "def calculate_fretchet(images_real,images_fake,model):\n",
        "     mu_1,std_1=calculate_activation_statistics(images_real,model,cuda=True)\n",
        "     mu_2,std_2=calculate_activation_statistics(images_fake,model,cuda=True)\n",
        "   \n",
        "     fid_value = calculate_frechet_distance(mu_1, std_1, mu_2, std_2)\n",
        "     return fid_value\n",
        "\n",
        "#Calculate FID score of generated sample\n",
        "block_idx = InceptionV3.BLOCK_INDEX_BY_DIM[2048]\n",
        "model = InceptionV3([block_idx])\n",
        "model = model.cuda()\n",
        "avg_fid_score = 0.0\n",
        "img_size=(h_image,w_image)\n",
        "h_image = 32//2\n",
        "w_image = 32//2\n",
        "\n",
        "for i in range(10):\n",
        "    d = c[0][i]\n",
        "    d = d-np.min(d)\n",
        "    d = d/np.max(d)\n",
        "    image = np.reshape(d, (8*2, 8*2, 3))\n",
        "    c = resize(np.asarray(X_test[i, :, :, 0]), (64, 64), anti_aliasing=True)\n",
        "    d = resize(np.asarray(output), (64, 64), anti_aliasing=True)\n",
        "    a = torch.from_numpy(c)\n",
        "    b = torch.from_numpy(d)\n",
        "    a = torch.from_numpy(np.expand_dims(a, 0)).float()\n",
        "    b = torch.from_numpy(np.expand_dims(b, 0)).float()\n",
        "    avg_fid_score += calculate_fretchet(a, b, model)\n",
        "print(avg_fid_score/10.0)\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import multivariate_normal\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from skimage.transform import downscale_local_mean\n",
        "from tqdm import tqdm\n",
        "from numba import jit\n",
        "from sklearn import metrics\n",
        "\n",
        "def process_data(path):\n",
        "\n",
        "    dataset = np.load(path)\n",
        "    train_x, train_y = dataset['train_images']/255.0, dataset['train_labels']\n",
        "    val_x, val_y = dataset['val_images']/255.0, dataset['val_labels']\n",
        "    test_x, test_y = dataset['test_images']/255.0, dataset['test_labels']\n",
        "\n",
        "    scale = 4\n",
        "\n",
        "    train_x_ = np.zeros((train_x.shape[0], train_x.shape[1]**2//scale**2))\n",
        "    val_x_ = np.zeros((val_x.shape[0], val_x.shape[1]**2//scale**2))\n",
        "    test_x_ = np.zeros((test_x.shape[0], test_x.shape[1]**2//scale**2))\n",
        "\n",
        "    for i in range(train_x.shape[0]):\n",
        "        temp = downscale_local_mean(train_x[i, :, :], (scale, scale))\n",
        "        train_x_[i, :] = temp.reshape(train_x.shape[1]**2//scale**2)\n",
        "        #train_x_[i, :] = train_x_[i, :]/np.max(train_x_[i, :])\n",
        "\n",
        "    for i in range(val_x.shape[0]):\n",
        "        temp = downscale_local_mean(val_x[i, :, :], (scale, scale))\n",
        "        val_x_[i, :] = temp.reshape(val_x.shape[1]**2//scale**2)\n",
        "        #val_x_[i, :] = val_x_[i, :]/np.max(val_x_[i, :])\n",
        "\n",
        "    for i in range(test_x.shape[0]):\n",
        "        temp = downscale_local_mean(test_x[i, :, :], (scale, scale))\n",
        "        test_x_[i, :] = temp.reshape(test_x.shape[1]**2//scale**2)\n",
        "        #test_x_[i, :] = test_x_[i, :]/np.max(test_x_[i, :])\n",
        "\n",
        "    return train_x_, train_y, test_x_, test_y, val_x_, val_y\n",
        "\n",
        "\n",
        "def class_conditionals(x, y):\n",
        "    x_0 = np.zeros((np.sum(y==0), x.shape[1]))\n",
        "    y_0 = np.zeros((np.sum(y==0)))\n",
        "    x_1 = np.zeros((np.sum(y==1), x.shape[1]))\n",
        "    y_1 = np.ones((np.sum(y==1)))\n",
        "\n",
        "    p0 = (np.sum(y==0))/y.shape[0]\n",
        "    p1 = (np.sum(y==1))/y.shape[0]\n",
        "\n",
        "    i_0 = 0\n",
        "    i_1 = 0\n",
        "    for i in range(len(y)):\n",
        "        if(y[i] == 0):\n",
        "            x_0[i_0, :] = x[i, :]\n",
        "            i_0 += 1\n",
        "        else:\n",
        "            x_1[i_1, :] = x[i, :]\n",
        "            i_1 += 1\n",
        "    return x_0, y_0, x_1, y_1, p0, p1\n",
        "\n",
        "\n",
        "\n",
        "def phi(x, mu, sigma, k):\n",
        "    if np.isfinite(np.linalg.cond(sigma)):\n",
        "        sigma = sigma + np.eye(sigma.shape[0])\n",
        "    \n",
        "    out = multivariate_normal(mu, sigma).pdf(x)\n",
        "    return out\n",
        "\n",
        "\n",
        "def em(data, K, iters):\n",
        "    # K = Number of mixture densities, N = Number of data samples\n",
        "    N = data.shape[0]\n",
        "    \n",
        "    # Initialize the parameters - mixing coefficients, means and covariances\n",
        "    lambdas = (1/K)*np.ones(K)\n",
        "    means = np.zeros((data.shape[1], K))\n",
        "    sigmas = np.zeros((K, data.shape[1], data.shape[1]))\n",
        "    for i in range(K):\n",
        "        sigmas[i, :, :] = np.eye(data.shape[1])\n",
        "    gamma = np.ones((N, K))\n",
        "    \n",
        "    for k in tqdm(range(iters)):\n",
        "        # E Step\n",
        "        for i in range(N):\n",
        "            for j in range(K):                    \n",
        "                gamma[i, j] = lambdas[j]*phi(data[i, :], means[:, j], sigmas[j, :, :], [i, j, k])\n",
        "                sums = 0.0\n",
        "                for l in range(K):\n",
        "                    sums += lambdas[l]*phi(data[i, :], means[:, l], sigmas[l, :, :], [i, j, k])\n",
        "\n",
        "                if not(sums == 0):\n",
        "                    gamma[i, j] = gamma[i, j] / sums\n",
        "                else:\n",
        "                    gamma[i, j] = gamma[i, j] / (0.00001 + sums)\n",
        "\n",
        "        # M Step\n",
        "        # Sum over rows of gamma for all data samples\n",
        "        denominator = np.sum(gamma, axis=0)\n",
        "\n",
        "        for j in range(K):\n",
        "            mean_sums = np.zeros(data.shape[1])\n",
        "            cov_sums = np.zeros((data.shape[1], data.shape[1]))\n",
        "            for i in range(N):\n",
        "                mean_sums += gamma[i, j]*data[i, :]\n",
        "                cov_sums += gamma[i, j]*np.outer((data[i, :]-means[:, j]),\n",
        "                                                 (data[i, :]-means[:, j]).transpose())\n",
        "\n",
        "            means[:, j] = mean_sums / \\\n",
        "                denominator[j] if not(denominator[j] == 0) else mean_sums / \\\n",
        "                (0.00001 + denominator[j])\n",
        "            sigmas[j, :, :] = cov_sums / \\\n",
        "                denominator[j] if not(denominator[j] == 0) else cov_sums / \\\n",
        "                (0.00001 + denominator[j])\n",
        "        lambdas = (1/N)*denominator\n",
        "\n",
        "    params = [lambdas, means, sigmas]\n",
        "    return params\n",
        "\n",
        "\n",
        "def calc_pdf(x, prior, lambdas, means, covariances, N):\n",
        "    s = 0.0\n",
        "    for i in range(N):\n",
        "        s += lambdas[i]*multivariate_normal.pdf(x, mean=means[i, :], cov=covariances[i, :, :])\n",
        "    posterior = prior * s\n",
        "    return posterior\n",
        "\n",
        "def bayes_with_gmms(train_x, train_y, test_x, test_y, N):\n",
        "    print(\"\\n[INFO] Fitting the data to a GMM using EM \\n\")\n",
        "    # Fit the GMMs with training data\n",
        "    x_0, y_0, x_1, y_1, p0, p1 = class_conditionals(train_x, train_y)\n",
        "    em_iters = 5\n",
        "    \n",
        "    ####\n",
        "    f0_lambdas, f0_means, f0_covariances = em(x_0, N, em_iters)\n",
        "    lambdas0, means0, covariances0 = f0_lambdas, f0_means.transpose(), f0_covariances\n",
        "    \n",
        "    ###\n",
        "    f1_lambdas, f1_means, f1_covariances = em(x_1, N, em_iters)\n",
        "    lambdas1, means1, covariances1 = f1_lambdas, f1_means.transpose(), f1_covariances\n",
        "    \n",
        "    tp = 0\n",
        "    tn = 0\n",
        "    fp = 0\n",
        "    fn = 0\n",
        "    print(\"[INFO] Calculating accuracy and F1 score on test data for GMM with \", N, \" component(s)\\n\")\n",
        "\n",
        "    test_preds = np.zeros_like(test_y)\n",
        "    for i in range(test_y.shape[0]):\n",
        "        q1 = calc_pdf(test_x[i, :], p1, lambdas1, means1, covariances1, N)\n",
        "        q0 = calc_pdf(test_x[i, :], p0, lambdas0, means0, covariances0, N)\n",
        "        if(q1 >= q0):\n",
        "            pred_class = 1\n",
        "            test_preds[i] = 1\n",
        "            if(test_y[i] == pred_class):\n",
        "                tp += 1\n",
        "            else:\n",
        "                fp += 1\n",
        "        else:\n",
        "            pred_class = 0\n",
        "            test_preds[i] = 0\n",
        "            if(test_y[i] == pred_class):\n",
        "                tn += 1\n",
        "            else:\n",
        "                fn += 1\n",
        "\n",
        "    acc = (tp+tn)/(tp+tn+fp+fn)\n",
        "    f1 = tp/(tp+0.5*(fp+fn))\n",
        "    print(\"\\nAccuracy = \", acc)\n",
        "    print(\"F-1 Score = \", f1, \"\\n\\n\")    \n",
        "    return test_preds\n",
        "\n",
        "def main():\n",
        "    path = '/content/drive/MyDrive/PRNN/pneumoniamnist.npz'\n",
        "    train_x, train_y, test_x, test_y, val_x, val_y = process_data(path)\n",
        "    #bayes_with_gmms(train_x, train_y, val_x, val_y, 1)\n",
        "    #bayes_with_gmms(train_x, train_y, val_x, val_y, 2)\n",
        "    test_preds = bayes_with_gmms(train_x, train_y, val_x, val_y, 3)\n",
        "    print(sklearn.metrics.f1_score(test_y, test_preds))\n",
        "    print(sklearn.metrics.roc_auc_score(test_y, test_preds))\n",
        "    #bayes_with_gmms(train_x, train_y, val_x, val_y, 4)\n",
        "    #bayes_with_gmms(train_x, train_y, val_x, val_y, 5)\n",
        "    #bayes_with_gmms(train_x, train_y, val_x, val_y, 6)\n",
        "    #bayes_with_gmms(train_x, train_y, val_x, val_y, 7)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "\"\"\"MAP with Gaussian Class conditionals and Gaussian prior\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import multivariate_normal\n",
        "from skimage.transform import downscale_local_mean\n",
        "\n",
        "def process_data(path):\n",
        "\n",
        "    dataset = np.load(path)\n",
        "    train_x, train_y = dataset['train_images']/255.0, dataset['train_labels']\n",
        "    val_x, val_y = dataset['val_images']/255.0, dataset['val_labels']\n",
        "    test_x, test_y = dataset['test_images']/255.0, dataset['test_labels']\n",
        "\n",
        "    scale = 4\n",
        "\n",
        "    train_x_ = np.zeros((train_x.shape[0], train_x.shape[1]**2//scale**2))\n",
        "    val_x_ = np.zeros((val_x.shape[0], val_x.shape[1]**2//scale**2))\n",
        "    test_x_ = np.zeros((test_x.shape[0], test_x.shape[1]**2//scale**2))\n",
        "\n",
        "    for i in range(train_x.shape[0]):\n",
        "        temp = downscale_local_mean(train_x[i, :, :], (scale, scale))\n",
        "        train_x_[i, :] = temp.reshape(train_x.shape[1]**2//scale**2)\n",
        "        #train_x_[i, :] = train_x_[i, :]/np.max(train_x_[i, :])\n",
        "\n",
        "    for i in range(val_x.shape[0]):\n",
        "        temp = downscale_local_mean(val_x[i, :, :], (scale, scale))\n",
        "        val_x_[i, :] = temp.reshape(val_x.shape[1]**2//scale**2)\n",
        "        #val_x_[i, :] = val_x_[i, :]/np.max(val_x_[i, :])\n",
        "\n",
        "    for i in range(test_x.shape[0]):\n",
        "        temp = downscale_local_mean(test_x[i, :, :], (scale, scale))\n",
        "        test_x_[i, :] = temp.reshape(test_x.shape[1]**2//scale**2)\n",
        "        #test_x_[i, :] = test_x_[i, :]/np.max(test_x_[i, :])\n",
        "\n",
        "    return train_x_, train_y, test_x_, test_y, val_x_, val_y\n",
        "\n",
        "\n",
        "def bayes_classifier(train_x, train_y, prior_mu0, prior_mu1):\n",
        "    \n",
        "    sum0 = np.zeros((train_x[1].shape[0]))\n",
        "    sum1 = np.zeros((train_x[1].shape[0]))\n",
        "    mean0 = np.zeros((train_x[1].shape[0]))\n",
        "    mean1 = np.zeros((train_x[1].shape[0]))   \n",
        "    sig0 = np.zeros((train_x[1].shape[0], train_x[1].shape[0]), dtype=np.double)\n",
        "    sig1 = np.zeros((train_x[1].shape[0], train_x[1].shape[0]), dtype=np.double)\n",
        "    sig0_N = np.zeros((train_x[1].shape[0], train_x[1].shape[0]), dtype=np.double)\n",
        "    sig1_N = np.zeros((train_x[1].shape[0], train_x[1].shape[0]), dtype=np.double)\n",
        "    prior_sig = np.eye(train_x[1].shape[0])\n",
        "    n0 = 0\n",
        "    n1 = 0\n",
        "    n = train_x.shape[0]\n",
        "\n",
        "    for i in range(train_y.shape[0]):\n",
        "        if(train_y[i] == 0):\n",
        "            sum0 += train_x[i]\n",
        "            n0 += 1\n",
        "        else:\n",
        "            sum1 += train_x[i]\n",
        "            n1 += 1\n",
        "\n",
        "\n",
        "    print(\"[INFO] Estimating class conditional densities\")\n",
        "\n",
        "    for i in range(train_y.shape[0]):\n",
        "        if(train_y[i] == 0):\n",
        "            sig0 += np.outer(train_x[i]-sum0/n0, (train_x[i]-sum0/n0).T)\n",
        "        else:\n",
        "            sig1 += np.outer(train_x[i]-sum1/n1, (train_x[i]-sum1/n1).T)\n",
        "\n",
        "    sig0 *= 1.0/n0\n",
        "    sig1 *= 1.0/n1\n",
        "\n",
        "    sig0_N = np.linalg.pinv(prior_sig + n0*np.linalg.pinv(sig0))\n",
        "    sig1_N = np.linalg.pinv(prior_sig + n1*np.linalg.pinv(sig1))\n",
        "    \n",
        "    mean0 = np.matmul(sig0_N, np.matmul(prior_sig, prior_mu0) + np.matmul(np.linalg.pinv(sig0), sum0))\n",
        "    mean1 = np.matmul(sig1_N, np.matmul(prior_sig, prior_mu1) + np.matmul(np.linalg.pinv(sig1), sum1))\n",
        "\n",
        "    p0 = n0/n\n",
        "    p1 = n1/n\n",
        "\n",
        "    (sign0, logdet0) = np.linalg.slogdet(sig0_N)\n",
        "    (sign1, logdet1) = np.linalg.slogdet(sig1_N)\n",
        "\n",
        "    return mean0, mean1, sig0_N, sig1_N, logdet0, sign0, logdet1, sign1, p0, p1\n",
        "\n",
        "\n",
        "def testing_bayes(test_x, test_y, mu0, mu1, sig0, sig1, logdet0, sign0, logdet1, sign1, p0, p1):\n",
        "    tp = 0\n",
        "    tn = 0\n",
        "    fp = 0\n",
        "    fn = 0\n",
        "\n",
        "    print(\"[INFO] Calculating accuracy and F1 score on test data\")\n",
        "\n",
        "    for i in range(test_y.shape[0]):\n",
        "        X = test_x[i, :]\n",
        "        log_q1 = np.log(p1) - 0.5*logdet1 - 0.5*np.matmul((X-mu1).transpose(),\n",
        "                                                          np.matmul(np.linalg.pinv(sig1), X-mu1))\n",
        "        log_q0 = np.log(p0) - 0.5*logdet0 - 0.5*np.matmul((X-mu0).transpose(),\n",
        "                                                          np.matmul(np.linalg.pinv(sig0), X-mu0))\n",
        "\n",
        "        if(log_q1 >= log_q0):\n",
        "            pred_class = 1\n",
        "            if(test_y[i] == pred_class):\n",
        "                tp += 1\n",
        "            else:\n",
        "                fp += 1\n",
        "        else:\n",
        "            pred_class = 0\n",
        "            if(test_y[i] == pred_class):\n",
        "                tn += 1\n",
        "            else:\n",
        "                fn += 1\n",
        "\n",
        "    acc = (tp+tn)/(tp+tn+fp+fn)\n",
        "    f1 = tp/(tp+0.5*(fp+fn))\n",
        "    print(\"Accuracy = \", acc)\n",
        "    print(\"F-1 Score = \", f1)\n",
        "    return\n",
        "\n",
        "\n",
        "def main():\n",
        "    path =  '/content/drive/MyDrive/PRNN/pneumoniamnist.npz'\n",
        "    train_x, train_y, test_x, test_y, val_x, val_y = process_data(path)\n",
        "    mu0, mu1, sig0, sig1, logdet0, sign0, logdet1, sign1, p0, p1 = bayes_classifier(\n",
        "        train_x, train_y, np.random.normal(size=(49))*10000000000, np.random.normal(size=(49))*-5000000000)\n",
        "    testing_bayes(test_x, test_y, mu0, mu1, sig0, sig1, logdet0, sign0, logdet1, sign1, p0, p1)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "PRNN_A1_Rahul_jeevithiesh_Nishanth_Bhartendu.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
